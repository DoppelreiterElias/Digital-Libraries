<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Click2Annotate: Automated Insight Externalization with Rich Semantics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Yang</forename>
								<surname>Chen</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Scott</forename>
								<surname>Barlowe</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC Charlotte</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jing</forename>
								<surname>Yang</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">UNC Charlotte</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Click2Annotate: Automated Insight Externalization with Rich Semantics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Visual Analytics, Decision Making, Annotation, In-</term>
					<term>sight Management, Multidimensional Visualization</term>
					<term>Index Terms: H50 [Information Interfaces and Presentation]:</term>
					<term>General;</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Insight Externalization (IE) refers to the process of capturing and recording the semantics of insights in decision making and problem solving. To reduce human effort, Automated Insight Externaliza-tion (AIE) is desired. Most existing IE approaches achieve automation by capturing events (e.g., clicks and key presses) or actions (e.g., panning and zooming). In this paper, we propose a novel AIE approach named Click2Annotate. It allows semi-automatic insight annotation that captures low-level analytics task results (e.g., clusters and outliers), which have higher semantic richness and abstraction levels than actions and events. Click2Annotate has two significant benefits. First, it reduces human effort required in IE and generates annotations easy to understand. Second, the rich semantic information encoded in the annotations enables various insight management activities, such as insight browsing and insight retrieval. We present a formal user study that proved this first benefit. We also illustrate the second benefit by presenting the novel insight management activities we developed based on Click2Annotate, namely scented insight browsing and faceted insight search.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multidimensional data exist in a wide variety of applications, such as financial analytics, genomic analysis, and health analytics. In these applications, seeking insights from data and using them as evidence for hypothesis generation and evaluation are important steps in Decision Making and Problem Solving (DMPS). Since a DMPS process may involve a large number of insights, insight externalization , namely the process of capturing and recording the semantics of insights <ref type="bibr" coords="1,108.92,503.32,13.75,8.20" target="#b11">[12]</ref>, is important for insight revisiting, association, comparison, and exchange. In this paper, we address the challenge of insight externalization for analytic insights. Analytic insights, a category of insights among other types, are the most traditional sense of insights supported in visualization systems <ref type="bibr" coords="1,251.38,543.18,13.74,8.20" target="#b10">[11]</ref>. They come from exploratory analysis, extrapolation, and consist in the large or small eureka moments where a body of data comes into focus for a user <ref type="bibr" coords="1,114.50,573.07,13.74,8.20" target="#b10">[11]</ref>. Wehrend and Lewis identified 11 low-level tasks which can result in an analytic insight for a user <ref type="bibr" coords="1,248.48,583.03,13.74,8.20" target="#b14">[15]</ref>. Insight Externalization (IE) in most existing information visualization systems, such as Many-Eyes <ref type="bibr" coords="1,200.14,603.96,14.94,8.20" target="#b13">[14] </ref>and Name Voyagers <ref type="bibr" coords="1,54.00,613.93,9.52,8.20" target="#b6">[7]</ref> , requires users to type notes, draw marks, or connect associated insights manually. When the number of insights grows larger, these manual approaches become tedious, inefficient, inaccurate, and time consuming <ref type="bibr" coords="1,395.16,159.25,9.52,8.20" target="#b5">[6]</ref> . To address these problems, multiple efforts have been conducted toward Automated Insight Externalization (AIE) in recent years. Existing AIE approaches can be classified according to the fourtier visual analytic activity model proposed by Gotz and Zhou <ref type="bibr" coords="1,545.30,199.38,9.52,8.20" target="#b5">[6]</ref>. In this model, visual analytic activities are abstracted into four levels namely tasks, sub-tasks, actions, and events. They range in semantic richness and abstraction levels from high to low. Tasks correspond to a user's highest-level analytic goals. Sub-tasks correspond to more objective, concrete analytic goals, such as finding clusters, outliers, or correlations. They are also called low level analytic tasks in other literatures <ref type="bibr" coords="1,439.31,269.12,9.52,8.20" target="#b2">[3]</ref> . Actions refer to atomic analytic steps such as zooming and panning. Events correspond to the lowest-level of interaction events, such as mouse clicks and button presses. The automation in most existing IE approaches are conducted at the action or event level. To the best of our knowledge, there exists no general IE approach for multidimensional datasets that conducts the automation at the sub-task level. We argue that conducting AIE at the sub-task level is a promising research direction. The reasons are: @BULLET Sub-tasks are less application-dependent than tasks. According to Amar and Stasko <ref type="bibr" coords="1,431.04,377.87,9.52,8.20" target="#b2">[3]</ref>, there exists a set of low-level analytic tasks (sub-tasks) that are common to most multidimensional datasets. Therefore, it is possible to develop AIE techniques independent from particular domains and applications at the sub-task level. </p><p>@BULLET Information captured from the sub-task level, such as clusters and outliers, can have higher semantic richness and abstraction levels than that from the action and event levels, such as zooming and mouse clicks. The former will be easier to understand , recall, retrieve, and use in the DMPS process than the latter. @BULLET Annotations with information from the sub-task level can be decoupled from the low level user exploration behaviors. For example, we can annotate an insight as a cluster without recording how this cluster is found. As a consequence, the annotations are independent from the visualization platforms on which the insights are captured. Thus the exchange of insights among different visualization systems can be enabled. In addition , the implementation of the annotation approach can be made simpler by not capturing the exploration process. The storage of the annotations can also be more efficient without the exploration process captured. </p><p> In this paper, we propose a novel AIE approach for analytic insights , insights for short in the rest of this paper. It conducts automation at the sub-task level and is named Click2Annotate. The approach is based on a three-component insight model <ref type="bibr" coords="1,518.46,654.01,9.52,8.20" target="#b3">[4]</ref>. In this model, an insight consists of a fact extracted from data under analysis , a domain/application specific knowledge base against which the fact is evaluated, and objective and subjective evaluations of the fact against the knowledge base. A fact, such as a cluster or an outlier, is a direct consequence of a sub-task. For example, a cluster is the consequence of finding a cluster sub-task. Facts are do-main and application independent while knowledge bases and evaluations are domain and application related <ref type="bibr" coords="2,212.49,67.72,9.52,8.20" target="#b3">[4]</ref>. As a general AIE approach, Click2Annotate enhances automation in fact annotation, and allows users to annotate knowledge bases and evaluations with light human effort. The automation of Click2Annotate in fact annotation is based on the following observations reported in <ref type="bibr" coords="2,205.74,117.84,9.52,8.20" target="#b3">[4]</ref> . First, most facts extracted from multidimensional data fall into multiple categories independent from the domains/applications and visualization tools. Second, users can effectively and efficiently classify facts into these categories. Third, the same set of context and content information is often used to annotate facts falling into the same category . According to the above observations, the core components of Click2Annotate are annotation templates each of which guides the semi-automatic annotation of a certain type of facts. They are either pre-defined for popular fact types or interactively created by users. During the annotation process, the users only need to highlight data composing a fact, decide the type of the fact, and select the corresponding annotation template. The system will automatically follow the template to fetch information, encode it, and generate a narrative annotation for the insight. Click2Annotate is introduced in detail in Section 3. Our formal user study, which is reported in Section 4, proved that Click2Annotate enhanced annotation efficiency and the annotations generated were easy to understand. Besides enhancing annotation efficiency, the semantic-rich information automatically captured at the sub-task level by our approach also enables a set of novel insight management activities, such as scented insight browsing and effective insight retrieval through faceted search. We have implemented a fully working prototype named ManyInsights that integrates these insight management activities and Click2Annotate. We present ManyInsights and its insight management functions in Section 5 to prove the benefits of our AIE approach in supporting insight management activities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Insight Externalization and Management</head><p>Most existing IE approaches for multidimensional datasets rely on human beings to input the task or sub-task information for insights captured. For example, Many-Eyes <ref type="bibr" coords="2,185.65,444.19,14.94,8.20" target="#b13">[14] </ref> provides a discussion forum where users can post threads to share their insights or free thoughts on visualizations. Systems such as Sandbox <ref type="bibr" coords="2,262.19,464.12,14.94,8.20" target="#b15">[16] </ref>and Name Voyagers <ref type="bibr" coords="2,113.80,474.08,10.45,8.20" target="#b6">[7] </ref>allow users to jot down their observations and opinions into visualization views. Multiple efforts have been conducted on AIE approaches for multidimensional datasets. Most of them conduct the automation at the action or event level. They automatically capture the actions or events that lead to the discovery of the insight to be externalized. They allow users to infer the high-level logical constructs of the analysis and recall the insight itself by reviewing the full history of analytic activities <ref type="bibr" coords="2,120.35,554.08,9.52,8.20" target="#b5">[6]</ref>. For example, Aruvi <ref type="bibr" coords="2,212.43,554.08,14.94,8.20" target="#b11">[12] </ref> provides a historical record of users' insight exploration process by automatically capturing their navigation steps. Gotz and Zhou <ref type="bibr" coords="2,228.35,574.01,10.45,8.20" target="#b5">[6] </ref> propose an approach that automatically captures user exploration activities at the action level. Since information automatically captured from the action or event level has limited semantic meanings to users, human effort is usually required in these approaches to input the semantics of the insights. Click2Annotate conducts automation in the sub-task level to enhance IE effectiveness and efficiency. We need to point out that our approach does not support users in reviewing the visual exploration process. However, there is no conflict between our approach and other AIE approaches that capture visual exploration histories. They can be used together if desired. Insight retrieval, the process of searching and processing information in insights, is supported by many existing multidimensional visualization systems. For example, Many-Eyes <ref type="bibr" coords="2,262.60,703.84,14.94,8.20" target="#b13">[14] </ref>and SparTag.us system <ref type="bibr" coords="2,124.57,713.80,10.45,8.20" target="#b8">[9] </ref> allow users to search insights with annotations containing keywords of interest. The effectiveness and efficiency of insight retrieval in visualization systems is limited by their IE approaches, since automatically captured actions and events are difficult to use as search criteria while manually generated annotations are not formalized and the information they contain is often incomplete and inaccurate. The semantic-rich annotations generated from Click2Annotate provide new potentials for effective and efficient insight retrieval as well as other insight management activ- ities. Many IE approaches have been proposed in the field of document visualization and management. For example, Zheng et al <ref type="bibr" coords="2,543.07,158.11,14.94,8.20" target="#b17">[18] </ref> propose a structured annotation approach that uses a unified annotation model to record and organize co-authors' insights in document revision tasks. This model is similar to the concept of templates in Click2Annotate. The tagging system developed by Hong et al uses two techniques, namely Click2Tag <ref type="bibr" coords="2,487.55,207.92,10.45,8.20" target="#b8">[9] </ref>and Fingerprint <ref type="bibr" coords="2,317.95,217.88,9.52,8.20" target="#b7">[8]</ref> , to help users generate and browse annotations for online documents . Click2Tag allows users to annotate documents using simple mouse clicks while the Fingerprint technique achieves the goal of " annotate once, appear anywhere " . Our approach achieves similar goals in the different environment of multidimensional visualization . Our work is also inspired by Document Cards <ref type="bibr" coords="2,515.32,267.70,13.74,8.20" target="#b12">[13]</ref>, which uses a mixture of images and key words to capture the semantics of a document in a small space <ref type="bibr" coords="2,431.55,287.63,13.74,8.20" target="#b12">[13]</ref>. The annotations generated by Click2Annotate can also be examined in a card representation with mixed visualization and semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">User Annotation Activities</head><p>In our previous research <ref type="bibr" coords="2,410.89,342.91,9.52,8.20" target="#b3">[4]</ref>, we conducted a set of experiments, user studies, and interviews to investigate users' annotation activities . Since the proposed AIE approach is inspired by the conclusions of these studies, we briefly introduce the studies and their conclusions here. In our experiments, we first conducted a literature survey on multidimensional visualization papers. It led to a rough fact categorization for facts from multidimensional data. We then manually classified a large number of annotations posted on Many-Eyes <ref type="bibr" coords="2,527.09,423.33,14.94,8.20" target="#b13">[14] </ref>into the initial fact categories and refined the categorization according to the results. After that, we conducted a user study in which users manually classified Many-Eyes annotations into the categories we provided. The task completion time and consistency of the classification results among the users were analyzed. Finally, we interviewed 16 experts from a variety of domains to learn what kinds of facts from multidimensional data they were interested in and how they annotated these facts. Three major conclusions were drawn from the above research. First, the experiments and interviews showed that most facts discovered from multidimensional data fall into multiple categories. It suggested that we can predict the types of facts that are most frequently sought. Second, the user study showed that users could effectively and efficiently classify facts into the categories we identified . It suggested that users can help determine the types of facts. Third, the interviews showed that the same set of information is often used to annotate facts falling into the same category, despite the fact that the experts we interviewed were from a wide variety of domains. It suggested that information to be annotated for popular fact types is predictable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CLICK2ANNOTATE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Approach Overview</head><p> Click2Annotate is implemented in ManyInsights, a novel multidimensional visualization system we propose. It is developed using Flex <ref type="bibr" coords="2,336.51,693.87,9.52,8.20" target="#b0">[1]</ref> . ManyInsights allows users to seek insights from scatterplots and parallel coordinates <ref type="bibr" coords="2,424.43,703.83,14.94,8.20" target="#b9">[10] </ref> and annotate, browse, and examine them. We briefly describe how users use Click2Annotate to annotate an insight. When a user discovers a fact of interest during the visual exploration, such as the cluster shown in <ref type="figure" coords="3,205.93,324.57,29.59,8.20" target="#fig_0">Figure 1</ref>(a), she brushes the relevant data (see <ref type="figure" coords="3,133.23,334.53,29.42,8.20" target="#fig_0">Figure 1</ref>(a-1)), specifies the dimensions (see <ref type="figure" coords="3,54.00,344.50,29.24,8.20" target="#fig_0">Figure 1</ref>(a-2)), judges the type of the fact, and selects the template for this type by a mouse click (see <ref type="figure" coords="3,180.26,354.46,29.19,8.20" target="#fig_0">Figure 1</ref>(a-3)). The system will then automatically create an annotation based on the template and present the annotation to the user (see <ref type="figure" coords="3,197.33,374.39,29.82,8.20" target="#fig_0">Figure 1</ref> (b)). The user reviews the annotation and interactively improves it, such as typing domain-related information and her evaluations (this step can be customized for individual applications to increase the level of automation ). Since mouse clicks rather than intensive typing effort are required from the user to accomplish the majority of the annotation process, our approach is named Click2Annotate. In the following sections, we introduce annotation templates, semi-automatic annotation generation, and interactive annotation review and modification in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Annotation Templates</head><p>Annotation templates are the key components of Click2Annotate. Each annotation template is associated with a fact type. It tells the system what information needs to be retrieved from the data and how to generate a semantic-rich annotation for this type of fact. A template can be either pre-defined or user-defined. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pre-Defined Templates</head><p>Click2Annotate provides pre-defined templates for popular fact types detected from our previous studies <ref type="bibr" coords="3,208.62,574.33,9.52,8.20" target="#b3">[4]</ref>. The templates are generated with the following steps. Determining Popular Fact Types: Six fact types, namely cluster , outlier, rank, difference, correlation, and compound fact, are determined to be popular fact types. Facts of these types were frequently posted on Many-Eyes <ref type="bibr" coords="3,161.60,624.14,10.45,8.20" target="#b1">[2] </ref>as revealed by our experiment <ref type="bibr" coords="3,281.35,624.14,9.52,8.20" target="#b3">[4]</ref>. Their definitions are self-explained by the type names. These fact types are further classified into three categories, namely dimensionoriented facts, data item-oriented facts, and compound facts. Fact types within the same category share common features. Dimensionoriented facts, such as correlation, describe relationships of dimensions . Data item-oriented facts, such as cluster, outlier, rank, and difference, describe clusters, anomalies, patterns, and relationships of data items. Compound facts describe relationships among multiple facts, such as that fact A is related to fact B. The fact type hierarchy guides the generation of the templates by extracting the common features among the fact types in the same category. It also guides the use of annotations in insight management activities (see Section 5.1 for an example). Predicting Information in Annotations: The templates tell the system what information needs to be retrieved from the data for generating an annotation. We predict such information for each pre-defined template based on the results of our domain expert in- terviews <ref type="bibr" coords="3,350.97,554.40,9.52,8.20" target="#b3">[4]</ref> . In these interviews, the experts reported what information they used to annotate the facts. We summarized the results and got an attribute list for each popular fact type. The percentage of the experts that used an attribute to annotate each type of facts was calculated. For example, <ref type="figure" coords="3,448.73,594.24,30.81,8.20" target="#fig_1">Figure 2</ref>shows how often the attributes listed were used to annotate a cluster by the experts. As shown in <ref type="figure" coords="3,376.54,614.17,29.61,8.20" target="#fig_1">Figure 2</ref>, an annotation of a cluster often consists of the following attributes, in descending order of their frequency: Type (the fact type, such as " cluster " in this example); Time (when the cluster was discovered); Dataset (the dataset where the This is a group of xx items that are (extremely/very/slightly) different from the others in dimensions xx. Rank Type, Dimensions, Items, Value, Rank Dataset Item xx ranks xx in dimension xx in dataset xx from highest to lowest. Difference Type, Dimensions Items, Difference , Distance Dataset There is an/a (extremely large/large/slightly) difference between item xx and item xx in dimension xx. The value of item xx is higher by xx. Correlation Type, Dimensions Coefficient Dataset For xx percent of data items in dataset xx, the higher their value in dimension xx, the (higher/lower) their values in dimension xx. Compound Pointers to the related facts N/A This is about insights xx. and Mean(O) (the mean of data outside the cluster). According to the statistics, we identify three categories of attributes for each popular fact type (shown in yellow, blue, and green in <ref type="figure" coords="4,86.83,259.82,29.62,8.20" target="#fig_1">Figure 2</ref> ). They include: general attributes, such as Author , Time, Title, and Rate, which are important information for all types of insights and they are not directly related to the data; context attributes, such as Size, Items, and Mean(I), which are frequently used to describe the content of a certain type of facts; and context attributes, such as Mean(O), which are frequently used to capture the context of a certain type of facts. <ref type="figure" coords="4,63.96,330.05,26.82,8.20">Table 1</ref> summarizes the frequently used content and context attributes (with percentage ≥ 50%) of the popular fact types. They are semantic-rich information widely used by the experts to describe facts. We include them into the templates of the types together with all the general attributes. Users can customize a template (refer to Section 3.2.2) if the information they desire is not included in the template. Shaping the Templates: From the previous step, a fairly large amount of attributes are determined to be included into the templates . How should they be presented to users so that the users can enjoy reading the annotations and grasp their content effectively and efficiently? To address this problem, we conducted a user study. First, we designed three template interface prototypes with the following goals: (1) Completeness: all attributes should be represented ; (2) Clearness: the information should be easy to read and understand; and (3) Briefness: key information of the attributes should be easily accessed. In Prototype A, each attribute was represented as a form entry, such as " cluster radius: 0.1 " for the radius of a cluster. Prototype B employed a narrative annotation that represents information textually <ref type="bibr" coords="4,416.39,239.42,9.52,8.20" target="#b4">[5]</ref>. All the attributes were presented in sentences that described the attributes using natural language. For example, the entry " cluster radius: 0.1 " in the previous example was expressed as " The items in this group have very similar values " . Prototype C used a mixed design. The general attributes were represented as form entries while the content and context attributes were represented textually. Two annotations were generated for each of the fact types, including cluster, outlier, and correlation following each prototype. A total of 18 annotations were generated. <ref type="figure" coords="4,534.60,319.13,23.41,8.20;5,54.00,57.76,24.89,8.20">Figurescribes</ref>the size, the quality (indicated by radius), the dimension labels, and the dataset of the cluster: this is a group of 8 data items that have extremely similar values in dimensions A and B in dataset NFL. The information in pink is automatically extracted by the system after a user selected the data and the template. We summarize examples of narrative sentences for the popular fact types in <ref type="figure" coords="5,84.39,117.54,25.31,8.20">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">User-Defined Templates</head><p>Although a set of templates is pre-defined for most popular fact types, it is impossible to predict all useful fact types as well as all possible attributes for each fact type. Therefore, Click2Annotate allows users to interactively modify pre-defined templates or create new templates from scratch. <ref type="figure" coords="5,63.96,404.49,30.41,8.20" target="#fig_4">Figure 4</ref>shows an example of how to create a new template for a user-defined fact type named extreme. In this window, there a lists of available attributes (see <ref type="figure" coords="5,172.42,424.42,30.14,8.20" target="#fig_4">Figure 4</ref> (1)), including all possible context and content attributes reported by the domain experts in the interviews <ref type="bibr" coords="5,116.82,444.34,9.52,8.20" target="#b3">[4]</ref>. They can be added to the template attributes list (see <ref type="figure" coords="5,83.88,454.31,29.01,8.20" target="#fig_4">Figure 4</ref> (2)). In this example, the general attributes are automatically included and the maximum and minimum of the relevant dimension are manually added into the template. The narrative sentences of these attributes are represented in the annotation area, providing a preview for the annotations generated by this template (see <ref type="figure" coords="5,70.41,504.11,28.75,8.20" target="#fig_4">Figure 4</ref>(3)). Users can interactively modify these sentences or change their order. The modification of an existing template can be accomplished in the same interface. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Semi-Automatic Annotation Generation</head><p>Click2Annotate semi-automatically generates annotations based on pre-defined or user-defined templates. To generate an annotation, users brush the relevant data items and dimensions and select a template according to the type of the fact. To allow quick access to the templates, a list of buttons are provided in a separated panel (see <ref type="figure" coords="5,54.00,604.21,29.67,8.20" target="#fig_0">Figure 1</ref>(a-3)), which is shared by all created views. Each button corresponds to a template. Users can add or remove buttons from the panel so that it only contains the buttons for templates they need. The users click on a button to select a template. After the template is selected, the system will automatically fetch information from the data and encode it to fill the incomplete information in the template. Thus, an annotation is automatically generated. The above process does not apply to compound facts because they contain pointers to other facts. To annotate a compound fact, an interactive approach is employed. In particular, users first open a compound fact annotation dialog (see <ref type="figure" coords="5,191.17,703.84,29.14,8.20" target="#fig_0">Figure 1</ref>(c)) by clicking on a " compound " button and then use drag-and-drop interactions to add the flags of desired insights (<ref type="figure" coords="5,423.81,57.76,28.76,8.20" target="#fig_8">Figure 8</ref>(a-1)) to the dialog. After an insight is added, its title will be displayed in the dialog (see <ref type="figure" coords="5,534.57,67.72,23.41,8.20;5,317.95,77.69,3.39,8.20" target="#fig_0">Figure  1</ref>(c-4)). It is hyperlinked to the related annotation so that users can click on it to examine the annotation in detail (see <ref type="figure" coords="5,517.57,87.65,29.60,8.20" target="#fig_0">Figure 1</ref> (c- 5)). Users can add insights into the dialog and type their notes to complete the annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Annotation Review and Modification</head><p>After an annotation is generated by the system, it will be presented to users in an annotation window (see <ref type="figure" coords="5,453.51,147.90,28.85,8.20" target="#fig_5">Figure 5</ref>(a)) within which the annotation can be reviewed and improved by the users. The annotation window directly mimics the design of Prototype C with a thumbnail added. The thumbnail is a screenshot that captures the visualization at the moment when the fact was discovered to help users recall this insight. The general attributes are represented below the thumbnail, followed by a set of sentences that textually represent the context and content attributes. If users are not satisfied with the automatically generated annotation , they can interactively improve it. In particular, the users can open a statistics window (see <ref type="figure" coords="5,427.04,534.47,29.63,8.20" target="#fig_5">Figure 5</ref>(c)) which presents a list of all available statistics about the fact and the whole dataset, and a list of the information that has already been included in the current annotation. Users can use drag-and-drop interactions to add or remove statistics into or from the annotation and adjust the order of their presentations in the annotation. The statistics in the annotation is represented textually according to pre-defined templates. Users can manually customize the text representations if they are not satisfied with the pre-defined ones. For example, in <ref type="figure" coords="5,515.91,614.16,28.77,8.20" target="#fig_5">Figure 5</ref>(c), a user drags and drops the mean value of the dimension population density to the annotation. A new sentence that conveys this mean value is then automatically added to the annotation, as shown in the sentence with the red underline in <ref type="figure" coords="5,441.47,654.02,28.98,8.20" target="#fig_5">Figure 5</ref>(a). The automatically generated annotation only captures the fact of an insight. To allow users to record the knowledge base and subjective evaluations of the insight, an interactive tagging function is supported. In particular, a user can click on a button in the annotation window to trigger a tagging interface (see <ref type="figure" coords="5,511.92,703.83,29.26,8.20" target="#fig_5">Figure 5</ref>(b)). Through the interface, the user can create tags or select existing tags to annotate the insight. A tag is generated once and reused later on. Thus users can type frequently used information once, save it as a tag, and reuse the tag in the future with light human effort. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">USER STUDY</head><p>A formal user study has been conducted to evaluate how Click2Annotate helped users generate annotations and if the generated annotations were understandable. The study was a 2×2 (system types×datasets) between-subjects design. We compared two systems: ManyInsights, which supported Click2Annotate, and a simple system, which provided users a text editor similar to those commonly found in many visualization systems to manually type notes for annotating insights. Our hypotheses were: (1) ManyInsights will reduce the time spent on annotating; and (2) annotations generated by ManyInsights will reduce the time-cost and errors for understanding the annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Insights</head><p>Two datasets were used in the user study: a small dataset (51 items, 4 dimensions) on state health measures and a large dataset (279 items, 10 dimensions) on the US census data. Before the user study, we manually extracted six insights, including a cluster, an outlier, a rank, a difference, a correlation, and a compound fact, from each dataset. The compound fact was about the difference between two clusters. All extracted insights were used in the user tasks described in the next section. The numbers of data items and dimensions involved in the insights were controlled according to the size of dataset. For example, a cluster in the large dataset had more data items and dimensions involved. This made annotating and comprehending insights in the large dataset more difficult. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tasks</head><p>The experiment included two sessions. Annotation session: Each participant was asked to annotate the six insights for each dataset on a computer. The insight was annotated one by one. For each insight, the fact type was explicated and the relevant data was highlighted in a parallel coordinates or scatterplot view of the dataset according to the number of dimensions involved in the insight. The participant was asked to record all possible information that could help them comprehend the insight. The task completion time was recorded. Comprehension session: Each participant was asked to understand the annotations generated by other participants. There were six tasks to complete for each dataset, each of which for an insight used in the annotation session. In each task, an annotation (randomly picked from annotations generated by other participants and text only) was provided along with four images on paper. One image was the screenshot of the view with the insight described by the annotation highlighted, namely the original view provided to the participants when the insight was annotated. The other three images presented different views, such as the same display with other data highlighted or a different display with a similar pattern. The participant was asked to find the view with the insight described by the highlighted annotation. The task completion time was recorded. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis Condition and Procedure</head><p>A total of 8 subjects (5 male and 3 female) participated in the study. All of them were graduate students and had strong English writing and reading abilities. Before the study, the subjects were evenly divided into two groups. One group of subjects used ManyInsights and the other group used the simple system. The same datasets and tasks were used in both groups. The subjects took the experiment one by one on the same computer following the same process. In the annotation session, the same views were used in both groups. When making annotations, participants were allowed to read dimension names, data names, and data values on the visualizations . In the simple system, participants used the text editor to type notes for annotating insights. In ManyInsights, participants were allowed to edit existing templates and interactively modify automatically generated annotations. At the beginning of the study, a tutorial was provided by an instructor to explain the definition of each fact type in the insights and show examples of how to annotate an insight. The annotation session was conducted right after the tutorial. The comprehension session was conducted three months after the annotation session. In each session, there were first practical tasks, second experimental tasks (the small dataset followed by the large dataset), and then survey questions specific to that session. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We present two types of results from the study, namely quantitative data (completion time and correctness) captured through the system and the subjective preferences reported from survey questions, in the following sections respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Task Completion Time and Correctness</head><p> The comparisons of the average completion time for annotating insights are shown in <ref type="figure" coords="6,389.03,282.07,29.28,8.20" target="#fig_6">Figure 6</ref>(a) (for the small dataset) and 6(b) (for the large dataset). <ref type="figure" coords="6,384.05,292.03,29.38,8.20" target="#fig_6">Figure 6</ref> (b) reveals the difficulty the subjects encountered in making annotations in the large dataset using the simple system, especially when annotating the cluster and rank. We observed that subjects had difficulty in manually summarizing information from complex data, such as estimating the size of a big cluster and the rank of a data item in a large dataset. Besides, in the simple system, the time the participants spent on determining the information to be recorded was often more than the time they spent on typing the note. Click2Annotate showed its strength in pre-defining the most essential information for insights and automatically capturing this information. Therefore, our first hypothesis was validated.  The comparisons of the average completion time for the comprehension tasks are shown in <ref type="figure" coords="6,418.80,644.05,29.79,8.20" target="#fig_7">Figure 7</ref>(a) (for the small dataset) and 7(b) (for the large dataset). The figures show that the participants were faster in selecting the views when reading the annotations generated by ManyInsights, especially for the large dataset. The average correct answer rate for all tasks was 89.6% for ManyInsights (with standard deviation 7%) and only 75.0% for the simple system (with standard deviation 11%). The result suggested that the subjects understood the ManyInsights annotations faster and better </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.0">2.5</head><p> than the manually generated annotations. Thus our second hypothesis was validated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Subjective Preferences</head><p>At the end of each session, the participant was asked to answer a set of survey questions each of which was answered in a 7-point Likert scale (0=strongly disagree, 6=strongly agree). A total of 10 questions were provided. The average score for ManyInsights was 4.8 and only 2.7 for the simple system. <ref type="figure" coords="7,220.96,490.22,27.21,8.20">Table 2</ref>summarizes the pair-wise comparisons of the questions where significant differences were detected. The significant differences indicate that ManyInsights was judged to be more helpful than the simple system in annotating insights by the participants. Annotations generated by ManyInsights were judged to be more helpful in understanding insights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">USE CASE: INSIGHT MANAGEMENT</head><p>The semantic-rich information automatically captured by Click2Annotate provides rich potentials for enhancing various insight management activities, such as insight retrieval, browsing, and association. As a proof of the concept, we present scented insight browsing and faceted insight retrieval as concrete examples in this paper. These functions have been implemented in ManyInsights. In the following sections, we briefly introduce them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Scented Insight Browsing</head><p>After insights are annotated, effective browsing approaches should be provided to help users review and reuse them. Towards this goal, we propose a scented insight browsing approach (see <ref type="figure" coords="7,250.49,693.87,29.63,8.20" target="#fig_8">Figure 8</ref>(a)) based on Click2Annotate. If a user turns on the scented browsing mode, insight flags are attached to the visualizations, not only the views where the insights were captured, but also other views where the relevant data items/dimensions of the insights can be observed. Users can retrieve an insight from any view where its flag is displayed by clicking on the flag. Compared to existing approaches that require users to manually mark insights on the visualizations <ref type="bibr" coords="7,317.95,107.57,9.52,8.20" target="#b6">[7]</ref>, our approach has several benefits. First, based on the pre-defined essential information for different fact types, our approach automatically marks different types of insights in different ways to avoid cluttering the display. For ex- ample, <ref type="figure" coords="7,344.86,148.09,29.65,8.20" target="#fig_8">Figure 8</ref> (a) shows a scatterplot with multiple annotated insights flagged. In this figure, the flags of data item-oriented facts are attached to their data items (see <ref type="figure" coords="7,450.64,168.02,29.77,8.20" target="#fig_8">Figure 8</ref>(a-1)) while the flags of dimension-oriented facts are attached to their dimensions (see <ref type="figure" coords="7,317.95,187.95,30.12,8.20" target="#fig_8">Figure 8</ref>(a-2)). In systems with manually generated annotations, users have to draw marks carefully to achieve similar effects. Second, insights can be flagged in any display where the relevant data items/dimensions of them can be observed, not only the visualization where the insights were discovered. Thus it is an " annotate once, appear anywhere " approach <ref type="bibr" coords="7,444.08,238.43,9.52,8.20" target="#b7">[8]</ref>. For example, an insight of dimension correlation can be marked in any of the visualizations where any of the dimensions involved is displayed. This feature allows users to access relevant insights anywhere during their visual exploration process, without going back to the previous views. Thus the visual exploration becomes more convenient and flexible. Such an " annotate once, appear anywhere " approach is not easily supported by manual annotation approaches because of the lack of accuracy and formalization of manually generated annotations. In addition, the scented insight browsing approach can work together with the faceted insight retrieval approach described below. In particular, users can interactively select the insights they want to flag using criteria such as the fact types and the dimensions involved (see <ref type="figure" coords="7,362.02,368.61,29.57,8.20" target="#fig_8">Figure 8</ref>(a-3)). In this way the users can display only flags of insights of interest in the display to reduce clutter. Again, this benefit is brought by the accuracy and formalization of the automatically generated semantic-rich annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Faceted Insight Retrieval</head><p>As the number of annotated insights grows larger, effective insight retrieval becomes essential. Faceted search <ref type="bibr" coords="7,476.03,443.47,13.74,8.20" target="#b16">[17]</ref> , a popular searching approach used in mass online markets, has shown its efficiency and flexibility in finding items that can be aggregated based on multiple attributes. Since the annotations generated by Click2Annotate can also be aggregated based on multiple attributes, faceted search can be applied to help users retrieve insights according to their specific interest. In particular, a set of common attributes shared by multiple templates, including author, time, rate, title, fact type, dataset, dimensions, and tags, are used as faceted filters for searching insights in ManyInsights. Users can search insights in any order using these filters through the faceted search interface provided by ManyInsights (see <ref type="figure" coords="7,386.06,553.07,29.01,8.20" target="#fig_8">Figure 8</ref>(b)). For example, <ref type="figure" coords="7,379.25,563.69,30.29,8.20" target="#fig_8">Figure 8</ref> (b) shows how a user retrieves the cluster annotated in <ref type="figure" coords="7,375.33,573.66,29.16,8.20" target="#fig_0">Figure 1</ref>(b) using faceted search. First, she uses the fact type " cluster " to filter out insights that are not clusters. Second, she narrows down the results using the dataset name " census " (see <ref type="figure" coords="7,317.95,603.54,29.27,8.20" target="#fig_8">Figure 8</ref>(b-4)). The search results dynamically roll over the screen from left to right. Inspired by <ref type="bibr" coords="7,424.60,613.51,13.74,8.20" target="#b12">[13]</ref>, each insight is represented as an annotation card which summarizes the insight using a visualization thumbnail and a short sentence that captures the essential information of the insight. The user can sort the search results by different criteria, such as rate and title. When the user clicks on an annotation card, the annotation will be presented in full detail in an annotation pop-up window. Once the user finds interesting insights, she can export them to XML files for reporting or sharing. Note that it is difficult to conduct faceted search on manually generated annotations since they are usually unformalized. It is also difficult to conduct faceted search based on AIE at the action level or the event level since the lack of semantic richness. The annotations generated by Click2Annotate, on the other hand, are based on formalized templates and provide a set of attributes with rich semantics that can be used to classify insights in dimensions that are meaningful to users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Insight Externalization is a critical requirement for an effective DMPS process. In this paper, we propose a novel approach that allows users to conduct semi-automatic insight externalization at the sub-task level. We also present a fully working prototype of this approach named Click2Annotate. Our formal user study showed that Click2Annotate reduced the human effort involved in IE and produced annotations that were easy to understand. Besides , Click2Annotate semi-automatically captures semantic-rich information that can be used in a variety of insight management activities, as illustrated in our use cases. Click2Annotate is our first step toward a whole insight management solution, which includes insight browsing and retrieval, insight network, insight sharing/exporting in collaborative visualization , and insight recommendation and notification. We will work on these functions based on Click2Annotate. To support a wider range of data types, we will also extend Click2Annotate to trees, graphs, text, and geospatial data. In addition, more user studies and experiments will be conducted. For example, we will compare the performance of users who are familiar / unfamiliar with the datasets visualized. We will also investigate the effectiveness and efficiency of Click2Annotate with different design options in real analytical reasoning processes. The effectiveness of our current insight management approaches on handling large amounts of insights will be also investigated. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,54.00,269.50,504.03,7.67;3,54.00,278.97,355.88,7.67"><head>Figure 1: </head><figDesc>Figure 1: Semi-automatic annotation generation using Click2Annotate. (a) A scatterplot with a cluster in it and the annotation process. (b) The automatically generated annotation for the cluster. (c) The annotation generated for a compound fact. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,328.72,493.95,218.52,7.67"><head>Figure 2: </head><figDesc>Figure 2: Frequencies of attributes used in cluster annotations </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,317.95,654.02,240.02,8.20;3,317.95,663.98,240.03,8.20;3,317.95,673.95,240.05,8.20;3,317.95,683.91,240.06,8.20;3,317.95,693.87,240.05,8.20;3,317.95,703.84,240.05,8.20;3,317.95,713.80,240.03,8.20"><head></head><figDesc>ter was discovered); Title (the title of the annotation); Dimensions (the dimension names of the subspace where the cluster existed ); Size (the number of items in the cluster); Rate (users' subjective evaluation); Author (who discovered the cluster); Extreme(I) (the extreme of data inside the cluster); Radius (the radius of the cluster ); Mean(I) (the mean of data inside the cluster); Items (the data item names of the cluster); Value (the data values of the cluster); </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,98.97,641.59,150.10,7.67"><head>Figure 3: </head><figDesc>Figure 3: Examples of prototypes A and B. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,70.34,383.22,207.35,7.67"><head>Figure 4: </head><figDesc>Figure 4: Interactive generation of a user-defined template. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,317.95,416.18,240.04,7.67;5,317.95,425.65,240.04,7.67;5,317.95,435.11,60.96,7.67"><head>Figure 5: </head><figDesc> Figure 5: The review, tagging, and modification of generated annotations . (a) A modified annotation. (b) The tagging interface. (c) The statistics window. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,366.40,610.29,143.17,7.67"><head>Figure 6: </head><figDesc>Figure 6: Results of Annotation sessions </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,93.37,243.61,161.32,7.67"><head>Figure 7: </head><figDesc>Figure 7: Results of Comprehension sessions </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="8,132.27,237.59,347.44,7.67"><head>Figure 8: </head><figDesc>Figure 8: Insight management activities. (a) Scented insight browsing. (b) Faceted insight retrieval. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true" coords="4,54.00,63.95,503.99,73.90"><figDesc coords="4,54.00,63.95,503.99,7.67;4,54.00,73.41,221.51,7.67">Table 1: Content attributes, context attributes, and narrative sentences for popular fact types. The attributes with " I " are about the data inside the fact. The attributes with " O " are about the data outside the fact.</figDesc><table coords="4,71.22,91.96,469.54,45.89">Type 
Content Attributes 
Context Attributes 
Narrative Sentences 
Cluster 
Type, Dimensions, Size, Ex-
treme(I), Radius, Mean(I) 

Dataset 
This is a group of xx items that have (extremely/very/slightly) similar 
values in dimensions xx in dataset xx. 
Outlier 
Type, Dimensions, Size, Items, 
Mean(I) 

Dataset, 
Mean(O), 
Distance 

</table></figure>

			<note place="foot" n="3"> shows examples of the generated cluster annotations for prototype A and B. The annotation of the same content for prototype C is shown in Figure 1(b). Twenty users who had good experiences with reading annotations in visualizations participated in the study one by one. The subject was asked to grade prototypes A, B, and C according to the following three criteria: (1) the annotations are pleasant to read; (2) the values of the attributes in the annotations can be quickly perceived ; and (3) it is easy to compare facts of the same type and facts of different types. A 7-point scale was used for the rating (0=strongly disagree, 6=strongly agree). User feedbacks were also collected. The results showed that there was a stronger preference to Prototype C. In particular, the average scores of Prototype A, B, and C in the first criterion were 2.8, 3.4, and 4.8 respectively; the average scores in the second criterion were 3.8, 3.2, and 4.4 respectively; and the average scores in the third criterion were 4.4, 3.0, and 4.2 respectively. According to user feedback, Prototype C had the following advantages: First, it represented the general attributes as form entries and thus reduced the number of sentences in the narrative annotation . Users had no difficulty in understanding general attributes, such as author and dataset name, in the form entries. Second, it represented context and content attributes using natural language, which makes them easy to understand by users who were not familiar with terms such as cluster radius. Therefore, Prototype C is used in Click2Annotate for shaping the templates. Encoding Attributes Using Natural Language: The context and content attributes are encoded into human-readable sentences in the templates to compose narrative annotations. Our encoding process is similar to the one described in [5] but improved from three aspects: First, multiple context and content attributes can be encoded in one sentence. This produces a less wordy annotation. Second, the numerical attribute values (e.g., the radius value) are explained in an easy to understand manner (e.g., &quot; very similar &quot; ). Third, the key information in the sentences, such as dimension names, is automatically highlighted and hyperlinked so that users can easily access related insights sharing the common content. For example, the narrative of a cluster may start by a sentence that</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS </head><p>This work was performed with partial support from the DHS Visual Analytics for Command, Control, and Interoperability (VACCINE) Center of Excellence, under the auspices of the SouthEast Regional Visual Analytics Center. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,72.26,676.63,139.34,7.29"  xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Flex</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,686.09,129.80,7.29"  xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName>
				<surname>Many-Eyes</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,695.56,221.78,7.29;8,72.26,705.02,221.78,7.29;8,72.26,714.48,139.55,7.29"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Amar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Eagan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>. IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,269.62,221.77,7.29;8,336.21,279.09,221.79,7.29;8,336.21,288.56,107.46,7.29"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward effective insight management in visual analytics systems</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Pacific Visualization</title>
		<meeting>. IEEE Symposium on Pacific Visualization</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,298.02,221.79,7.29;8,336.21,307.48,221.78,7.29;8,336.21,316.94,105.81,7.29"  xml:id="b4">
	<analytic>
		<title level="a" type="main">In search of personal information: Narrative-based interfaces</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Goncalves</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jorge</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Conference on Intelligent User Interfaces</title>
		<meeting>. ACM Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,326.41,221.76,7.29;8,336.21,335.88,221.78,7.29;8,336.21,345.34,150.47,7.29"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Characterizing users visual analytic activity for insight provenance</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gotz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Visual Analytics Science and Technology</title>
		<meeting>. IEEE Symposium on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,354.81,221.79,7.29;8,336.21,364.26,221.78,7.29;8,336.21,373.73,221.77,7.20;8,336.21,383.20,77.94,7.29"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Voyagers and voyeurs: Supporting asynchronous collaborative information visualization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Heer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Viegas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wattenberg</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1029" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,392.66,221.80,7.29;8,336.21,402.13,221.78,7.29;8,336.21,411.59,221.77,7.20;8,336.21,421.05,77.94,7.29"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Annotate once, appear anywhere: Collective foraging for snippets of interest using paragraph fingerprinting</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Chi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1791" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,430.52,221.79,7.29;8,336.21,439.98,221.79,7.29;8,336.21,449.45,201.43,7.29"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Spartag.us: A low cost tagging system for foraging of web content</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Hong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Chi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Budiu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pirolli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Nelson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Working Conference on Advanced Visual Interfaces</title>
		<meeting>. Working Conference on Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,458.92,221.80,7.29;8,336.21,468.37,204.43,7.29"  xml:id="b9">
	<monogr>
		<title level="m" type="main">The plane with parallel coordinates. Special Issue on Computational Geometry, The Visual Computer</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Inselberg</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="69" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,477.84,221.78,7.29;8,336.21,487.30,221.78,7.29;8,336.21,496.77,123.39,7.29"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Casual information visualization: Depictions of data in everyday life</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Pousman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>. IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,506.24,221.80,7.29;8,336.21,515.69,221.78,7.29;8,336.21,525.16,214.28,7.29"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Supporting the analytical reasoning process in information visualization</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Shrinivasan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,534.62,221.78,7.29;8,336.21,544.09,221.78,7.29;8,336.21,553.56,221.79,7.29;8,336.21,563.02,57.79,7.29"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Document cards: A top trumps visualization for documents</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Strobelt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Oelke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Rohrdantz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stoffel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Deussen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>. IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,572.48,221.80,7.29;8,336.21,581.95,221.78,7.29;8,336.21,591.41,198.44,7.29"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Manyeyes: a site for visualization at internet scale</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Viegas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Wattenberg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Van Ham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kriss</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mckeon</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symposium on Information Visualization</title>
		<meeting>. IEEE Symposium on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,600.88,221.79,7.29;8,336.21,610.34,221.79,7.29;8,336.21,619.80,49.81,7.29"  xml:id="b14">
	<analytic>
		<title level="a" type="main">A problem-oriented classification of visualization technique</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wehrend</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Lewis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st Conference on Visualization</title>
		<meeting>. 1st Conference on Visualization</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="139" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,629.27,221.78,7.29;8,336.21,638.73,221.79,7.29;8,336.21,648.20,221.80,7.29;8,336.21,657.66,17.94,7.29"  xml:id="b15">
	<analytic>
		<title level="a" type="main">The sandbox for analysis: Concepts and methods</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Wright</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Schroh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Proulx</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Skaburskis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Cort</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,667.13,221.80,7.29;8,336.21,676.59,221.78,7.29;8,336.21,686.05,186.61,7.29"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Faceted metadata for image search and browsing</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Swearingen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hearst</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.20,695.52,221.79,7.29;8,336.21,704.99,221.78,7.29;8,336.21,714.45,136.59,7.29"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-authoring with structured annotations</title>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Zheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Booth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Mcgrenere</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>. ACM SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
