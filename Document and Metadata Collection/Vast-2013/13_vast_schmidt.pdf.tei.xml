<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VAICo: Visual Analysis for Image Comparison</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Johanna</forename>
								<surname>Schmidt</surname>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">M</forename>
								<forename type="middle">Eduard</forename>
								<surname>Gröllergr¨gröller</surname>
								<roleName>Member, Ieee Computer Society</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Stefan</forename>
								<surname>Bruckner</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
							<affiliation>
								<orgName type="department">Computer Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VAICo: Visual Analysis for Image Comparison</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Comparative visualization</term>
					<term>focus+context visualization</term>
					<term>image set comparison</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—Scientists, engineers, and analysts are confronted with ever larger and more complex sets of data, whose analysis poses special challenges. In many situations it is necessary to compare two or more datasets. Hence there is a need for comparative visualization tools to help analyze differences or similarities among datasets. In this paper an approach for comparative visualization for sets of images is presented. Well-established techniques for comparing images frequently place them side-by-side. A major drawback of such approaches is that they do not scale well. Other image comparison methods encode differences in images by abstract parameters like color. In this case information about the underlying image data gets lost. This paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information, but also allows the detailed analysis of subtle variations. Our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy. The results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features. We demonstrate the flexibility of our approach by applying it to multiple distinct domains.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> A common task in data analysis is to compare two or more related results to discover their differences or similarities. In addition to choosing the appropriate similarity measure, it is also important to make use of appropriate techniques to visualize the comparison results. Comparative visualization refers to the process of visually depicting differences and similarities when comparing multiple datasets <ref type="bibr" coords="1,235.96,348.39,13.74,8.02" target="#b25">[26]</ref>. In the past few years an increasing number of comparative visualization systems have been developed. All together, these systems demonstrate that there is a strong demand for comparison tasks in various different domains caused by the ever-growing amount of acquired data. For example, in biology additional information about certain species can be gained by comparing multiple genetic sequences. In the medical domain, deviations in MRI or sonographic images can indicate anomalies which should be further inspected. In image processing , results of different edge detection or segmentation algorithms have to be compared. In visualization and rendering, results have to be compared against each other to evaluate variances that are caused by different parameter settings. Comparative visualization tools have to meet certain requirements to be appropriate for a variety of different datasets. One important issue is scalability pertaining to dataset size (i.e., number of items in the dataset). In the case of 2D image comparison, many approaches place the images to be compared side-by-side or in multiple views, or they overlay images semi-transparently. However, due to limitations in the human perceptual capacity as well as due to limited screen space, such comparative visualizations do not scale well. These tools are only suitable for comparing a limited number of images. This is a problem, for example, for the investigation of biological data, where datasets often are based on the analysis of several hundreds of specimens. Another important issue when developing comparative visualization techniques is how to provide information about the underlying original data. In many approaches differences between datasets are @BULLET Johanna Schmidt, Vienna University of Technology, Austria. E-mail: jschmidt@cg.tuwien.ac.at. @BULLET M. Eduard Gröller, Vienna University of Technology, Austria. E-mail: groeller@cg.tuwien.ac.at. @BULLET Stefan Bruckner, University of Bergen, Norway. E-mail: shows two pictures that look similar, but in fact exhibit local changes. A very common way to compare them is by placing them side-by-side (a). To help the user to find the variations more quickly, a difference image (b) can be computed. In this illustration information about the similar parts of the data gets lost. Another possibility is to highlight differences by certain patterns (e.g., colored circles) as shown in (c). In this case similar parts of the data are still visible; however, no further information is provided on how the differences are structured. mapped to visual attributes such as colored patterns (<ref type="figure" coords="1,491.79,501.21,28.96,8.02" target="#fig_0">Figure 1</ref> ). Although this clearly highlights differences and similarities between the datasets, it hides the original data that has been used for calculation. Having knowledge about the original data allows us to identify patterns in the datasets (i.e., to detect outliers). We believe that an interactive visualization providing insight into the underlying raw data can lead to a better overall understanding of the studied datasets. We propose a new comparative visualization approach which preserves contextual information while allowing us a detailed analysis of the variations between datasets. Our approach provides effective means for examining local differences in a large image dataset. It supports users in gaining a better overview of different image characteristics and allows them to further investigate individual local differences. The main features of our approach are: @BULLET Scalability: Unlike previous approaches, the proposed visualization technique is specifically designed to compare large sets of images (i.e., hundreds of images in one set). @BULLET Focus+Context <ref type="bibr" coords="1,370.42,684.15,9.71,8.02" target="#b4">[5]</ref> : Our comparative visualization approach provides an overview of the image differences (i.e., how much of the image space is affected) and allows users to drill-down on individual features. @BULLET Flexibility: Our approach is not targeted to a certain type of image and is not tied to any particular image comparison metric. Interactice Visual Analysis <ref type="figure" coords="2,22.50,137.61,18.98,7.37">Fig. 2</ref>. Overview of our comparative image visualization approach. The input data consists of a set of images which contain small local variations when compared to each other. To locate regions of differences (RoDs), an image comparison metric is applied (Section 3.1.1). In the next step hierarchical clustering is performed on the corresponding image data of every RoD. This classifies the differences according to their significance (Section 3.1.2). The results are then presented in an interactive comparative visualization which is described in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Images </head><p>The paper is organized as follows: Section 2 contains a survey of previous work related to the topic of comparative visualization, parameter studies, focus+context techniques, and hierarchical clustering. In Section 3 an overview of our visualization method is provided. The process of identifying and structuring differences in a set of images is described and the interactive visualization tools are introduced. The implementation details are discussed in Section 4 and results are presented in Section 5. We conducted a user study to evaluate our technique , which is discussed in Section 6. Advantages and limitations are outlined in Section 7. The paper is concluded in Section 8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the last years a great variety of systems and approaches have been developed in the field of comparative visualization. For analyzing different light intensities in renderings, Pang and Freeman <ref type="bibr" coords="2,257.94,337.92,14.94,8.02" target="#b26">[27] </ref>used color and other parameters like textures to highlight differences. Verma and Pang <ref type="bibr" coords="2,84.31,357.84,14.94,8.02" target="#b38">[38] </ref>implemented an approach for comparative flow visualization. To compare video data, Gareth and Chen <ref type="bibr" coords="2,234.60,367.80,14.94,8.02" target="#b12">[13] </ref>introduced an approach to visualize differences between video frames in 3D space. They treated a video sequence as a 3D volume and applied volume rendering to it. For analyzing biological data, Munzner et al. <ref type="bibr" coords="2,42.41,407.65,14.94,8.02" target="#b24">[25] </ref> provided a visualization technique for comparing large phylogenic trees, and Procter et al. <ref type="bibr" coords="2,142.62,417.62,14.94,8.02" target="#b29">[30] </ref>used comparative visualization to check multiple genetic sequences. In archeology objects are often classified by comparison. Masuda et al. <ref type="bibr" coords="2,176.62,437.54,14.94,8.02" target="#b20">[21] </ref> visually analyzed ancient Chinese bronze mirrors and their shape differences. Baudrier and Riffaud <ref type="bibr" coords="2,69.32,457.47,10.45,8.02" target="#b0">[1] </ref> proposed an approach for comparing ancient documents . For dealing with other data like office documents, Drucker et al. <ref type="bibr" coords="2,35.90,477.39,10.45,8.02" target="#b7">[8] </ref> presented a method to spot differences between multiple versions of a PowerPoint presentation. Piringer et al. <ref type="bibr" coords="2,208.44,487.36,14.94,8.02" target="#b28">[29] </ref>implemented an interactive approach for a comparative visual analysis of 2D function ensembles. Tory et al. <ref type="bibr" coords="2,123.81,507.28,14.94,8.02" target="#b36">[36] </ref> developed new techniques to visualize projects in construction management. All together, these systems show the demand for developing tools that explicitly support comparison tasks. However, the presented approaches are all targeted to a specific domain and to a certain type of dataset. Eler et al. <ref type="bibr" coords="2,222.74,547.13,14.94,8.02" target="#b10">[11] </ref>proposed a method to visually analyze image collections. Their visualization method allows feature-based grouping and classification of images, but does not provide means to further inspect individual features. Existing approaches for 2D comparative image visualization generally place the objects to be compared side-by-side or in multiple views <ref type="bibr" coords="2,46.49,606.91,13.74,8.02" target="#b37">[37]</ref> . Other approaches for 2D image comparison depict images to be compared in the same space. The simplest solution for this is blending (i.e., to overlay images semi-transparently). Kammerer et al. <ref type="bibr" coords="2,44.57,636.80,14.94,8.02" target="#b16">[17] </ref>used this method to spot differences between infrared and color images of ancient paintings. Other methods are color weaving introduced by Hagh-Shenas et al. <ref type="bibr" coords="2,151.54,656.72,14.94,8.02" target="#b14">[15] </ref>or attribute blocks proposed by Miller <ref type="bibr" coords="2,58.53,666.68,13.74,8.02" target="#b21">[22]</ref> . Many approaches use color to indicate differences between 2D images. Since this is a very simple and intuitive way to display differences, it can be applied to various domains. Hollingsworth et al. <ref type="bibr" coords="2,45.31,696.57,14.94,8.02" target="#b15">[16] </ref>used a specific colorization scheme for difference images to compare 2D gas chromatographies. Sahasrabudhe et al. <ref type="bibr" coords="2,238.82,706.53,14.94,8.02" target="#b31">[31] </ref>used color coding to highlight differences between unequal visualizations, whereas Suomi and Oikarinen <ref type="bibr" coords="2,133.34,726.46,14.94,8.02" target="#b34">[34] </ref>concentrated on MRI datasets and Da Silva et al. <ref type="bibr" coords="2,82.86,736.42,10.45,8.02" target="#b6">[7] </ref>analyzed diffusion tensor volumes. Apart from color-coding, other methods of abstraction have been used to analyze image differences. Malik et al. <ref type="bibr" coords="2,409.90,207.11,14.94,8.02" target="#b18">[19] </ref>proposed a multi-image view for comparing images. Their technique subdivides the image space into hexagonal regions, and each region is subdivided into smaller elements which depict data from different series. This way contextual information is provided about the data, and outliers can be spotted very easily. The more elements a dataset consists of, the more subelements have to be created for every hexagonal region. At some point the sub-elements will be too small for a proper analysis, which makes this method unsuitable for large image datasets. The authors also state that their approach is targeted to grey-scale values only. Our approach for comparative image visualization is somewhat similar to the multiimage view, since it also aims at preserving information about the underlying data. However, due to the use of clustering, our method is scalable with respect to a significantly larger number of images. Our visualization technique is also only applied to regions where changes take place and therefore provides a better localization of differences. </p><p>Various approaches dealing with parameter space analysis also involve image comparison. Marks et al. <ref type="bibr" coords="2,432.80,383.55,14.94,8.02" target="#b19">[20] </ref> introduced Design Galleries , which allows the user to browse result images that have been produced by varying a given input-parameter vector. Ma <ref type="bibr" coords="2,485.99,403.47,14.94,8.02" target="#b17">[18] </ref>proposed Image Graphs to visually analyze the process of data visualization. VisTrails <ref type="bibr" coords="2,318.74,423.40,14.94,8.02" target="#b32">[32] </ref>is an interesting tool which provides visual comparisons for work-flows and images. Bruckner and Möller <ref type="bibr" coords="2,463.48,433.36,10.45,8.02" target="#b1">[2] </ref> developed a system to explore simulation parameters. They suggest to sample the given parameter space and then apply clustering to the output images to identify key changes. Tuner, a system developed by Torsney-Weir et al. <ref type="bibr" coords="2,305.26,473.21,13.74,8.02" target="#b35">[35]</ref> , allows them to find proper parameter values for image segmentation by comparing the segmentation results to a ground truth. All these approaches rely on image comparison to evaluate output images. Agglomerative hierarchical clustering is a statistical method of cluster analysis which aims at building a hierarchy of clusters <ref type="bibr" coords="2,511.51,520.13,9.52,8.02" target="#b8">[9]</ref>. In this bottom-up approach pairs of clusters are merged as one moves up the hierarchy. Hierarchical clustering has become a de facto standard for analyzing biological gene expression data in the past years <ref type="bibr" coords="2,518.32,550.02,13.74,8.02" target="#b9">[10]</ref>. It is also used in other domains, for example to analyze audio data as described by Clarkson and Pentland <ref type="bibr" coords="2,427.77,569.95,10.45,8.02" target="#b3">[4] </ref>or to classify ocean colors as proposed by Yacoub et al. <ref type="bibr" coords="2,400.23,579.91,13.74,8.02" target="#b39">[39]</ref>. We use hierarchical clustering to embed differences in the set of images in a hierarchy to identify different levels of data variances. Focus+context and in-place interaction techniques give an overview over the available data, but also allow to further inspect details on demand. Several focus+context approaches in 2D and 3D can be found in the literature <ref type="bibr" coords="2,377.68,646.76,9.52,8.02" target="#b4">[5]</ref>. Mistelbauer et al. <ref type="bibr" coords="2,461.12,646.76,14.94,8.02" target="#b23">[24] </ref>proposed Smart Super Views, a tool for medical visualization which provides only the currently most relevant views to the user. Zhao et al. <ref type="bibr" coords="2,493.63,666.68,14.94,8.02" target="#b40">[40] </ref>implemented an interactive focus+context approach where the user can select regions of interest in 3D. Details about the ROIs are presented by preserving the context around the selection. Contextual Snapshots, presented by Mindek et al. <ref type="bibr" coords="2,382.88,706.53,13.74,8.02" target="#b22">[23]</ref>, allow the user to keep track of spatial selections in visualizations. We use focus+context and in-place interaction techniques which enables the user to inspect individual image features. In this paper a new approach for comparative image visualization is presented. To effectively convey information about image differences, we decided to provide an interactive visualization system that preserves contextual information while enabling the inspection of individual image differences. According to Gleicher et al. <ref type="bibr" coords="3,230.60,106.55,13.74,8.02" target="#b13">[14]</ref>, there are three common approaches used to compare data structures: side-byside comparison (juxtaposition), blending (superposition) and explicit difference encoding (aggregation). VAICo uses a mixed approach of superposition and aggregation. In our work we focus on comparison tasks which aim to identify and analyze differences in a large number (i.e., up to hundreds) of similar images. None of the images has to be defined as a reference image. Image differences are interpreted as variations in the image dataset (i.e., local color changes). In particular, VAICo is designed to assist users in identifying distinct classes of variations which characterize the underlying phenomena. Such investigations are common in medicine and biology, for instance. There they can provide important insights into the underlying cause of a disease, or the genetic mechanisms behind certain phenotypical variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analysis of Image Set Differences</head><p>VAICo is based on an image comparison step where the image space, defined by the images in the given dataset, is divided into the two parts of contextual information and regions of differences (RoDs). Contextual information refers to parts of the image space that are the same in all images. RoDs correspond to variations in the image set. Image variations are interpreted as color changes between at least two images in the dataset. Image data related to a certain RoD is then embedded in a hierarchy, which enables the classification of changes in the data (Section 3.1.2). <ref type="figure" coords="3,91.11,356.69,30.52,8.02">Figure 2</ref>provides an overview of our approach. The results of the image comparison and clustering can be explored in our interactive visual analysis interface (Section 3.2) which is driven by the visualization of the RoDs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Region-of-Difference Computation</head><p>In the first step RoDs have to be identified in image space. We propose two image comparison approaches which both result in a list of RoDs for the given image dataset. To apply an unbiased image comparison, we employ a pixel-based image comparison metric based on the Mean Squared Error (MSE) as described by Zhou et al. <ref type="bibr" coords="3,126.54,467.36,13.74,8.02" target="#b41">[41]</ref> . All pixels from one image are compared to pixels at the same location in all other images in the dataset. A threshold is used to filter out low color variations. This enables users to control the algorithm's sensitivity with respect to changes in the data (see <ref type="figure" coords="3,48.60,507.21,35.04,8.02" target="#fig_0">Figure 18</ref> for further information). After applying the MSE calculations to all pixels in image space, a set of difference pixels is identified which represent pixels with varying color values in the images. Region growing <ref type="bibr" coords="3,90.66,537.10,14.94,8.02" target="#b33">[33] </ref>is then used to group difference pixels together to form disjoint subsets of pixels. Difference pixels are grouped together based on their spatial arrangement. In the region growing approach all difference pixels are considered to be seeds which potentially could start a new region. The region growing is initiated with a randomly selected pixel. Then neighboring pixels (according to an 8-connected neighborhood) are merged into a connected region. The process is iterated until all pixels have been assigned to a region, as demonstrated in <ref type="figure" coords="3,40.68,616.80,28.99,8.02">Figure 3</ref>. The subsets resulting from the region growing step define the RoDs which form the basis of our interactive visual analysis approach . RoDs indicate the locations where changes take place in the data. All together, they give an overview on how much of the image space is affected by differences in the dataset. In addition to the unbiased image comparison we also propose a biased image comparison. This enhances the image comparison by allowing us to include prior knowledge about the data. For some datasets not only image variations are of interest for the analysis, but also differences of features in the image. Features are represented as connected image regions which can have different characteristics in the image data (i.e., different color values). In contrast to the unbiased image comparison, RoDs are calculated for every image in the contextual information diierence pixels seed pixels pixels assigned to subset t = n t = n + 1 t N n ... time number of iterations region growing iterations (0 ≤ n ≤ N) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>... ... </head><p>t = n + 2 <ref type="figure" coords="3,294.12,205.19,19.49,7.37">Fig. 3</ref>. Illustration of subset computation by region growing. After the image comparison step a set of so-called difference pixels has been identified. Region growing is used to group them together into disjoint subsets. During region growing, difference pixels are assigned to the current subset until it cannot grow any more (t = n). Then another difference pixel is selected (t = n + 1) and the iteration is continued (t = n + 2). dataset individually. We employ a color segmentation approach based on Mean Shift <ref type="bibr" coords="3,348.27,298.91,10.45,8.02" target="#b5">[6] </ref>to identify regions of interest in every image in the dataset. The segmented regions then define a set of m RoDs per image in the dataset. Afterwards image comparison is applied, which in this case refers to sets of RoDs being compared to each other. The goal of the comparison is to find out whether RoDs of different images represent the same information. <ref type="figure" coords="3,402.52,348.72,30.73,8.02">Figure 4</ref>illustrates the process of RoD comparison. RoDs are compared based on their spatial position, shape and size. RoDs of different images are considered to represent the same region if their overlap exceeds a threshold (we use 90% as a default ). The biased image comparison leads to a final list of RoDs for the dataset, where every RoD is present in at least one image. Using the biased instead of the unbiased image comparison allows the user to eliminate certain regions (e.g., background information) from further analysis. The unbiased as well as the biased image comparison step result in a list of final RoDs that are used for further analysis. Since all images are registered and of the same size, a part covered by the RoD's location and size (defined by its corresponding set of difference pixels) can be found in every image. This leads to an unordered list of i image parts per RoD for i images in the dataset (<ref type="figure" coords="3,429.66,490.85,28.64,8.02" target="#fig_1">Figure 5</ref>). In the case of biased image comparison, only images that contain a certain RoD are further considered. This leads to a list of l image parts per RoD in the case of biased image comparison, where l ≤ i. These image parts are called RoD-related image parts in the following. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Data Analysis by Clustering</head><p>In the second step we apply clustering to the RoD-related image parts to embed them in a hierarchy. This enables the classification of changes in the data and conveys information about the underlying data. For the clustering, complete-linkage agglomerative hierarchical clustering <ref type="bibr" coords="4,60.86,349.06,10.45,8.02" target="#b8">[9] </ref>is used. Initially, every RoD-related image part forms a separate cluster. Then in each iterative step clusters are merged together according to their distance. The difference between two image parts is defined by their amount of pixel-wise differences (ranging from 0 to 100 percent). We employ a similarity metric based on the Mean Squared Error (MSE) as described in Section 3.1.1. If another image comparison metric should be used to compare the images, this metric should be employed here as well. The clustering terminates once all elements are included in one big cluster. Based on the clustering a tree is built which describes the results of the hierarchical clustering process (<ref type="figure" coords="4,93.01,448.69,28.10,8.02" target="#fig_2">Figure 6</ref>). Every hierarchy level in the hierarchical clustering tree describes a valid clustering result for the RoD-related image parts. However, for the best description of the given data, an optimal clustering has to be found. A clustering is basically defined by the number of available clusters. A clustering with maximum accuracy assigns each RoDrelated image part to its own cluster. By comparison, a clustering with maximum compression uses one single cluster to include all RoDrelated image parts. <ref type="figure" coords="4,235.41,697.31,27.60,7.37" target="#fig_1">Figure 5</ref>. h depicts the hierarchy level in the tree. At the beginning (h = 0) all image parts are in separate clusters. In further steps (h = 1, h = 2) clusters are subsequently merged together according to their distance, until all elements are enclosed in one cluster (h = 3). A clustering is considered to be optimal if it strikes a balance between the maximum accuracy and the maximum compression clus- tering <ref type="bibr" coords="4,309.46,73.31,13.74,8.02" target="#b27">[28]</ref> . An optimal clustering of the data will classify the image differences and provide more information about outliers as well as similarities in the data. To get an optimal clustering, the hierarchical clustering tree has to be cut at a certain level. We decided to use the elbow criterion as described by Pedersen and Kulkarni <ref type="bibr" coords="4,474.04,113.16,14.94,8.02" target="#b27">[28] </ref>to determine the tree level which contains the best clustering result. In essence, the elbow criterion specifies that a clustering should be chosen in a way that adding another cluster does not give a better modeling of the data. In a clustering, outliers are defined by their distance to other clusters in the set. For our approach cluster outliers are of special interest , since they are considered to represent significant changes in the data. Therefore, clusters from the level with the best clustering result are sorted according to their inter-cluster distance. The cluster sorting process is done in an iterative way. In every step, the cluster with the maximum inter-cluster distance (defined by the sum of all distances to other clusters) is determined. It is then stored in an ordered list and excluded from further sorting operations. The process is repeated until no unsorted cluster is left. At the end, an ordered list of clusters is created for every RoD. </p><formula>h = 0 h = 1 h = 2 h = 3 </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interactive Visual Analysis Interface</head><p>The results of the image comparison step can be explored with VAICo's interactive visualization tools. Although the main interactions are based on the RoD visualization, it is possible to view the clustering and data analysis results as well. The main parts of the user interface are illustrated in <ref type="figure" coords="4,379.15,326.25,29.02,8.02">Figure 7</ref>. The interactive visualization elements are embedded in image space which is defined by the images in the given dataset. The entire set of images is visualized in one view. Parts of the image space which represent the same information in every image are depicted as contextual information (<ref type="figure" coords="4,336.70,376.49,31.68,8.02">Figure 7a</ref>). We create an average image of all images in the dataset, and this image is displayed in the background. Pixels are faded out to enhance the visibility of the image variations. This contextual information is needed to embed the interactive visualization tools in the appropriate context. RoDs are emphasized through colored polygons in the foreground (<ref type="figure" coords="4,288.89,436.69,32.80,8.02">Figure 7a</ref>). The shape and position of the polygons correspond to the set of difference pixels assigned to the RoD (as described in Section 3.1.1). The RoD visualization provides a visual overview of the image comparison results. In combination with the contextual information , the RoD visualization allows the user to immediately differentiate between image variations and regions of similar information. In contrast to marking differences by abstract shapes like circles (<ref type="figure" coords="4,521.15,496.47,10.76,8.02">Fig</ref><ref type="figure" coords="4,414.32,666.26,33.29,8.02">Figure 7a</ref>, the number of clusters for RoD 1 is lower than the number of clusters for RoD 2 . Therefore, the color of RoD 2 is darker in the visualization. RoD polygons can be inspected individually through mouse manipulation . When activated, RoD polygons are expanded to form RoD widgets. The widgets are displayed in image space in relation to the corresponding RoD polygons (<ref type="figure" coords="4,399.13,726.46,32.77,8.02">Figure 7b</ref>). This embeds the widgets into the appropriate context and enables the user to analyze image <ref type="figure" coords="5,31.50,190.12,19.85,7.37">Fig. 7</ref>. Main components of the interactive user interface. The image space is displayed in one view (a). Similar information is displayed in the background (context) and the variations in the images are highlighted by RoDs. The RoDs can be further explored individually by using the assigned RoD widgets (b). The cluster bullets assigned to the RoD widget depict the clusters available for the corresponding RoD. The bullets can be expanded (c) to cluster views which consist of the assigned cluster average images and icons depicting the number of images in the cluster. This icon can be used to retrieve the list of images assigned to the cluster. In addition, the clustering selection (d) gives an overview over the hierarchical clustering tree and allows the user to select a new clustering (see also <ref type="figure" coords="5,323.73,237.44,27.90,7.37">Figure 8</ref>). variations without switching between different views. RoD widgets consist of a circle with colored bullets arranged around it. The colored bullets of the RoD widgets depict the corresponding clusters. The color of the cluster bullets changes according to the number of images that are included in the respective cluster. The color hue of the RoD polygon is always different from the hue of the bullets to prevent ambiguity. The color of a bullet is the darker, the more images the corresponding cluster contains. The bullets are ordered according to the number of images in the clusters. The user can decide whether they should be ordered in descending or ascending order. This allows the user to decide whether the cluster with the highest number of images should be at the top (i.e., to find patterns in the data), or whether the cluster with the lowest number of images should be first (i.e., to find outliers in the data). The cluster bullets of the RoD widgets can be further expanded by mouse manipulation (<ref type="figure" coords="5,110.20,421.28,32.36,8.02">Figure 7c</ref>) to display the cluster views. Clusters are then represented by the average image calculated from the images in the clusters. This immediately gives an overview on the image data that is encoded in the cluster. To get more quantitative information about the cluster size, the number of images is depicted for every cluster in a separate icon. These icons are attached to the average cluster images and provide additional functionality which can be controlled by mouse manipulation. When activated, the list of images encoded in the cluster can be viewed in a popup-window. This allows the user to select groups of images in the original data based on certain patterns. In addition to the condensed view on the data, every RoD widget provides means to further analyze the results of the hierarchical clustering process. Mouse manipulation of the RoD polygons can also be used to view a visualization of the complete clustering tree (<ref type="figure" coords="5,267.53,553.95,14.35,8.02;5,31.50,563.91,21.86,8.02">Fig- ure 7d</ref>). We use a dendogram approach, since this gives an instant structural overview of the hierarchical clustering results. A dendrogram is a tree diagram which illustrates the arrangement of clusters produced by hierarchical clustering. Clusters which are created by merging are represented by the average image computed from their leaves. Visualizing the clustering helps to understand how the final RoD-related clusters have been generated. Furthermore, the number of hierarchy levels in the tree indicates the diversity of the RoD-related image parts. The clustering which is visible in a RoD widget refers to a specific level in the clustering tree. The elbow criterion has been used to select the clustering. This decision may not be accurate in all cases. We therefore enable the user to overrule this decision by simply selecting another level in the clustering selection. The corresponding RoD widget is immediately updated to represent the new clustering (<ref type="figure" coords="5,247.22,706.53,28.18,8.02">Figure 8</ref>). The top level of the clustering tree contains only one cluster which includes all available images. At the bottom level of the clustering tree, all clusters only contain one image. Additional control tools are provided in VAICo which the user can employ to influence the visualization. The color of both the RoD polygons and the RoD widgets can be changed according to given color schemes. To skip variations that are not of interest for the data analysis , and to prevent the visualization from getting overly cluttered, the user can employ command tools to hide individual RoDs. According to the size and shape of the image variations, it may happen that RoD widgets will cover a large area of the image space when activated. Therefore, the RoD widgets can be shrunk. In the case of a biased image comparison (Section 3.1.1), the semantics of the color coding are slightly different, since information is collected on the number of images the RoDs are present in. The darker the RoD polygon, the higher the number of corresponding image parts. This way the colors of the polygons depict outliers in the data that are either available in many, or in just a fraction of the input images. With its interactive visualization tools, VAICo provides an overview of the image variations and allows the user to further explore them. Differences in the data are encoded visually, but also functionality is provided to go back to the original input data if necessary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>The raw data for the presented approach comprise an unsorted set of PNG images. The pre-processing, consisting of an image comparison step and a data clustering step, has been implemented in JAVA. The interactive visualization tools have been implemented as an interactive web application. The widgets are embedded in an HTML5 canvas and interactions are done in JavaScript. The cost of the pre-processing depends on the number and size of the images in the set. The run-time depends on the number of identified RoDs, since for every RoD hierarchical clustering has to be ap- plied. current clustering new clustering <ref type="figure" coords="5,294.12,706.66,19.39,7.37">Fig. 8</ref>. Interactive clustering tree. In the interactive clustering tree the user can specify a new level by mouse manipulation. This will overrule the decision made during the clustering process. The new clustering is immediately available in the RoD widget. </p><p> The pre-processing for the largest data set called Gene Expression , which consists of 578 images of size 200x200 pixels (see also Section 5), takes 2.1 minutes. No user input is required during preprocessing . Afterwards the results of the pre-processing step are transferred to the visualization system by JSON files. The results are then loaded into the visualization system, which means that the provided JSON files are parsed. This takes 10 seconds for the largest data set. Afterwards the visualization system is ready to be used by the user. The interaction itself works in real-time. Our web-based architecture enables easy deployment of VAICo to explore and analyze online image collections and repositories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In order to evaluate the proposed comparative image visualization five different sets of images have been analyzed (<ref type="figure" coords="6,189.94,201.00,28.91,8.02" target="#fig_4">Figure 9</ref>). The images are coming from different domains to show the applicability of the proposed method to different types of datasets. The selection of the first dataset called Puzzle was inspired by the well-known spot-the-difference puzzles that can be found in many newspapers. This dataset consists of 10 pictures which show a realworld scene. The images are of size 1050x700 pixels. In the images small elements disappear or change their color. The second dataset, called Shapes, is a synthetic dataset which consists of 52 images. The images are of size 600x400 pixels. They contain different types of shapes (rectangles, triangles and circles) of various colors and sizes on a white background. In the series of images, individual shapes disappear , re-appear or change their color. Analyzing the differences of both datasets by juxtaposition (i.e., placing images side-by-side) would be a tedious task for the user. This is primarily due to the number of images per dataset. Approaches that use juxtaposition do not scale well. ,RoD 2 ), and some of them also change their color (In the comparative visualization of VAICo, dark spots on the retina can be identified as outliers in the data (<ref type="figure" coords="7,180.94,272.18,32.67,8.02" target="#fig_0">Figure 12</ref>). Images without anomalies are included in one cluster since they contain similar information . The RoD widget allows us to explore the variations in the data, so that anomalies can be distinguished from normal data variations that do not indicate pathological changes. The RoD widgets allow us to retrieve the original image data, which keeps track of the investigated patients. When viewing the comparison results with VAICo, the image part showing the coast-line is covered by one RoD. The damage caused by the tsunami on the coast-line is clearly visible as an outlier in the RoD data (i.e., separate cluster). In the remaining clusters the images before and after the tsunami are summarized (<ref type="figure" coords="7,434.84,376.74,31.96,8.02" target="#fig_0">Figure 13</ref>). The fifth set of images called Gene Expression is from the biological domain and consists of 578 images showing gene expression data. The images are of size 200x200 pixels. The data has been created from point cloud datasets of different fruit fly embryos as described by Fowlkes et al. <ref type="bibr" coords="7,356.19,426.55,13.74,8.02" target="#b11">[12]</ref> . In the images gene expressions of the EVE protein are color-coded from very high (red) to very low (blue). The EVE protein forms a pattern of seven vertical stripes of gene expression. Every image in the Gene Expression dataset corresponds to one fruit fly embryo. On the one hand, differences in the gene expression can be analyzed. On the other hand, outliers in the data can be detected which show unexpected gene expression patterns. However, due to the large number of images, a juxtapositional visualization does not scale well with this dataset. A superpositional comparison may hide differences in the gene expression of the seven regions. We analyzed this dataset by using the biased image comparison approach as described in Section 3.1.1. We located the seven stripes of the EVE protein in the images by segmentation. Then the RoD data of the images was compared and the results were visualized with VAICo (<ref type="figure" coords="7,297.89,566.03,31.75,8.02" target="#fig_0">Figure 14</ref> ). The color coding of the RoD polygons indicates the number of images the RoDs are present in. In the visualization it can be seen that the first stripe shows up in more images than the other six stripes. This is due to the fact that some images in the earlier stage of the embryo development only contain one stripe. Additionally, outliers can be detected very easily in VAICo. They refer to patterns in the images where the gene expression does not look like as expected. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>We collected user feedback to evaluate the presented visualization techniques. We hypothesized that using our visualization technique, participants would: (1) get results faster when searching for differences in a set of images; (2) get a better overview of individual variations in the image data; and (3) be able to better spatially localize differences in the image data. The results of the evaluation can be found in <ref type="figure" coords="7,326.50,716.50,33.13,8.02" target="#fig_0">Figure 15</ref>. For the feedback we compared a juxtapositional comparative visualization of an image set to the comparative visualization presented in ) and the y-axis gives the time (in seconds) it took to complete the task. Every participant had to complete every task twice: once by using a juxtapositional visualization and once by using VAICo. Therefore, two different measurements are available per task for every participant. The evaluation results show that VAICo clearly is of benefit for completing the examined tasks. this paper. In the juxtapositional visualization participants could scroll through the list of images, sort them by drag-and-drop and click on images to enlarge them (<ref type="figure" coords="8,101.19,456.79,31.69,8.02" target="#fig_0">Figure 16</ref>). VAICo, on the other hand, contained all the main features as described in Section 3.2 (<ref type="figure" coords="8,199.59,466.75,31.96,8.02" target="#fig_0">Figure 17</ref>). The images taken from the Shapes dataset contained shapes like triangles, circles and rectangles on a white background. Some of the shapes changed their color or disappeared in the image set. For the evaluation, six different dataset versions have been prepared. Due to their very general composition, images could be easily interpreted by users from different domains. We designed three tasks which refer to the three hypotheses mentioned above: Participants had to complete all three tasks twice: once by using the juxtapositional comparative view and once by using the comparative visualization technique proposed in this paper. Datasets were switched between different tasks, and also between different visualization techniques. Therefore, participants never worked on the same dataset twice. The order of the tasks (as stated above) was the same for every participant. The order in which the two different visualization techniques were presented to the user was selected at random. For every task we measured the completion times and the correctness of the answers. The user study was conducted with 11 participants from three different domains (computer science (7), engineering (3) and medicine (1)). Participants were between 26 and 57 years old. The group of participants consisted of 9 men and 2 women. The results of the user study are presented in <ref type="figure" coords="8,465.43,530.14,33.98,8.02" target="#fig_0">Figure 15</ref> . The results show that VAICo definitely improves the search time for image differences. In addition to time, the error rate was measured for every task. For the juxtapositional comparative visualization some participants made errors when trying to solve task T2 (error rate of 9.1 %) and task T3 (error rate of 27.3%). With VAICo participants did not give wrong answers. When analyzing the results, it can be seen that VAICo helps users to quickly get an overview on differences in the image set (task T1). It also helps to analyze local variations (task T2). Both tasks would require time-consuming analysis steps when done manually by the user. The comparative visualization brings significant improvements for users to localize variations in image space (task T3). This is a very tedious task if done manually on an unordered set of im- ages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND LIMITATIONS</head><p>VAICo operates on a large number of images and analyzes similarities and differences among them. As demonstrated in Section 6, image comparison tasks can be facilitated and accelerated by making use of our approach. VAICo is scalable to a large number of images and works with images from different domains (as outlined in Section 5).  Our approach nonetheless has some limitations, which point to interesting directions for future work. VAICo is scalable to a large number of images. We use clustering to cope with scale and to identify patterns in the data. Clusters are depicted as bullets around the RoD widgets (as shown in <ref type="figure" coords="9,245.64,212.80,29.77,8.02">Figure 7</ref>). If many clusters have to be displayed in the widget, there might not be enough space around the widget to visualize all of them. In this case additional functionality is provided to the user, which means that clusters are presented in a separate popup-window (where scrolling can be activated easily). However, we would like to point out that a high number of clusters bullets for a RoD (see <ref type="figure" coords="9,214.38,272.58,34.50,8.02">Figure 7b</ref>) usually indicate that the underlying clustering should be refined. VAICo provides the possibility to adjust the clustering by selecting another level in the clustering tree (as described in <ref type="figure" coords="9,167.45,302.46,29.13,8.02" target="#fig_2">Figure 6</ref>). We think that having the ability to refine the clustering is more appropriate for analyzing the data, than providing means to deal with high numbers of clusters. Including additional visualization techniques which can handle these special cases might be an interesting idea for future work. The used Mean Squared Error (MSE) computation is a very simple image comparison approach which has some limitations. As one possibility to deal with these limitations, we provide the possibility to adjust the sensitivity of the algorithm. The user can define a sensitivity threshold for intensity changes. <ref type="figure" coords="9,177.28,393.40,35.18,8.02" target="#fig_0">Figure 18</ref>illustrates how the threshold effects the RoD calculation. The higher the threshold, the less pixels will be considered as variations in the data. Increasing the threshold will shrink the resulting RoDs, and might even cause RoDs to disappear. This way, particular intensity variations in the data can be skipped if they are not of interest for the data analysis. MSE is very sensitive to global intensity shifts, and adjusting the sensitivity threshold might not be enough to deal with global changes in the data. <ref type="figure" coords="9,75.99,474.38,39.02,8.02" target="#fig_0">Figure 19a</ref>shows an example of an image dataset where one of the images is lighter than the other ones. When applying MSE to this dataset, one big RoD is returned which covers almost the whole image space (<ref type="figure" coords="9,82.43,504.27,36.49,8.02" target="#fig_0">Figure 19b</ref>). We used Normalized MSE <ref type="bibr" coords="9,228.74,504.27,14.94,8.02" target="#b41">[41] </ref>instead of MSE to compare the images. This solves the problem and returns the expected result (<ref type="figure" coords="9,105.17,524.20,35.96,8.02" target="#fig_0">Figure 19c</ref>). In addition, this example shows that VAICo can be adapted to work with other image comparison metrics as well. in the given dataset shows more intensity variations than others (a). Since MSE is very sensitive to global intensity shifts (b), the approach fails in this case (differences are indicated in orange). To solve this problem and retrieve all desired RoDs, another image comparison metric (i.e., Normalized MSE) can be used (c). </p><p>(a) a b  VAICo operates on a set of images, where the individual images exhibit small local differences. The approach has its limitations if dealing with a high number of variations in the data. <ref type="figure" coords="9,469.78,280.51,34.64,8.02" target="#fig_23">Figure 20</ref>shows two examples where VAICo does not produce appropriate results, namely varying data in landscape images and motion data. VAICo is designed for scenarios where the obtained images have undergone a certain form of standardization. This is often done during the acquisition process itself (e.g., by employing a particular protocol for mounting and imaging ) or through post-processing such as registration. As such procedures are highly dependent on the application domain, they are not the focus of our work. Instead, we assume that all images in our input set have undergone such a process, i.e., they are of the same size and represent similar regions in space. In the case of landscape data (<ref type="figure" coords="9,414.16,391.04,35.98,8.02" target="#fig_23">Figure 20a</ref>), an additional clustering step could be applied to the images, and VAICo could be used to further explore images in the individual clusters. In the case of motion data (<ref type="figure" coords="9,314.93,420.93,35.93,8.02" target="#fig_23">Figure 20b</ref>), VAICo gives a quick overview on the motion tracks in the image. Although our approach is currently not suited to deal with time-dependent data, it will be interesting to explore its applicability for event detection in video data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p> In this paper a visualization technique for the comparative visualization of multiple images has been presented. Interactive visualization tools are provided to explore the image space and drill-down on individual variances. Our visualization approach addresses the scalability of image comparisons and proposes ways to integrate contextual information and more detailed information in one view. Contextual information is preserved, whereas image variances can be efficiently spotted and put into context. Our approach can be applied to quickly identify small local differences in an image set. It is also helpful for analyzing the occurrence and value ranges of previously defined image features. We have demonstrated the scalability of our technique by applying it to five sets with varying numbers of images. In the future, it will be interesting to explore how the proposed visualization techniques can be extended to image sets with spatially larger variations and what additional information can be extracted from the hierarchical clustering process. We also plan to investigate the applicability of VAICo for parameter space analysis. Another direction for further research is to extend our approach for the detection of key events in video data. Furthermore, we see the possibility to include our method in image tools like revision control <ref type="bibr" coords="9,464.66,678.78,9.52,8.02" target="#b2">[3]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,294.12,395.41,250.38,7.37;1,294.12,404.88,250.38,7.37;1,294.12,414.34,250.38,7.37;1,294.12,423.81,250.38,7.37;1,294.12,433.27,250.38,7.37;1,294.12,442.74,250.38,7.37;1,294.12,452.20,250.38,7.37;1,294.12,461.67,250.38,7.37;1,294.12,471.13,92.46,7.37"><head>Fig. 1. </head><figDesc>Fig. 1. Image Comparison. This figure shows two pictures that look similar, but in fact exhibit local changes. A very common way to compare them is by placing them side-by-side (a). To help the user to find the variations more quickly, a difference image (b) can be computed. In this illustration information about the similar parts of the data gets lost. Another possibility is to highlight differences by certain patterns (e.g., colored circles) as shown in (c). In this case similar parts of the data are still visible; however, no further information is provided on how the differences are structured. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,22.50,210.86,250.38,7.37;4,22.50,220.32,250.38,7.37;4,22.50,229.79,250.38,7.37;4,22.50,239.25,250.38,7.37;4,22.50,248.71,196.97,7.37"><head>Fig. 5. </head><figDesc>Fig. 5. Image data corresponding to a certain RoD (unbiased approach). In this Figure one particular RoD together with its corresponding image parts is displayed. The image parts are taken from all images in the set (as indicated by the arrows) and are defined by the RoD's size and location (i.e., the corresponding set of difference pixels). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,22.50,697.31,250.38,7.37;4,22.50,706.77,250.38,7.40;4,22.50,716.23,250.38,7.40;4,22.50,725.70,250.38,7.37;4,22.50,735.16,157.24,7.40"><head>Fig. 6. </head><figDesc>Fig. 6. Hierarchical clustering result for the RoD introduced in Figure 5. h depicts the hierarchy level in the tree. At the beginning (h = 0) all image parts are in separate clusters. In further steps (h = 1, h = 2) clusters are subsequently merged together according to their distance, until all elements are enclosed in one cluster (h = 3). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,531.91,496.47,3.59,8.02;4,285.12,506.43,250.38,8.02;4,285.12,516.39,84.45,8.02;4,295.08,526.78,240.42,8.02;4,285.12,536.74,250.38,8.02;4,285.12,546.71,250.38,8.02;4,285.12,556.67,250.38,8.02;4,285.12,566.63,250.38,8.02;4,285.12,576.59,250.38,8.02;4,285.12,586.56,250.38,8.02;4,285.12,596.52,250.38,8.02;4,285.12,606.48,250.38,8.02;4,285.12,616.44,250.38,8.02;4,285.12,626.41,250.38,8.02;4,285.12,636.37,250.38,8.02;4,285.12,646.33,250.38,8.02;4,285.12,656.29,250.38,8.02;4,285.12,666.26,126.45,8.02"><head></head><figDesc>ure 1), the polygon shape provides more information about the extent of the image variations. The number of RoD polygons in image space depicts how many image variations have been identified. The position and distribution of the RoD polygons shows where variations are located in the images , and how much of the image space is affected. In addition to size, shape and location of the image variations, information about the RoD-related clustering results is also included in the visualization. In case the clustering leads to a higher number of clusters than in other RoDs, the underlying RoD-related image data shows a greater variety than in other cases. This may indicate the existence of outliers in the data, wherefore these cases should be further analyzed by the user. Color-coding is used in the visualization to indicate a higher number of RoD-related clusters. A higher number of clusters is mapped to a darker color of the RoD polygon. Color-coding allows us to integrate the cluster information in the RoD visualization without occluding additional contextual information. In </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,22.50,546.22,250.38,7.37;6,22.50,555.68,250.38,7.37;6,22.50,565.15,250.38,7.37;6,22.50,574.61,250.38,7.37;6,22.50,584.07,250.38,7.37;6,22.50,593.54,173.80,7.37"><head>Fig. 9. </head><figDesc> Fig. 9. Image datasets. The dataset Puzzle contains pictures of a realworld scene (a). The dataset Shapes contains images with shapes of different color. The dataset Retina contains retina images from different patients (c). The dataset Satellite consists of satellite images of a coastline in Indonesia (d). The dataset Gene Expression contains images with color coded gene expression information (e). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="6,285.12,243.19,250.38,7.37;6,285.12,252.65,250.38,7.37;6,285.12,262.12,250.38,8.22;6,285.12,271.58,250.38,8.31;6,285.12,281.04,250.38,7.37;6,285.12,290.51,171.96,7.37"><head>Fig. 10. </head><figDesc>Fig. 10. Results for dataset Puzzle. Our approach identified five RoDs which depict the data changes in the images. One object in the scene changes its color (RoD 3 ), and some other objects are not present in all images (RoD 1 , RoD 2 , RoD 4 , RoD 5 ). The color-coding of the RoDs shows the number of corresponding clusters. The color-coding of the cluster bullets indicates the number of assigned images. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="6,285.12,678.27,250.38,7.37;6,285.12,687.73,250.38,7.37;6,285.12,697.19,250.38,7.37;6,285.12,706.66,250.38,8.22;6,285.12,716.12,250.38,8.22;6,285.12,725.59,250.38,7.37;6,285.12,735.05,210.94,7.37"><head>Fig. 11. </head><figDesc>Fig. 11. Results for dataset Shapes. This dataset consists of images containing shapes of different color on a white background. Our approach identified five RoDs (three of them expanded in this Figure). Some of the objects are not present in all images (RoD 1 ,RoD 2 ), and some of them also change their color (RoD 3 ). The color-coding of the RoDs shows the number of corresponding clusters. The color-coding of the cluster bullets indicates the number of assigned images. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="7,31.50,183.26,250.38,7.37;7,31.50,192.72,250.38,7.37;7,31.50,202.19,250.38,7.37;7,31.50,211.65,250.38,7.37;7,31.50,221.11,250.38,7.37;7,31.50,230.58,99.91,7.37"><head>Fig. 12. </head><figDesc>Fig. 12. Results for dataset Retina. This dataset consists of retina images from different patients. The image comparison identified dark spots on the retina, which can be further analyzed by using the RoD widgets. The color-coding of the RoDs shows the number of corresponding clusters. The color-coding of the cluster bullets indicates the number of assigned images. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="7,41.46,342.11,240.42,8.02;7,31.50,352.07,250.38,8.02;7,31.50,362.03,250.38,8.02;7,31.50,372.00,250.38,8.02;7,31.50,381.96,250.38,8.02;7,31.50,391.92,250.38,8.02;7,31.50,401.88,250.38,8.02;7,31.50,411.85,250.38,8.02;7,31.50,421.81,250.38,8.02;7,31.50,431.77,250.38,8.02;7,31.50,441.73,40.09,8.02;7,40.97,640.67,35.82,7.40;7,53.26,614.22,13.26,7.40;7,35.61,629.53,8.39,6.66;7,72.78,629.53,10.55,6.66;7,74.63,655.58,10.55,6.66;7,36.22,655.57,8.39,6.66"><head></head><figDesc> The set Satellite consists of 12 satellite images of size 614x450 pixels . They cover a time period from 2000 to 2011. One image has been produced every year. The images show a coast-line in Indonesia which has been affected by a tsunami in 2004. In these images, outliers like the damage caused by the tsunami which are present in only one image , can be easily missed. Additionally, differences between the state of the coast-line before and after the tsunami are not inherently available . Since no prior information was given about the structure of the image data, we used the unbiased image comparison approach. It was necessary to adjust the threshold to skip small changes in some parts of the data. cluster bullets RoDs low high high low </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="7,31.50,678.27,250.38,7.37;7,31.50,687.73,250.38,7.37;7,31.50,697.19,250.38,7.37;7,31.50,706.66,250.38,7.37;7,31.50,716.12,250.38,7.37;7,31.50,725.59,250.38,7.37;7,31.50,735.05,99.91,7.37"><head>Fig. 13. </head><figDesc>Fig. 13. Results for dataset Satellite. The images in this dataset show a coast-line in Indonesia. Damage caused by the tsunami in 2004 is identified as an outlier in the data (a). The images showing the state of the coast-line before (b) and afterwards (c) are summarized in separate clusters. The color-coding of the RoDs shows the number of corresponding clusters. The color-coding of the cluster bullets indicates the number of assigned images. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="7,294.12,257.68,250.38,7.37;7,294.12,267.15,250.38,7.37;7,294.12,276.61,250.38,8.22;7,294.12,286.08,250.38,8.22;7,294.12,295.54,250.38,7.37;7,294.12,305.00,250.38,7.37;7,294.12,314.47,82.27,7.37"><head>Fig. 14. </head><figDesc>Fig. 14. Results for dataset Gene Expression. The color-coding of the RoD widgets depicts in how many images the corresponding RoD can be found. The first stripe (RoD 1 ) is available in more images than other stripes. Furthermore, an outlier region RoD 2 can identified. This region corresponds to unexpected gene expression data and only shows up in two images. The color-coding of the RoDs shows the number of corresponding clusters. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="8,22.50,166.66,513.00,7.37;8,22.50,176.13,513.00,7.37;8,22.50,185.59,513.00,7.37;8,22.50,195.06,331.44,7.37"><head>Fig. 15. </head><figDesc>Fig. 15. Evaluation Results. The charts indicate the time it took the participants to complete the tasks T1-3. The x-axis depicts the participant number (1-11) and the y-axis gives the time (in seconds) it took to complete the task. Every participant had to complete every task twice: once by using a juxtapositional visualization and once by using VAICo. Therefore, two different measurements are available per task for every participant. The evaluation results show that VAICo clearly is of benefit for completing the examined tasks. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="8,22.50,377.65,250.38,7.37;8,22.50,387.11,250.38,7.37;8,22.50,396.58,250.38,7.37;8,22.50,406.04,173.31,7.37"><head>Fig. 16. </head><figDesc>Fig. 16. Juxtapositional comparative visualization. In this visualization images that should be compared are placed side-by-side (a). Users can scroll through the list of images, sort images by drag-and-drop (b), and enlarge individual images by clicking on them (c). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="8,32.84,563.30,240.05,8.66;8,42.43,573.91,230.45,8.02;8,42.43,583.87,230.45,8.02;8,42.43,593.83,27.64,8.02;8,32.84,610.90,240.04,8.66;8,42.43,621.51,230.45,8.02;8,42.43,631.47,230.45,8.02;8,42.43,641.44,97.60,8.02;8,32.84,658.50,240.04,8.66;8,42.43,669.11,230.45,8.02;8,42.43,679.08,155.42,8.02"><head>@BULLET </head><figDesc>T1: Depict variations with certain parameters. In this task participants had to identify one shape which is present in all images . None of the remaining shapes did show up in all of the images. @BULLET T2: Analyzing local variations. In this task participants had to identify one shape that is colored in three different ways in the images. Then they had to sort the three colors for this shape by the number of occurrences. @BULLET T3: Localization of changes. In this task participants had to identify the most mutable image region (i.e., the image space part with the highest number of variations). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18" coords="8,285.12,377.62,250.38,7.37;8,285.12,387.09,250.38,7.37;8,285.12,396.55,250.38,7.37;8,285.12,406.02,52.47,7.37"><head>Fig. 17. </head><figDesc>Fig. 17. Comparative visualization of images as employed for the user feedback. The set of images is presented in one view (a). Users can employ the RoD widgets (b) to get more details about individual image differences (c). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20" coords="9,31.50,118.93,250.38,7.37;9,31.50,128.40,250.38,7.37;9,31.50,137.86,160.51,7.37"><head>Fig. 18. </head><figDesc>Fig. 18. Effect of sensitivity threshold. Three different threshold values increasing from (a) to (c) have been applied. Higher threshold values result in less image space occupied by RoDs. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22" coords="9,31.50,687.84,250.38,7.37;9,31.50,697.31,250.38,7.37;9,31.50,706.77,250.38,7.37;9,31.50,716.23,250.38,7.37;9,31.50,725.70,250.38,7.37;9,31.50,735.16,122.64,7.37"><head>Fig. 19. </head><figDesc>Fig. 19. Different image comparison metrics. One of the images in the given dataset shows more intensity variations than others (a). Since MSE is very sensitive to global intensity shifts (b), the approach fails in this case (differences are indicated in orange). To solve this problem and retrieve all desired RoDs, another image comparison metric (i.e., Normalized MSE) can be used (c). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23" coords="9,294.12,189.68,250.38,7.37;9,294.12,199.15,250.38,7.37;9,294.12,208.61,250.38,7.37;9,294.12,218.08,250.38,7.37;9,294.12,227.54,72.00,7.37"><head>Fig. 20. </head><figDesc>Fig. 20. Data analysis limitations. Landscape images (a), which show a high number or variations, and motion data (b) result in very large RoDs. This is because VAICo was designed for images that exhibit small local differences. Adjusting VAICo for other types of images is an interesting topic for future work. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="4,23.25,26.18,516.15,7.54"><figDesc coords="4,523.40,26.46,16.00,7.26;4,23.25,26.18,268.18,7.50">2093 SCHMIDT ET AL: VAICO: VISUAL ANALYSIS FOR IMAGE COMPARISON</figDesc><table coords="5,65.80,60.71,474.85,114.69">a 

b 
c 
d 

context 

RoD 1 

RoD 2 

RoD widget 

clusters bullets 
cluster average image 

number 
of images 

cluster views 
clustering selection 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false" coords="6,285.12,716.12,250.38,26.30"><figDesc coords="6,426.43,716.12,109.07,8.22;6,285.12,725.59,250.38,7.37;6,285.12,735.05,210.94,7.37">RoD 3 ). The color-coding of the RoDs shows the number of corresponding clusters. The color-coding of the cluster bullets indicates the number of assigned images.</figDesc><table></table></figure>

			<note place="foot">contextual information pixels assigned to RoDs IMG 1 IMG 2 IMG 3 Fig. 4. Illustration of RoD comparison. After color segmentation a set of RoDs has been defined per image. In the comparison step RoDs of different images are identified that refer to the same information (i.e., overlap). In this figure the upper RoD can only be identified in the first two images (IMG 1 and IMG 2 ). The second RoD can be identified in all three images (IMG 1 , IMG 2 , and IMG 3 ).</note>

			<note place="foot">s10733-21oc06-01 Dev. Stage: 0% s10876-20no06-01 Dev. Stage: 0% in 2 Images s10899-10ja06-10 Dev. Stage: 2% s10439-13no07-16 Dev. Stage: 1% s10476-02fe05-04 Dev. Stage: 47%</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The work presented in this paper has been partially supported by the ViMaL project (FWF -Austrian Research Fund, no. P21695) and by the AKTION project (Aktion OE/CZ grant number 64p11). </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,74.58,232.12,7.13;10,40.76,84.05,232.12,7.13;10,40.76,93.51,232.12,7.13;10,40.76,102.97,201.48,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">A method for image local-difference visualization</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Baudrier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Riffaud</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Document Analysis and Recognition, volume 2 of ICDAR &apos;07</title>
		<meeting>the 9th International Conference on Document Analysis and Recognition, volume 2 of ICDAR &apos;07<address><addrLine>Curitiba , Paran, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="949" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,112.44,232.12,7.13;10,40.76,121.90,232.12,7.13;10,40.76,131.37,186.51,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Result-driven exploration of simulation parameter spaces for visual effects design</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1467" to="1475" />
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,140.83,232.12,7.13;10,40.76,150.30,168.23,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlinear revision control for images</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Chang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="10510" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,159.76,232.12,7.13;10,40.76,169.23,232.12,7.13;10,40.76,178.69,232.12,7.13;10,40.76,188.16,232.12,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised clustering of ambulatory audio and video</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Clarkson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pentland</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP &apos;99</title>
		<meeting>the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP &apos;99<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="3037" to="3040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,197.62,232.12,7.13;10,40.76,207.08,232.12,7.13;10,40.76,216.55,102.35,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">A review of overview+detail, zooming, and focus+context interfaces</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Cockburn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Karlson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">B</forename>
				<surname>Bederson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="231" />
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,226.01,232.12,7.13;10,40.76,235.48,232.12,7.13;10,40.76,244.94,34.53,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Comaniciu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Meer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,254.41,232.12,7.13;10,40.76,263.87,232.12,7.13;10,40.76,273.42,232.12,6.86;10,40.76,282.80,206.10,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing the differences between diffusion tensor volume images</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Dasilva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Demiralp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">H</forename>
				<surname>Laidlaw</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISMRM Workshop on Diffusion MRI: What Can We Measure?, ISMRM &apos;02</title>
		<meeting>the ISMRM Workshop on Diffusion MRI: What Can We Measure?, ISMRM &apos;02<address><addrLine>St. Malo, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-12" />
			<biblScope unit="page" from="237" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,292.26,232.12,7.13;10,40.76,301.73,232.12,7.13;10,40.76,311.27,232.12,6.86;10,40.76,320.66,229.68,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparing and managing multiple versions of slide presentations</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Drucker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Petschnigg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Agrawala</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual ACM symposium on User Interface Software and Technology, UIST &apos;06</title>
		<meeting>the 19th Annual ACM symposium on User Interface Software and Technology, UIST &apos;06<address><addrLine>Montreux, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,330.12,232.12,7.13;10,40.76,339.59,95.86,7.13"  xml:id="b8">
	<monogr>
		<title level="m" type="main">Pattern Classification and Scene Analysis</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">O</forename>
				<surname>Duda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">E</forename>
				<surname>Hart</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>John Wiley &amp; Sons Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,349.05,232.12,7.13;10,40.76,358.52,232.12,7.13;10,40.76,367.98,204.76,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Cluster analysis and display of genome-wide expression patterns</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">B</forename>
				<surname>Eisen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">T</forename>
				<surname>Spellman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">O</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Botstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="1998-12" />
			<biblScope unit="page" from="9514863" to="14868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,377.45,232.12,7.13;10,40.76,386.91,232.12,7.13;10,40.76,396.37,232.12,7.13;10,40.76,405.92,232.12,6.86;10,40.76,415.30,232.12,7.13;10,40.76,424.77,59.26,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Multidimensional visualization to support analysis of image collections</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">M</forename>
				<surname>Eler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">Y</forename>
				<surname>Nakazaki</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">V</forename>
				<surname>Paulovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">P</forename>
				<surname>Santos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C F</forename>
				<surname>Oliveira</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E S B</forename>
				<surname>Neto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Minghim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XXI Brazilian Symposium on Computer Graphics and Image Processing , SIBGRAPI &apos;08</title>
		<meeting>the XXI Brazilian Symposium on Computer Graphics and Image Processing , SIBGRAPI &apos;08<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,434.23,232.12,7.13;10,40.76,443.70,232.12,7.13;10,40.76,453.16,232.12,7.13;10,40.76,462.63,232.12,7.13;10,40.76,472.09,157.18,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">A quantitative spatiotemporal atlas of gene expression in the drosohpila blastoderm</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Fowlkes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">L</forename>
				<surname>Hendriks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Keränen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Weber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Rübel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Chatoor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Simirenko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Depace</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Henriquez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Beaton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Weiszmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Celniker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hamann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Knowles</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Biggin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Eisen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Malik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="364" to="374" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,481.55,232.12,7.13;10,40.76,491.02,232.12,7.13;10,40.76,500.48,160.18,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Visualising video sequences using direct volume rendering</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gareth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Min</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Vision, Video and Graphics, VVG &apos;03</title>
		<meeting>Vision, Video and Graphics, VVG &apos;03<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-11" />
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,509.95,232.12,7.13;10,40.76,519.41,232.12,7.13;10,40.76,528.88,128.26,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual comparison for information visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gleicher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Albers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Walker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Jusufi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Hansen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">C</forename>
				<surname>Roberts</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="289" to="309" />
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,538.34,232.12,7.13;10,40.76,547.81,232.12,7.13;10,40.76,557.27,232.12,7.13;10,40.76,566.82,232.12,6.86;10,40.76,576.20,232.12,7.13;10,40.76,585.66,95.55,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Weaving versus blending: a quantitative assessment of the information carrying capacities of two alternative methods for conveying multivariate data with color</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hagh-Shenas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Interrante</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Healey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Symposium on Applied Perception in Graphics and Visualization, APGV &apos;06</title>
		<meeting>the 3rd Symposium on Applied Perception in Graphics and Visualization, APGV &apos;06<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="164" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,595.13,232.12,7.13;10,40.76,604.59,232.12,7.13;10,40.76,614.06,143.56,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparative visualization for comprehensive two-dimensional gas chromatography</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hollingsworth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Reichenbach</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Tao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Visvanathan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chromatogr A</title>
		<imprint>
			<biblScope unit="volume">1105</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,623.52,232.12,7.13;10,40.76,632.99,232.12,7.13;10,40.76,642.45,232.12,7.13;10,40.76,651.92,135.63,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">A visualisation tool for comparing paintings and their underdrawings</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Kammerer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Hanbury</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Zolda</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Electronic Imaging for the Visual &amp; Performing Arts, EVA &apos;04</title>
		<meeting>the Conference on Electronic Imaging for the Visual &amp; Performing Arts, EVA &apos;04<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,661.38,232.12,7.13;10,40.76,670.84,232.12,7.13;10,40.76,680.31,199.91,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Image graphs: a novel approach to visual data exploration</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Visualization, VIS &apos;99</title>
		<meeting>the Conference on Visualization, VIS &apos;99<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,689.77,232.12,7.13;10,40.76,699.24,232.12,7.13;10,40.76,708.70,157.51,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparative visualization for parameter studies of dataset series</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">M</forename>
				<surname>Malik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Heinzl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="829" to="840" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,718.17,232.12,7.13;10,40.76,727.63,232.12,7.13;10,40.76,737.10,232.12,7.13;10,303.38,54.06,232.12,7.13;10,303.38,63.52,232.12,7.13;10,303.38,72.99,177.10,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Design galleries: a general approach to setting parameters for computer graphics and animation</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Marks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Andalman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Beardsley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Freeman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Gibson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Hodgins</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Mirtich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pfister</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ruml</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Ryall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Seims</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Shieber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97</title>
		<meeting>the 24th annual conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;97<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="389" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,232.12,7.13;10,303.38,101.38,232.12,7.13;10,303.38,110.85,85.68,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Shape difference visualization for ancient bronze mirrors through 3d range images</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Masuda</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Imazu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Auethavekiat</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Furuya</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kawakami</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Ikeuchi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,120.31,232.12,7.13;10,303.38,129.78,197.19,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Attribute blocks: Visualizing multiple continuously defined attributes</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">R</forename>
				<surname>Miller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="57" to="69" />
			<date type="published" when="2007-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,139.24,232.12,7.13;10,303.38,148.70,232.12,7.13;10,303.38,158.17,230.49,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Contextual snapshots: Enriched visualization with interactive spatial annotations</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Mindek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Spring Conference on Computer Graphics, SCCG &apos;13</title>
		<meeting>the 29th Spring Conference on Computer Graphics, SCCG &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,167.63,232.30,7.13;10,303.38,177.10,232.12,7.13;10,303.38,186.56,232.12,7.13;10,303.38,196.03,232.12,7.13;10,303.38,205.49,165.07,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Smart super views -a knowledge-assisted interface for medical visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Mistelbauer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Bouzari</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Schernthaner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Baclija</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Köchl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bruckner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Srámek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">E</forename>
				<surname>Gröller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Visual Analytics Science and Technology, VAST &apos;12</title>
		<meeting>the IEEE Conference on Visual Analytics Science and Technology, VAST &apos;12</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,232.12,7.13;10,303.38,233.88,232.12,7.13;10,303.38,243.35,201.24,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Treejuxtaposer: scalable tree comparison using focus+context with guaranteed visibility</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Munzner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Guimbretì Ere</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tasiran</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH, SIGGRAPH &apos;03</title>
		<meeting>the ACM SIGGRAPH, SIGGRAPH &apos;03<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="453" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,252.81,232.12,7.13;10,303.38,262.28,232.12,7.13;10,303.38,271.74,194.64,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparative visualization: approaches and examples</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Pagendarm</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Post</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization in scientific computing (H. Göbel and H. Müller and B. Urban)</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="95" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,281.21,232.12,7.13;10,303.38,290.67,203.85,7.13"  xml:id="b26">
	<monogr>
		<title level="m" type="main">Methods for comparing 3d surface attributes. Visual Data Exploration and Analysis III</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Freeman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,300.14,232.12,7.13;10,303.38,309.60,232.12,7.13;10,303.38,319.15,232.12,6.86;10,303.38,328.53,232.12,7.13;10,303.38,337.99,232.12,7.13;10,303.38,347.46,57.57,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic cluster stopping with criterion functions and the gap statistic</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Pedersen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kulkarni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL-Demonstrations &apos;06</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL-Demonstrations &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="276" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,356.92,232.12,7.13;10,303.38,366.39,232.12,7.13;10,303.38,375.85,50.92,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Comparative visual analysis of 2d function ensembles</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Piringer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Pajer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Berger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Teichmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,385.32,232.12,7.13"  xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">B</forename>
				<surname>Procter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Letunic</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Creevey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Jossinet</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">J</forename>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,394.78,232.12,7.13;10,303.38,404.25,162.72,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualization of multiple alignments, phylogenies and gene family evolution</title>
		<author>
			<persName>
				<surname>Barton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,413.71,232.12,7.13;10,303.38,423.17,232.12,7.13;10,303.38,432.64,232.12,7.13;10,303.38,442.10,188.76,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Structured spatial domain image and data comparison metrics</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Sahasrabudhe</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">E</forename>
				<surname>West</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Machiraju</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Janus</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Visualization, VIS &apos;99</title>
		<meeting>the Conference on Visualization, VIS &apos;99<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,451.57,232.12,7.13;10,303.38,461.03,232.12,7.13;10,303.38,470.50,164.92,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Querying and creating visualizations by analogy</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Scheidegger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Vo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Koop</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Freire</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Silva</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1560" to="1567" />
			<date type="published" when="2007-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,479.96,232.12,7.13;10,303.38,489.43,162.88,7.13"  xml:id="b33">
	<monogr>
		<title level="m" type="main">Image Processing, Analysis, and Machine Vision</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Sonka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Hlavac</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Boyle</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
	<note>2. edition</note>
</biblStruct>

<biblStruct coords="10,303.38,498.89,232.12,7.13;10,303.38,508.36,232.12,7.13;10,303.38,517.82,198.01,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualization of changes in magnetic resonance image data</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Suomi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Oikarinen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter School of Computer Graphics, WSCG &apos;00</title>
		<meeting>the Winter School of Computer Graphics, WSCG &apos;00<address><addrLine>Plzen, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,527.28,232.12,7.13;10,303.38,536.75,232.12,7.13;10,303.38,546.21,232.12,7.13;10,303.38,555.68,182.97,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Tuner: Principled parameter finding for image segmentation algorithms using visual response surface exploration</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Torsney-Weir</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Saad</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Möller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hege</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Weber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Verbavatz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page" from="1892" to="1901" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,565.14,232.12,7.13;10,303.38,574.61,232.12,7.13;10,303.38,584.07,115.13,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Comparative visualization of construction schedules</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Tory</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Staub-French</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Swindells</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Pottinger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automation in Construction</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="68" to="82" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,593.54,232.12,7.13;10,303.38,603.00,104.57,7.13"  xml:id="b37">
	<monogr>
		<title level="m" type="main">The visual display of quantitative information</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">R</forename>
				<surname>Tufte</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>Graphics Press</publisher>
			<pubPlace>Cheshire, CT, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,612.47,232.12,7.13;10,303.38,621.93,232.12,7.13;10,303.38,631.39,17.93,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparative flow visualization</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Verma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="609" to="624" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,640.86,232.12,7.13;10,303.38,650.32,232.12,7.13;10,303.38,659.79,232.12,7.13;10,303.38,669.25,194.82,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">A topological hierarchical clustering: Application to ocean color classification</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Yacoub</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Badran</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Thiria</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks, ICANN &apos;01</title>
		<meeting>the International Conference on Artificial Neural Networks, ICANN &apos;01<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="492" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,678.72,232.12,7.13;10,303.38,688.18,232.12,7.13;10,303.38,697.65,83.24,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Texture-guided volumetric deformation and visualization using 3d moving least squares</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kaufman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="204" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,707.11,232.12,7.13;10,303.38,716.57,232.12,7.13;10,303.38,726.04,232.12,7.13;10,303.38,735.50,211.45,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">Comparative evaluation of visualization and experimental results using image comparison metrics</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">F</forename>
				<surname>Webster</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Visualization, VIS &apos;02</title>
		<meeting>the Conference on Visualization, VIS &apos;02<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002-11-01" />
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
