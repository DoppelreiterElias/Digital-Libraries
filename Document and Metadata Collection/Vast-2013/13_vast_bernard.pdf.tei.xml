<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">J</forename>
								<forename type="middle">¨</forename>
								<surname>Urgen Bernard</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Nils</forename>
								<surname>Wilhelm</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Björnbj¨björn</forename>
								<surname>Krügerkr¨krüger</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Thorsten</forename>
								<surname>May</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Tobias</forename>
								<surname>Schreck</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">J</forename>
								<forename type="middle">¨</forename>
								<surname>Orn Kohlhammer</surname>
							</persName>
						</author>
						<title level="a" type="main">MotionExplorer: Exploratory Search in Human Motion Capture Data Based on Hierarchical Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms—Visual analytics</term>
					<term>exploratory search</term>
					<term>multivariate time series</term>
					<term>motion capture data</term>
					<term>data aggregation</term>
					<term>cluster glyph</term>
				</keywords>
			</textClass>
			<abstract>
				<p>—We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> Human motion capture data can be regarded an instance of multivariate time series data. It is applied in various research fields such as medicine, sports and animation. It may be acquired from human actors labeled with detectable markers using video tracking or obtained synthetically by simulation. Resulting human motion time series are stored in large data collections where they are potentially available for analysis and reusage. Motion capture data typically consists of frames that contain a high-dimensional vector representation of a human pose. For large data collections, manual analysis is very time-consuming and the retrieval of a motion sequence of interest is often done by manually screening the set of motion sequences. We target the user group of researchers in human motion synthesis, who are called domain experts in the following. They focus on an effective re-use of existing motion capture data to synthesize new human motions. Thus, an overview of existing data is important to understand the potential building blocks of existing motion. Our users distinguish motion on the macro and micro level. On the macro level, different types of motions 'from A to B' are denoted (e.g., activities such as cross-country-skiing in the classical or the skating technique). On the micro level, different style variations of the same motion type are distinguished (e.g., different speeds, dynamics, or skill levels of the same motion). Motion synthesis typically involves identifying first the motion type of interest and then selecting an appropriate style to use or adapt. We identified the following problems we aim at supporting with MotionExplorer: 1.) As motion data collections are typically large, an overview is needed. To our knowledge, visual access to large collections of motion capture data has not become a common application field for visual analytics yet. In addition, the spatio-temporal variations in existing human motion make this complex data type inappropriate for many existing tools. Due to a lack of specialized tools, the domain experts report on working through their data collections manually in many cases. 2.) Visual-interactive functionality to efficiently obtain meaningful data subsets is scarce. While algorithmic search methods for indexing and matching of motion data exist <ref type="bibr" coords="1,500.20,432.54,13.74,8.02" target="#b26">[27]</ref>, to date in practice, retrieval in motion data often still relies on textual metadata and not directly on the content. This also includes a lack of visual query formulation interfaces in practice. 3.) Identifying interesting sequences from the retrieved search results remains challenging. While in large motion databases, style variations typically exist, these are hard to distinguish automatically but often require the subjective interpretation by the expert. To that end, again, visual representations are expected to be useful. As yet however, they are not often leveraged in practice. To help domain experts with these problems, we developed Motion- Explorer as an ESS type system (Exploratory Search System, <ref type="bibr" coords="1,526.56,542.44,14.34,8.02" target="#b43">[43]</ref>) supporting large human motion capture data collections. The overall system is shown in <ref type="figure" coords="1,363.16,562.37,28.77,8.02">Figure 1</ref>. The development was guided by a design study and the system provides overview first (via clustering) and detail on demand (via search and drill-down) functionality at its core: @BULLET An interactive dendrogram visualization supports exploration of a hierarchical clustering and the steering of the aggregation level of the data set. Human motion between pose aggregates can be explored in a node-link diagram where poses define the nodes and edges represent motion. The views are interactively linked, and enable drill-down exploration. @BULLET Domain experts are able to visually search for motion sequences. The system provides a detail-on-demand visualization for human pose aggregates, which also serves as a query-by-example pool. Search results of human motion sequences can be interactively explored in detail for style variations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>@BULLET </head><p> We developed MotionExplorer in collaboration with domain experts , who provided the data set and defined the analytical challenges in their research workflow. With the domain experts, we acquired <ref type="figure" coords="2,37.86,300.95,3.32,7.37">1</ref>. MotionExplorer enables the exploratory search in human motion capture data. The pose hierarchy explorer (upper left) allows adjustment of the aggregation level of a hierarchical clustering and filtering of human poses. The motion explorer (upper right) shows human pose and movement aggregates. The search interface (below) allows for a visual query definition and an interactive search result exploration to identify style variations. high-level requirements at an early stage of the design. The collaborations in this two-year design study are shown in <ref type="figure" coords="2,207.22,352.60,29.01,8.02" target="#fig_1">Figure 2</ref>. The remainder of this paper is structured as follows. Section 2 reports on the targeted user domain and the collaborative requirement definition process. We then present related work in Section 3. Data abstractions are discussed in Section 4. The implementation results of the visual encodings and interactive capabilities are described in Section 5. Additionally, we report on how the outcome of a formative laboratory study with non-experts helped us successively improve the system in the design process. Finally, we conducted a summative field study with the domain experts presented in Section 6. Section 7 discusses our results, and Section 8 concludes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p> We conducted formative field observations and interviews with the domain experts to clarify the user needs on human motion synthesis, our targeted domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Analytical Tasks in Human Motion Synthesis</head><p> The motion synthesis products of our domain experts are often targeted towards computer animation. To this end, they need large numbers of camera takes to create human motion sequences. In the field of computer animation, the creation of realistic motion sequences by hand is a time-consuming task. For this reason, motion capturing of human actors has become a standard technique in computer animation. The motion sequences performed by the actor are recorded by a multicamera system. The 3D positions of the markers can be reconstructed from the two-dimensional images via triangulation for each frame. Reusing previously recorded data for motion synthesis has proven to be successful. These techniques are commonly based on the two following principles: Parametric synthesis generates motion spaces from a given set of example motion sequences. Allowing interpolation , the results can be adapted to specific spatial or temporal con- straints <ref type="bibr" coords="2,51.33,676.65,13.74,8.02" target="#b21">[22]</ref>. Concatenation of motion segments links together short motion sequences at appropriate locations to create a longer sequence. Using the latter approach, motion graphs <ref type="bibr" coords="2,179.63,696.57,14.94,8.02" target="#b22">[23] </ref> have become a standard technique in data-driven computer animation. A motion graph is a model that has a prominent role in many applications as an automated method of generating character animation, especially since motion graphs have a range of benefits for interactive applications. Technically , all possible transitions between given motion sequences need to be computed in advance. The transitions and the corresponding motion sequences are subsequently stored in a graph structure. Consequently , to create a new motion, sequences are concatenated to a path through nodes of the graph. An overview of the most popular routines and approaches in the field of example-based motion synthesis is found in the state-of-the-art report of Pejsa and Pandzic <ref type="bibr" coords="2,486.43,402.41,13.74,8.02" target="#b29">[30]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> 2.2 Identification of Current Problems / Limitations of Current Tools </head><p>The main drawback of data-driven motion synthesis approaches, such as motion graphs, is that the domain experts have no direct control over every single step that is performed during the synthesis. Firstly, there is no overview of the underlying motion capture database. Thus, much time is spent on checking the database for the motions. Secondly, the domain experts want to select possible paths through the graph. Thirdly, more sophisticated techniques are based on local statistical models on motion sequences. Here the domain experts want to identify meaningful sequences in terms of number, length and selection. For research purposes, many test data sets have to be generated . Usually, data are recorded for a specific project only and only reused as long as persons that recorded the data are available. To overcome these issues, a visual interface to the data is desirable. The domain experts should be enabled to get a quick overview of the data that can be refined for parts of special interest. In addition, the workflow could benefit from a simple searching interface to identify if motion sequences are available in the data set. Based on manually selected sequences, the domain experts would be able to apply local modifications to the motion graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Requirement Analysis</head><p>The requirements of the domain experts originally comprised four high-level questions that are addressed by our system: (1) Which variations of poses are available in the data set? (2) How can queries be specified intuitively? (3) Does a motion exist to re-use (or do I need to record new camera takes)? (4) Which style variations exist from start pose A to end pose B? In the course of the requirement analysis phase, we were able to report on the knowledge gain with respect to the research questions of the domain experts. In return, the domain experts helped us with the interpretation of the motion capturing data. In this phase, we discussed data sets, still images of rapid prototypes and continued with in-depth discussions. At the end of the requirement analysis phase after six months of collaboration, we were able to characterize the requirements on a finer level of detail. R 1 Overview: The system should provide a 'big picture' of the data. A global ordering would be nice to assess similar data items. R 2 Aggregation: To reduce the complexity, large data sets can be aggregated. The level of aggregation should be steerable.  R 7 Visual query formulation: Formulating example queries for motion sequence search should be as intuitively as possible. </p><p> R 8 Path search: A state of the art retrieval technique should be integrated to search for motion sequences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>This work relates to the wider field of automated and visual-interactive analytics. We focus on the analysis of time-oriented data <ref type="bibr" coords="3,236.99,427.22,9.52,8.02" target="#b1">[2]</ref>. We start with methods and techniques for time-series aggregation and clustering . We then focus on visualization techniques for graphs, hierarchies and glyphs, which have been proposed specifically to enhance the creation and/or use of clustering techniques. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Aggregation of Time-Oriented Data</head><p> One can distinguish three different methods for time-series aggregation: descriptor-based techniques to obtain compact representations, projection-based techniques to reduce dimensionality and clustering techniques to group the data. Our approach relates to all of the three aggregation concepts. Popular aggregation techniques based on time-series descriptors have been surveyed by Ding et al. <ref type="bibr" coords="3,161.53,557.09,13.74,8.02" target="#b11">[12]</ref>. However, most time-series descriptors have been defined for univariate time-series. In this work, we use a descriptor for multivariate time-series based on the recommendation of the domain experts <ref type="bibr" coords="3,149.75,586.98,13.74,8.02" target="#b23">[24]</ref> . Projection-based techniques reduce the dimensionality of the input data by the least significant com- ponents <ref type="bibr" coords="3,62.41,606.91,13.74,8.02" target="#b15">[16]</ref> . In contrast to descriptor-based techniques, projectionbased techniques are well suited for multivariate time-series <ref type="bibr" coords="3,252.87,616.87,13.74,8.02" target="#b42">[42]</ref>. In this approach, we project high-dimensional vector representations of human poses in 2D via the PCA projection technique just like it was presented for high-dimensional earth observation data <ref type="bibr" coords="3,233.76,646.76,9.52,8.02" target="#b7">[8]</ref> . In addition , we use the Self-organizing Maps method as a projection technique to create a similarity-preserving color legend for human poses <ref type="bibr" coords="3,31.50,676.65,9.52,8.02" target="#b6">[7]</ref>. Specifically visualization-driven approaches based on clustering multivariate segments are relatively scarce. Berkhin <ref type="bibr" coords="3,226.10,686.61,10.45,8.02" target="#b5">[6] </ref>provides an overview of clustering on multidimensional data in general, while Liao <ref type="bibr" coords="3,31.50,706.53,14.94,8.02" target="#b24">[25] </ref>analyzes how existing clustering techniques have been modified to support time-series data. For time-series clustering we can further distinguish techniques which cluster (a) complete series, (b) subsequences , or (c) single timestamps of input time series. Andrienko and Andrienko <ref type="bibr" coords="3,351.15,53.38,10.45,8.02" target="#b2">[3] </ref>present a raw-data-based approach for movement trajectory data. Scheepens et al. <ref type="bibr" coords="3,414.73,63.35,14.94,8.02" target="#b32">[32] </ref>present an aggregation method for multivariate trajectories based on continuous density surfaces. A technique for visual-interactive clustering of 2D time-dependent subsequences has been presented by Schreck et al. <ref type="bibr" coords="3,473.72,93.23,13.74,8.02" target="#b34">[34]</ref>. Turkay et al. <ref type="bibr" coords="3,294.12,103.20,14.94,8.02" target="#b39">[39] </ref> provide yet another perspective by examining the clusters as timedependent entities, based on a frame-by-frame clustering of all trajectories . For univariate time series, popular 'single-frame' aggregations like the SAX approach <ref type="bibr" coords="3,381.50,133.09,14.94,8.02" target="#b25">[26] </ref>have been proposed by Lin et al. They show that a time series can also be represented by a discrete symbolic alphabet, which can transform the analysis task to a state-transition analysis. Scherer et al. <ref type="bibr" coords="3,386.95,162.97,14.94,8.02" target="#b33">[33] </ref>present an exploratory search system where single frames of bivariate time series are clustered. In our approach , we apply hierarchical divisive clustering on single frames of multi-variate human motion capture data (human poses). In combination with the PCA-projection technique, we enhance multivariate time series (motion capture data) analysis by node-link-oriented state transition analysis. Using the vocabulary of the domain experts, we present an aggregation of both, the spatial (descriptor, clustering) and the temporal (projection, state-transition) domain of the data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Analysis of High-Dimensional Aggregated Data</head><p> Visualization techniques have been used in combination with timeseries clustering mostly for three purposes, all of which are relevant for our approach. The first purpose is the interactive steering to create or modify the clustering. The second purpose is to view its results. The third purpose is to use the clustering results to refine and filter views. Heinrich et al. <ref type="bibr" coords="3,346.06,336.01,14.94,8.02" target="#b19">[20] </ref> present a technique for the visual-interactive aggregation of genome sequence data, allowing the user to control the aggregation strategies. Guo et al. <ref type="bibr" coords="3,408.27,355.93,14.94,8.02" target="#b18">[19] </ref> combine multivariate and spatiotemporal information in a system for the semi-automatic generation of aggregations. Hierarchies created from clusters can be used to show the aggregated data on multiple levels-of-details, as it has been presented for vector field data <ref type="bibr" coords="3,391.46,395.78,13.74,8.02" target="#b38">[38]</ref>. Following this idea, our visualization of the state-transitions is also related to graph visualization <ref type="bibr" coords="3,503.47,405.74,13.74,8.02" target="#b40">[40]</ref>. In our node-link-diagram, we adopt Balzer and Deussen's idea of surfacemetaphors to represent cluster hierarchy levels as graphs <ref type="bibr" coords="3,505.19,425.67,9.52,8.02" target="#b4">[5]</ref>. In the works of Archambault et al. <ref type="bibr" coords="3,398.34,435.63,10.45,8.02" target="#b3">[4] </ref>and Abello et al. <ref type="bibr" coords="3,474.26,435.63,9.52,8.02" target="#b0">[1]</ref>, steerable graph hierarchy construction and drawing techniques are presented. Similar to our work, a top-down approach is used for the hierarchical aggregation . However, while we apply a global aggregation level, the authors chose unbalanced drill-down techniques. Most tree-visualization techniques <ref type="bibr" coords="3,335.44,485.45,14.94,8.02" target="#b35">[35] </ref>can naturally be applied to show cluster hierarchies. They have e.g., been adopted by biologists to show genome sequence clusters <ref type="bibr" coords="3,324.20,505.37,13.74,8.02" target="#b12">[13]</ref>. Bisson and Blanch <ref type="bibr" coords="3,416.34,505.37,10.45,8.02" target="#b8">[9] </ref> propose stacked trees for showing cluster hierarchy and content in a single visualization for cluster comparison. Seo and Shneiderman <ref type="bibr" coords="3,438.46,525.30,14.94,8.02" target="#b36">[36] </ref>present an example of a dendrogram-based view coupled with a heatmap showing the details of the cluster. We adopt this approach with respect to how dendrograms can be used for cluster inspection, while differing by the underlying data type. Ward <ref type="bibr" coords="3,326.52,577.02,14.94,8.02" target="#b41">[41] </ref>presents a taxonomy of glyph-based visualizations for high-dimensional data. He describes the strength, but also ascertains limitations of glyph placement strategies. The DICON framework <ref type="bibr" coords="3,529.55,596.94,14.94,8.02" target="#b10">[11] </ref>is a promising approach for cluster glyph visualization. Similar to our approach, the glyphs are used to provide meaningful high-level statistical information to represent multidimensional clusters. There are projection-based visualizations based on path metaphors for sequential n-grams <ref type="bibr" coords="3,338.65,646.76,13.74,8.02" target="#b42">[42]</ref>, text and music collections <ref type="bibr" coords="3,451.66,646.76,14.19,8.02" target="#b28">[29,</ref><ref type="bibr" coords="3,467.70,646.76,10.64,8.02" target="#b37"> 37]</ref>, multivariate time series <ref type="bibr" coords="3,317.04,656.72,9.52,8.02" target="#b7">[8]</ref>, metabolic pathways <ref type="bibr" coords="3,406.46,656.72,14.94,8.02" target="#b9">[10] </ref>and motion capture data <ref type="bibr" coords="3,513.60,656.72,14.19,8.02" target="#b20">[21,</ref><ref type="bibr" coords="3,530.29,656.72,10.64,8.02" target="#b30"> 31]</ref>. While these approaches convince in the differentiation of multivariate data items, challenges in preventing occlusion for large data sources may remain. In our approach, we determine the layout of the statetransition graph by a projection, providing a map-metaphor for an 'information landscape' for high-dimensional data. We discarded the alternative use of graph layout methods (see, for example <ref type="bibr" coords="3,497.68,716.50,14.94,8.02" target="#b17">[18] </ref> for a recent survey), because they do not convey any additional information on the position of the nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DATA ABSTRACTION AND FUNCTIONAL SUPPORT</head><p> In this section, we describe the data processing capabilities of the system . We present a brief overview of the choices made for the feature extraction, the data structures, the aggregation techniques, and the search functionality. For research purposes, multiple motion capture databases are publicly available. In our experiments we use data obtained from the HDM05 motion capture database <ref type="bibr" coords="4,202.83,212.98,13.74,8.02" target="#b27">[28]</ref>. This database is a systematic recording of a wide variety of motions performed by various actors, in multiple repetitions. Many instances of similar human movement are also available as sets of extremely varied behaviors . Thus, the database provides a good basis for investigating the effectiveness of our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Feature Vectors of Human Poses</head><p>In agreement with the domain experts, we define a single human pose as one 'atomic' data object, see <ref type="figure" coords="4,143.98,305.14,33.86,8.02" target="#fig_2">Figure 3a</ref>. After the import of the data source (binary C3D-files), motions are represented as sequences of single poses. In the following, we present preprocessing routines and feature extraction steps as suggested by the domain experts. In a normalization step, the pelvis of each human pose is shifted to the origin of the coordinate system. To obtain poses that are always viewed 'head-on', each pose is normalized by a rotation around the z-axis. For overview and search, the domain experts recommend the extraction of a compact feature vector (FV) representation which keeps most of the original information. For this reason, the set of 3D-markers of each human pose is reduced to a relevant subset <ref type="bibr" coords="4,177.96,404.77,13.74,8.02" target="#b23">[24]</ref>. We obtain a FV with 48 dimensions containing 16 3D marker coordinates in absolute scale (cm unit). FVs will be the basis for subsequent aggregation techniques and the search functionality, respectively. In agreement with the domain experts, we use the Euclidean distance measure as the default similarity function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Hierarchical Aggregation of Human Poses</head><p>A pose cluster (<ref type="figure" coords="4,82.55,486.97,33.52,8.02" target="#fig_2">Figure 3b</ref>) contains large numbers of similar human poses. In the vocabulary of our targeted domain, a cluster represents a spatial aggregation of the data. It is the most important data structure in the analytical and exploratory components of the system. The domain experts can directly interact with clusters using the visualizations of MotionExplorer. MotionExplorer also provides statistical information to assess cluster quality. MotionExplorer uses a hierarchical clustering algorithm to create aggregations, which can be inspected on multiple levels of detail (R 2 ). The pose cluster hierarchy (see <ref type="figure" coords="4,139.34,576.92,33.72,8.02" target="#fig_2">Figure 3c</ref>) is calculated in a separate thread using a divisive clustering algorithm to scale for large data sets <ref type="bibr" coords="4,22.50,596.85,13.74,8.02" target="#b13">[14]</ref> . That way, the coarsest clusters are promptly available for inspection . The domain experts may control two clustering parameters <ref type="bibr" coords="4,259.92,606.81,9.71,8.02" target="#b5">[6]</ref>: @BULLET (a) which cluster is to be split next @BULLET (b) the splitting strategy for a particular cluster Since we want to obtain compact clusters, we define (a) splitting the cluster with the maximum standard deviation as a default. Alternatively , we suggest splitting the cluster with the highest number of elements . Concerning (b) we follow a representative approach and apply a k-means clustering with k = 2 as a default. Based on an inquiry with the domain experts, we provide both parameters as steerable. Hence, the data aggregation process becomes interactively adaptable. In an early prototype of MotionExplorer, the algorithm performance of the divisive hierarchical clustering was compared with a k-means  (partitioning) and DBScan (density-based) implementation. Two advantages of the divisive clustering became apparent: (a) the level-ofdetail concept can be achieved with a single calculation and (b) the hierarchical structure allows for multiple levels of detail with the same data elements in respective sub-trees. We did not detect differences in the results, whether we tested with data with similar human poses or sets of extremely varied behavior. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Domain-specific Data Structures and Functionality</head><p> In addition to the described spatial aggregation, we provide an aggregation of the temporal data component. The clusters are used to create the adapted motion graph metaphor as a directed cluster graph (R 4 ). With the clusters defining the nodes, two nodes are connected if there is at least one motion sequence in the database, which connects the corresponding poses (see <ref type="figure" coords="4,379.98,316.46,33.58,8.02" target="#fig_2">Figure 3d</ref>). For each level of detail in the cluster hierarchy, an individual graph is calculated. A last step concerns the search functionality (R 8 ). A crucial step for data-driven methods are fast searching techniques which allow the domain experts to efficiently and effectively retrieve motion capture data of interest. The domain experts mostly apply a searching technique with a complexity of k log(n), where n is the size of the database, and k is the number of nearest neighbors. We adapt a technique by Krüger et al. <ref type="bibr" coords="4,305.07,396.16,14.94,8.02" target="#b23">[24] </ref>and exploit the generated cluster data structure as an index. First, all sequences are identified that contain the start cluster and the end cluster of the query in the right order. Then, the sub-sequences surrounded by the two cluster centroids are returned as the search re- sult. The result of this phase is a data abstraction and a functional adaption to the domain. Each decision in the development phase was made in collaboration with the domain experts. In the following section, we report on the visual encodings of the targeted system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUAL MAPPINGS AND INTERACTIVE FUNCTIONALITY</head><p>Based on the data abstraction described in Section 4 we now report on the different visualizations (Sections 5.1 -5.4) and interaction methods (Section 5.5) of MotionExplorer. We conclude this section with a summarization of the design process during the implementation phase. Our approach to the visual mapping process is in line with Fuchs and Hauser's recommendation to create multiple visualizations to support multiple operations and show all important aspects of the data <ref type="bibr" coords="4,285.12,576.66,13.74,8.02" target="#b16">[17]</ref>. Our data models and visual representations are also inspired by Elmqvist and Fekete <ref type="bibr" coords="4,360.56,586.62,13.74,8.02" target="#b13">[14]</ref>, where a model for building, visualizing and interacting with multi-scale representations of high-dimensional data is presented. The ability of meaningful data aggregate visualizations is regarded crucial to deal with highly aggregated data. Moreover, providing multiple levels of detail is seen important in current visual aggregation approaches. MotionExplorer is developed for our domain experts, but there is potential for use beyond this domain. However, we continue to denote the users of the visual-interactive capabilities as domain experts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A Cluster Glyph for Human Poses</head><p> Inspired by the human anatomical drawing by Leonardo da Vinci (Vitruvian Man), we decided to use a circular design for the cluster glyph (see <ref type="figure" coords="4,302.61,716.50,29.71,8.02" target="#fig_5">Figure 4</ref>). We encode the information for cluster interpretation and visual data summary based on the preference of the domain experts (R 3 ). We show the cluster centroid as a human stick-figure pose, and the set of poses in the cluster as deviating, transparent figures, which is important for the domain experts. We limit the number of shown poses for each cluster to a maximum of 500. Based on an inquiry with the domain experts, the sampling method is in favor of the nearest neighbors to the centroid. For cluster quality assessment, we use the quantization error of a cluster as a measure of compactness, represented by a label at the bottom left. The relative cluster size is displayed by the size of the surrounding circle, the absolute size is labeled at the bottom right. Finally, we color the cycle of each cluster glyph to illustrate similarity among the clusters. The color value depends on the cluster centroid vector, and follows the system-global, similarity-preserving color legend at the bottom left corner of the system for a straightforward look-up (see <ref type="figure" coords="5,169.91,433.66,28.82,8.02">Figure 1</ref>). The grid of the color legend is the result of a SOM that is trained using all feature vectors in the manner of a vector quantization scheme <ref type="bibr" coords="5,207.74,453.58,9.52,8.02" target="#b6">[7]</ref>. Thus, the most prominent human poses are arranged on the grid structure for a quick overview. We use the 2D colormap proposed by Ziegler et al. <ref type="bibr" coords="5,257.41,473.51,14.94,8.02" target="#b44">[44] </ref>to make effective use of the RGB color space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Exploration of Hierarchically Clustered Human Poses</head><p>The pose hierarchy explorer meets the following requirements: @BULLET it serves as a global overview (R 1 ) by representing the tree structure of the cluster hierarchy @BULLET it enables filtering to narrow down the number of elements (R 6 ) @BULLET it provides interactive level of detail functionality (R 2 , R 5 ) Starting with the root cluster at the very top, sub-tree structures are displayed top-to-bottom. Clusters are represented with the pose aggregate glyph that helps to provide the global overview of the data set (see <ref type="figure" coords="5,60.38,606.90,28.89,8.02" target="#fig_6">Figure 5</ref>). We provide an aggregation slider to define the number of shown clusters (see <ref type="figure" coords="5,78.67,626.83,29.32,8.02" target="#fig_8">Figure 7</ref>). Depending on the slider position, the highest cluster of each subtree below the current slider position is rendered. Elmqvist and Fekete describe this approach as 'level traversal' <ref type="bibr" coords="5,264.69,646.76,13.74,8.02" target="#b13">[14]</ref>. The level of each cluster (the depth in the tree) depends on the rank of their standard deviations and thus on the order of the split criterion in the clustering. As a consequence, the vertical axis of the hierarchy explorer encodes the standard deviation ranking of each cluster. This 'position-by-ranking' method was preferred by the domain experts in favor of an allocation by the binary depth. It provides the additional analytical benefit of identifying the cluster compactness ranking along the vertical axis. This is also the main advantage of the dendrogram over a Windows Explorer-like visual encoding of the clustering tree. An interaction with the vertical aggregation slider drills-down or rolls-up the number of visible clusters, respectively. Thus, the domain expert can obtain different levels of granularity. The effects of this aggregation level hold for the complete system. While approaches with local aggregation capability in single sub-trees exist <ref type="bibr" coords="5,486.22,364.14,10.45,8.02" target="#b0">[1] </ref><ref type="bibr" coords="5,499.47,364.14,9.52,8.02" target="#b3">[4]</ref> , Motion- Explorer provides a global aggregation level. With this specification, domain experts can always analyze clusters of equivalent standard deviations , which serves as an orientation in the assessment of style variations . If a local drill-down is needed, two approaches are possible: 1) present a detailed overview first and then exclude irrelevant data by filtering, or 2) start with a coarse aggregate and then explicitly open subtrees for more detail. This interesting design question also depends on the application goal. We plan to further examine the applicability of local and global aggregation level adaption in the future work. The domain experts may exclude or include parts of the hierarchy from the view to focus on relevant parts of the dataset. Filter operations always apply to a cluster and its subtree. Filtered clusters are indicated as gray pose aggregates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis of Human Motion Sequences</head><p>The motion explorer (see <ref type="figure" coords="5,385.65,523.96,29.18,8.02" target="#fig_7">Figure 6</ref>) serves as a complementary view to the hierarchy explorer. It shows human motion sequences between the aggregated poses that exist in the data set. The visual encodings fulfill the following requirements: @BULLET overview of all poses (R 1 ) at the current aggregation level (R 5 ) following the motion graph metaphor (R 4 ) @BULLET global order of all poses to intuitively assess the similarity @BULLET interactive means to avoid local overplotting @BULLET support of visual query formulation (R 7 ) </p><p> A node-link diagram provides a global overview of the aggregation level (R 1 ), where nodes represent the human pose aggregates and edges show existing motion sequences. Domain experts are able to explore the diversity of poses and the available motion transition between them, thus meeting requirement R 4 . Concerning the node layout, the domain experts consider the global ordering of human poses as most important. Although close arrangement of two poses produces visual overplotting, the domain experts confirm the analytical importance of identifying dense regions on the map. Thus, we allocate the node positions via the linear and deterministic PCA projection in favor of force-directed layout algorithms. High-dimensional cluster information is projected in 2D by means of , the domain expert drilled down the level to 17, and 39, respectively. Zooming and filtering was applied to focus on relevant human motion. Various cycles of human movement exist. For example, at the aggregation levels of 17 and 39, a 'cartwheel' motion beginning with an upright stand was identified at the bottom left. topology-preservation. A benefit of the deterministic projection is the reduced transformation-cost when the set of displayed clusters is modified by the domain experts. To avoid overplotting, a zooming technique and a local prevent-overlap functionality were selected. The zooming technique is combined with an overview at the upper right. This enables the domain experts to navigate through the human poses and to explore relevant poses on various levels of detail (R 5 ). The local prevent-overlap functionality (also called 'flower-metaphor') rearranges colliding human poses as an overlap-free local layout around the pose in focus. An example is shown by the green and blue poses in <ref type="figure" coords="6,31.72,516.65,29.01,8.02" target="#fig_7">Figure 6</ref>. We provide a detail-on-demand view on a single pose, triggered by a mouse click. The enlarged visual representation of a cluster glyph allows the domain experts to explore details of a single pose cluster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Exploratory Search</head><p>The motion search interface enables the domain experts to: @BULLET run the human motion retrieval algorithm of the domain experts (R 8 ) based on visually formulated queries (R 7 ) @BULLET analyze style variations in the search result (R 9 ) based on exploratory analysis capability @BULLET drill down the number of retrieved sequences to the number of relevant results (R 6 ) The search interface at the bottom of the system (see <ref type="figure" coords="6,230.21,656.72,30.04,8.02">Figure 1</ref>) is divided into (a) a field that represents the start pose, (b) the motion search result list, (c) the end pose field and (d) the pose bundle animation . To enable visual query definition for the search interface, we provide a drag-and-drop technique by which example poses can be dragged into the start and the end pose of the search panel to start the search. This allows for visual search operations on large data sets with the effort of only two mouse interactions. Furthermore, the detail-ondemand view also provides an interface to define the pose either as start or as end pose for the query. As soon as the domain experts have defined a query by example (R 7 ), the retrieval algorithm of the domain domain experts is executed (R 8 ). Resulting motion subsequences are arranged in the motion search list interface (see <ref type="figure" coords="6,460.35,446.91,29.26,8.02" target="#fig_3">Figure 9</ref>). While the visual query formulation for the search is based on aggregated poses (clusters), the search result provides motion sequences on the granularity of single frames of the data set. The frames are separated by black vertical lines. Since the query formulation only assigns the start and the end pose, there are usual variations in the course of the retrieved motion sequences. In consultation with the domain experts, resulting motion sequences are warped to fit with the horizontal display space per default. This enables an easy comparison of retrieved sequences along the relative time axis. The display of sequences in an absolute arrangement of time is also possible. In this case, each frame in the result visualization has an identical length but only the longest retrieved sequence spans the complete display space along the time axis, see <ref type="figure" coords="6,458.59,576.72,29.25,8.02" target="#fig_3">Figure 9</ref>. Inspired by the idea of coloring queried sequences by an intrinsic property <ref type="bibr" coords="6,520.55,586.69,14.94,8.02" target="#b19">[20] </ref><ref type="bibr" coords="6,285.12,596.65,13.74,8.02" target="#b14">[15]</ref>, we again apply the similarity-preserving color legend and color the resulting subsequences according to the cluster affiliations. For an interactive exploration of various resulting sequences, a time slider is provided. By dragging the slider, an animation of all corresponding result poses is shown in the pose bundle animation view (see <ref type="figure" coords="6,512.08,636.50,23.41,8.02;6,285.12,646.46,4.48,8.02">Figure  1</ref>bottom right) and the domain experts can observe style variations between different motions (R 9 ). A filter function enables the domain experts to reduce the number of style variations (R 6 ). Inverting the filter status is also possible. For additional information on single subsequences , a focus-and-context interaction is implemented. By focusing the result list, the currently accessed subsequence is enriched by a key frame representation of the particular human motion. This search interface allows our domain experts to find interesting human motion style variations across different results (R 9 ). The application examples and <ref type="figure" coords="6,360.43,736.42,30.13,8.02" target="#fig_3">Figure 9</ref>further illustrate this aspect. <ref type="figure" coords="7,31.50,194.67,18.90,7.37" target="#fig_3">Fig. 9</ref>. The search interface with search results in a relative (upper) and an absolute (lower) representation. Two style variations can be identified. One short sequence from light green to dark green and a long sequence from light green to yellow to dark green. Focus and context is applied to show the frames of a single search result. The animation slider enables the domain experts to analyze the temporal variations of the result. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Linking Views</head><p>The system links together the visual encodings of clusters and poses in all views (R 10 ). If domain experts change the aggregation level in the pose hierarchy explorer, the motion explorer is automatically adjusted to the new level of detail. Excluded clusters are grayed out in the hierarchy explorer and removed from the motion explorer. With a click on one of the clusters in the color legend, the system automatically adapts the aggregation level to the granularity that fits with the clicked pose of interest. Thus, the domain experts are able to start their analysis with a click on an interesting pose at the color legend. If a cluster in one of the views is focused, the cluster pose is also highlighted in every view where the respective cluster is visualized. This is especially helpful if the number of clusters is large and the color information is no longer sufficient to identify identical clusters in multiple views. We designed the search algorithm to ignore the current filter status, and always search on the entire data set. Finally, we attach tool tips to all active elements of the system to make the system self-explanatory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Design Process</head><p>The design process was highly iterative. We conducted a formative laboratory design study during the implementation phase of Motion- Explorer in order to optimize the design choices. As a result, a variety of visual mappings and interactive capabilities were refined in the course of the process. 14 non-experts were asked to complete a questionnaire with 57 questions about visual encodings and interactivity preferences. Based on still images, we conducted informal interviews with the participants, which partially led to creative discussions beyond the concerns of the questionnaire. We observed the participants, using the prototypical system to solve analytical tasks or to communicate perceptual and cognitive preferences. We now describe improvements based on the results of the qualitative study. Pose hierarchy explorer One of the heavily discussed design aspects was the visual representation of the pose hierarchy explorer. During the development process, Windows Explorer-like and treemap-like representations were presented to the participants. In the end, the latter representation was rejected because of the aggregation slider concept. Different visual representations of a cluster with respect to the changing aggregation level were another matter of concern. For parent and child clusters in the hierarchy, a trade-off between showing important information and an over-representation was achieved. Motion explorer Since the domain experts preferred the similarity-preserving projection in favor of a force-directed layout, we discussed local overplotting avoidance strategies in the interviews. We list six considered design choices in order of preference from worst to best: (6) a global slider to adjust the glyph sizes, (5) focus and hide neighbors, (4) focus and highlight, (3) semantic zooming by locally adapting the aggregation level, (2) focus for a overlap-free local layout -the 'flower-metaphor', (1) geometric zooming + small zooming window. In the end, we decided for the last two choices. </p><p>Motion search interface The capability of the visual-interactive definition of queries was an important concern, the choices were: (4) left click and right click to assign start / end pose, (3) an interactive context menu, (2) detail-on-demand + assign start / end pose, (1) dragand-drop . Again, the last two choices were implemented. Linking views We recognized the need to choose visual encodings for the clusters that are as similar in all views as possible. In the end, we used the cluster glyph wherever possible. Additionally, brushing and linking is now used to indicate clusters in multiple views. Based on an inquiry with many participants of the design interviews, the color legend was made 'clickable'. As a result, the system now adjusts the aggregation level to fit best to the clicked cluster. This functionality can serve as a starting point for data exploration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CASE STUDY</head><p> The motivation for the case studies was the question of how Motion- Explorer helps the domain experts with their research. We were interested in finding out if the domain experts would be able to identify their data collections and if the visual encodings and the interactive functionality is intuitive and usable. We were also interested in whether the domain experts would accept the system as a tool for decision-making in their scientific work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Field Study</head><p> We conducted an insight-based summative field study with five domain experts of human motion synthesis at their lab. Due to the small number of participants, we will not discuss our findings in terms of quantitative measures. The procedure was as follows: After a familiarization , we introduced the domain experts to the purpose of the field study. We presented an introduction to the tool and to the supported analysis tasks. We divided the study into two parts. Each participant was asked to (a) perform four specific tasks and after that (b) was invited to run the system in an exploratory manner on her own data sets without supervision. The data collection for the task-based tests is presented in <ref type="figure" coords="7,524.79,586.98,19.70,8.02;7,294.12,596.94,3.36,8.02" target="#tab_1">Table  1</ref>. The four tasks were to (1) become confident with the system, to (2) identify a 'T-pose', to (3) formulate a visual query of a 'jumpingjack' motion, and to (4) identify style variations in the search result. Concerning (1) we collected informal feedback as to how the domain experts familiarized themselves with the system. The identification of the 'T-pose' (2) was successfully accomplished by all domain experts, the slowest participant needed two minutes. Recalling the number of human poses in the collection, this means a 'browsing speed' of at least 150 human poses per second when MotionExplorer is used for the identification of poses. A 'T-pose' is shown at the upper right of <ref type="figure" coords="7,521.08,686.61,23.41,8.02;7,294.12,696.57,4.48,8.02" target="#fig_9">Figure  8</ref>(red pose). Most domain experts clicked on the color legend as the entry point to focus on a particular pose. The result of a 'jumpingjack' search can be seen in the title <ref type="figure" coords="7,424.69,716.50,29.40,8.02">Figure 1</ref>. The task of identifying style variations in the search result proved that both the absolute and the relative search result representation was used. Subsequent to the <ref type="figure" coords="8,22.50,213.23,24.22,7.37">Fig. 10</ref>. Exploration of micro style variations (rotating arms). Many micro-variations within the green poses can be explored. However, the yellow and the magenta poses are traversed by each motion sequence. task-based tests, the participants were requested to use the system with their own data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results of the Data Exploration</head><p> The exploratory part of the field study was conducted on data collections recommended by the domain experts. Thus, we were able to observe the domain experts using the system in a real-world setup. To enhance the outcome of this insight-based method, we collected still images, photos and videos. Most interesting for us was determining how fast the domain experts would familiarize themselves with the system, and what data findings or insights would occur. We present five notable observations made when the domain experts performed complex analysis tasks in <ref type="figure" coords="8,116.01,381.32,25.30,8.02" target="#tab_2">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY</head><p>We present a qualitative summary of our findings in the field study and subsequently discuss the approach of this design study in general. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Generalization of Field Study Results</head><p>In our summative field study, we applied MotionExplorer in a variety of search tasks. The exploratory search results differed due to variations in the applied data sets, candidates and tested task completion tests. We believe that the decision to allow users the exploration of datasets of their own choice paid off. We obtained valuable qualitative feedback. The basic idea of the case study was to observe whether the system meets the domain experts' requirements. In addition, we hoped that the domain experts would acknowledge benefits of the system. New Data Findings 'I have found way more style variations in my data than expected. For me, locally adapting the motion graph model has become even more important' The domain experts report on a surprisingly large number of style variations within their search results. Filtering unwanted retrieved motion sequences in the search result helped the domain experts to obtain motion sequences of interest. They liked the exploratory nature of the <ref type="figure" coords="8,285.12,213.16,24.39,7.37">Fig. 11</ref> . Observation of a domain expert exploring offense and defense behavior of boxing movement. The search result presents two sequences that contain style variations with similar behavior. Here, filtering was heavily applied to better resolve boxing motions in detail. The red pose (focused in the search result) is highlighted in the upper views. motion explorer. Based on the provided overview of their data, the domain experts identified the capability to decide which motion sequences are still missing in their data collection and should be recorded in the near future. </p><p>Communication 'Can I take screen shots with this tool? I would like to show my analysis results to my colleagues' The domain experts identified the ability of the system to help communicating search and analysis results. Domain experts reported on the need to perform plausibility checks and identified MotionExplorer as a suitable communication medium. Hypothesis generation on pose and motion variations is a common task for the domain experts. The possibility to visually verify hypotheses and to communicate the outcome was appreciated. Usability 'It simply fits very well. Usually we start with reading tutorials' The domain experts were pleasantly surprised that they learned to use the system very quickly. They welcomed the intuitive description of the cluster glyph of human poses and the interplay of the different views. The domain experts considered most of the visual encodings appropriate and identified most of the interactive capabilities without assistance. The attachment of tool tips turned out to be beneficial to lookup the functionality of previously unknown active elements. After the task completion tests, all domain experts familiarized with the system and were thus able to work autonomously. Effectiveness 'I've been working with motion capture data for eight years now, but this perspective on the data was really enriching. I had a lot of fun with the tool' We recognized a curiosity about the system among the domain experts , who recognized the components of the system and their interplay as helpful. The pose hierarchy explorer and the motion graph in combination form a helpful solution to obtain a global overview. Filter and zooming functionality were readily used to drill-down to local features in the data. The domain experts liked visualizing the data in detail to analyze problems with their currently applied models. As demonstrated in the case study, domain experts successfully performed a re-identification of known data aspects. Before, they had to choose statistical means to approach similar problems. Efficiency 'No need to analyze poses and sequences by hand. Producing an effective result in only a few minutes. I like that' The increased efficiency is maybe the most substantial benefit of the system. A reduced effort in performing analytical tasks was recognized by all domain experts. Gaining fast insight into the complex data type of human motion sequences by the overview visualizations was widely accepted, especially if the data collection is large. The identification of human poses, motion sequences and style variations thereof Long uncut camera take of an actor; previously known motion One expert explored a previously known uncut camera take. With the motion explorer, she identified style variations on a macro level. Different paths were identified to get 'from A to B', <ref type="figure" coords="9,398.07,91.22,23.61,6.23" target="#fig_9">Figure 8</ref>details. The domain expert was affirmed when she identified the expected 'cartwheel' motion cycle at an aggregation level of 17. One analyst was interested in style variations of 'rotating-arms' motion at the micro-level. As can be seen in <ref type="figure" coords="9,169.58,127.48,26.56,6.23">Figure 10</ref> , there are variations in the green phase of the motion sequence. Apparently the actor of the cutsequences performed various body configurations while her arms pointed at the ground. </p><formula>R 1 , R 2 , R 3 , R 4 , R 5 , R 6 , </formula><formula>R 1 , R 2 , R 3 , R 4 , R 5 </formula><p>Exploration of boxing poses Collection of cutsequences , including boxing Another domain expert was interested in the exploration of cut sequences containing boxing motions, the observation is shown in <ref type="figure" coords="9,221.38,154.78,25.62,6.23">Figure 11</ref>. After an information drill-down, a query with two different defensive stance poses was executed (orange and purple). The first and the third retrieved sequence show similar behavior. We observed a domain expert looking for missing cross-country skiing motion data. She set the aggregation level to 26 (see <ref type="figure" coords="9,216.26,182.08,23.17,6.23" target="#fig_6">Figure 5</ref>), two sub-graphs are visible in the motion explorer (see <ref type="figure" coords="9,405.23,182.08,23.17,6.23" target="#fig_7">Figure 6</ref>). Apparently, no human motion data is available that connects the two sub-graphs, i.e. body rotation. It might be recorded in future. </p><formula>R 1 , R 2 , R 3 , R 4 , R 5 , R 6 , R 7 , R 8 , R 9 , </formula><formula>R 1 , R 2 , R 3 , R 4 , R 5 , R 10 </formula><p>Searching for a pose to connect two different motion sequences Cut-sequences of two different motion sequences In this use case, a domain expert explored two different human motions for a common pose. By identifying this pose, a synthesis of both motions could be created. The result is shown in <ref type="figure" coords="9,380.91,218.34,22.83,6.23">Figure 1</ref>, the green upstand pose at the bottom right fulfills this criterion. Based on this finding, the domain expert may now connect the red pose and the purple pose to a new synthesized human motion in a subsequent work step. </p><formula>R 1 , R 2 , R 3 , R 4 , R 7 , R 8 , R 9 , R 10 </formula><p>was considered simple, which was quoted as very time-consuming in the past. We acknowledge that not all participants were able to perform a search with the minimal costs of five clicks. Nevertheless, we report on a significant improvement of efficiency, especially for large collections. E.g. the identification of human 'T-poses' in an unknown data set (and a new system) took between seconds and minutes. The domain experts stated that going through 20,000 human poses would usually take the domain experts several hours. To conclude, the aggregation of multiple occurring poses and motions considerably reduces the work load for going through such large data collections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Discussion and Extension Possibilities</head><p> Our developed system supports overviewing and searching in a complex type of data; human motion capture data. To the best of our knowledge, our system defines for the first time exploratory search and analysis for large motion data based on a customized and interactive data clustering approach. Our defined visual display integrates cluster visualization by a custom glyph visually aggregating sets of poses. It uses a graph representation to show similarities of pose clusters together with state transitions. We believe visual-interactive analysis in motion data can help to better understand and re-use large repositories of motion data. Considering new technologies for motion tracking, we expect much more motion data, inducing the need for scalable retrieval and analysis techniques to become available in the future. As shown qualitatively, our system could be operated by expert and non-expert users in an intuitive and effective way. The received comments indicate that the approach can benefit tasks in motion analysis. The motion data representation in our case is an instance of multivariate , time-dependent data. This data is typically hard to visualize. In our case, we could draw on representing the motion poses by a stickfigure-oriented glyph, which is easy to interpret by users, as opposed to more abstract high-dimensional visualizations, which may be harder to interpret by users, especially in case of large data sets. We also identified a number of limitations of our current study that should be investigated as a next step. First, pose clustering in our approach is done by interactive clustering. This relies on the user being able to find useful cluster parameters and interpret the obtained cluster. Also, automatic classification of poses should be integrated. Based on training data, one could train classifiers to automatically, and possibly more robustly, segment different poses. Also, the identification of style variations at the micro level may be difficult to do only visually, especially if variations are all mapped to the same glyph. In densely populated glyphs it may be difficult to visually discriminate the styles. Again, automatic classification could be useful to this end. Our state transition view currently uses the PCA to project the pose clusters to 2D space. This may induce overlap and make it difficult to get an overview of complex state transition graphs. Constrained layout or space filling approaches should be considered to scale for larger transition graphs. In effect, loosing some positional similarity may not be expensive, considering the gain in display space for larger data sets. Also, the motion glyph or transitions could be enriched with meta data (if available) of the motion recordings. If certain poses have been pre-classified, this should be included in the visualization. Interactive capability for efficiently labeling glyphs and transitions is also possible . We can also imagine several promising extensions to increase the analytical potential of the system. For example, if we consider motion data captured in a soccer game, additional data could be captured on the result of a certain motion. E.g, a players motion led to a goal, the player missed the goal, or a player was injured. We imagine that through appropriate visual overlays, correlation analysis between motion and respective results can be useful for training purposes. Finally, we performed a qualitative study involving mainly university students on the non-expert side. This may be seen as a bias in the considered user population. More formal studies, including also quantitative measurements on more precisely defined tasks, are desirable to further assess the system capabilities. For example, we may more closely incorporate our analysis and search system into workflows for interactive motion synthesis and observe the improvement potential. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>We presented MotionExplorer, an exploratory search system for large data collections of motion capture data. The system provides an overview of human poses in a dendrogram visualization that represents the result of a hierarchical clustering. A node-link diagram enables the user to analyze human poses as nodes, where each node shows a collection of similar pose instances by a stick-figure-oriented glyph. Links between the pose glyphs indicate motion sequences and allow exploration of the poses in context. Visual interfaces for query formulation and the exploratory search result analysis support the user in retrieving human motion sequences and style variations thereof. We evaluated the system by laboratory design interviews with non-experts, and in a field study with domain experts. Based on the field study, the experts confirm the usability and efficiency of the system to support their day-to-day work. Analysis results within the field study also presented new aspects of the data that had not been taken into consideration or had not been visually communicated before. A number of limitations and extension possibilities of our work that should be considered for future work have been discussed in the preceding section. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,22.50,300.95,513.00,7.37;2,22.50,310.42,513.00,7.37;2,22.50,319.88,513.00,7.37"><head>Fig. </head><figDesc>Fig. 1. MotionExplorer enables the exploratory search in human motion capture data. The pose hierarchy explorer (upper left) allows adjustment of the aggregation level of a hierarchical clustering and filtering of human poses. The motion explorer (upper right) shows human pose and movement aggregates. The search interface (below) allows for a visual query definition and an interactive search result exploration to identify style variations. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,31.50,91.09,229.55,7.37"><head>Fig. 2. </head><figDesc>Fig. 2. Collaboration with users in the course of this design study. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,36.98,205.26,6.47,8.06;3,43.46,205.36,238.41,8.50;3,51.43,215.33,208.89,8.02;3,36.98,229.61,6.47,8.06;3,43.46,229.72,238.42,8.50;3,51.43,239.68,193.98,8.02;3,36.98,253.96,6.47,8.06;3,43.46,254.07,238.42,8.50;3,51.43,264.04,182.14,8.02;3,36.98,278.32,6.47,8.06;3,43.46,278.43,238.41,8.50;3,51.43,288.39,208.72,8.02"><head>R 3 </head><figDesc>Cluster glyph: Data aggregations should be displayed as visual structures. These should allow for an intuitive assessment. R 4 Motion graph: This metaphor should be integrated. Poses should be displayed as nodes and motion sequences as edges. R 5 Level of detail: Local behavior of the data should be explorable. This enables the domain experts to analyze details. R 6 Filtering: It should be possible to exclude and re-include data. Filtering helps to concentrate on relevant parts of the data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,36.98,351.38,6.47,8.06;3,43.46,351.49,238.41,8.50;3,51.43,361.45,191.97,8.02;3,34.49,375.74,6.47,8.06;3,40.96,375.84,240.91,8.50;3,51.43,385.81,187.48,8.02"><head>R 9 </head><figDesc>Search result exploration: Style variations of different retrieved sequences are interesting and should be recognizable. R 10 Multiple visual data representations: Different aspects of the data should be shown side-by-side in the same view. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="4,22.50,116.67,250.38,7.37;4,22.50,126.13,249.65,7.37"><head>Fig. 3. </head><figDesc> Fig. 3. Data types provided: (a) single human pose/frame, (b) pose aggregate/cluster , (c) hierarchical clustering tree (d) cluster motion graph. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,285.12,146.14,250.38,7.37;4,285.12,155.60,220.66,7.37"><head>Fig. 4. </head><figDesc>Fig. 4. Cluster Glyph for Human Poses. The style variations of human poses around the cluster centroid are shown by opacity bands. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="5,31.50,267.31,250.38,7.37;5,31.50,276.78,250.38,7.37;5,31.50,286.24,250.38,7.37"><head>Fig. 5. </head><figDesc>Fig. 5. Pose Hierarchy Explorer. The result of a hierarchical clustering is visualized as a dendrogram. Domain experts can adapt the number of shown clusters (currently 26) by dragging the aggregation level slider. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="5,294.12,267.09,250.38,7.37;5,294.12,276.55,250.38,7.37;5,294.12,286.01,250.38,7.37;5,294.12,295.48,250.38,7.37;5,294.12,304.94,250.38,7.37"><head>Fig. 6. </head><figDesc>Fig. 6. The motion explorer shows human poses as nodes and motion sequences as edges. In this example skiing poses are represented from a lateral and a frontal view. The projection-based layout clearly distinguishes between the two postures. The prevent-overlap function is active in the sub-graph at the bottom right, based on the mouse focus. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="6,22.50,198.25,513.00,7.37;6,22.50,207.71,513.00,7.37"><head>Fig. 7. </head><figDesc>Fig. 7. Pose Hierarchy Explorer. The result of a hierarchical clustering is visualized as a dendrogram visualization. Domain experts can change the level of detail by dragging the aggregation level slider. Here, poses of skiing movement were drilled-down to the aggregation levels of 5, 15, and 30. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="6,22.50,378.18,513.00,7.37;6,22.50,387.64,513.00,7.37;6,22.50,397.11,482.96,7.37"><head>Fig. 8. </head><figDesc>Fig. 8. Exploration of style variations. Based on an overview of the large data collection at aggregation level 6, the domain expert drilled down the level to 17, and 39, respectively. Zooming and filtering was applied to focus on relevant human motion. Various cycles of human movement exist. For example, at the aggregation levels of 17 and 39, a 'cartwheel' motion beginning with an upright stand was identified at the bottom left. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true" coords="8,22.50,636.10,250.38,103.82"><figDesc coords="8,22.50,636.10,250.38,7.37;8,22.50,645.57,250.38,7.37">Table 1. HDM05 motion classes used for task completion tests in the field study. The applied collection contains 19,598 human poses in total.</figDesc><table coords="8,29.72,661.47,232.65,78.45">Motion Class 
# 
Motion Class 
# 
clapAboveHead 
17 
kickLFront 
29 
elbowToKneeLelbowStart 
27 
kickRFront 
30 
elbowToKneeRelbowStart 
27 
punchLFront 
30 
grabFloorR 
16 
punchRFront 
30 
grabHighR 
29 
skierLstart 
30 
grabMiddleR 
28 
squat 
52 
jumpingJack 
52 
walk2StepsLstart 
31 
walk2StepsRstart 
31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="9,31.95,53.99,512.10,61.39"><figDesc coords="9,31.95,53.99,512.10,7.37">Table 2. Complex analysis tasks performed by domain experts in the observational phase of the case study. Mapping to the system requirements.</figDesc><table coords="9,37.94,72.80,241.56,6.27">Analysis Goal 
Data Set 
Analysis Process and Analysis Result 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>The authors thank the 'Multimedia, Simulation and Virtual Reality Group' at the University of Bonn, Germany. We appreciated the collaborative work and the bilateral exchange in knowledge and expertise during the complete duration of this design study. We also thank the 14 non-experts that participated in the laboratory design interviews for their time and feedback. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,40.76,74.58,232.12,7.13;10,40.76,84.05,232.12,7.13;10,40.76,93.51,136.25,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">ASK-GraphView: a large scale graph visualization system</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Abello</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Van Ham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Krishnan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="page">2006</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,102.97,232.12,7.13;10,40.76,112.44,118.33,7.13"  xml:id="b1">
	<monogr>
		<title level="m" type="main">Visualization of Time-Oriented Data</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Aigner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Miksch</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Schumann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Tominski</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,121.90,232.12,7.13;10,40.76,131.37,232.12,7.13;10,40.76,140.83,69.29,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatial generalization and aggregation of massive movement data</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">V</forename>
				<surname>Andrienko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">L</forename>
				<surname>Andrienko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="219" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,150.30,232.12,7.13;10,40.76,159.76,232.12,7.13;10,40.76,169.23,227.30,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Tuggraph: Path-preserving hierarchies for browsing proximity and paths in graphs</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Archambault</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Munzner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Auber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Vis</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,178.69,232.12,7.13;10,40.76,188.16,232.12,7.13;10,40.76,197.62,229.18,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Level-of-detail visualization of clustered graph layouts</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Balzer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Deussen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Asia-Pacific Symposium on Visualization</title>
		<editor>S.-H. Hong and K.-L. Ma</editor>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="133" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,207.08,232.12,7.13;10,40.76,216.55,140.14,7.13"  xml:id="b5">
	<monogr>
		<title level="m" type="main">A survey of clustering data mining techniques. Grouping Multidimensional Data</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Berkhin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="25" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,226.01,232.12,7.13;10,40.76,235.48,232.12,7.13;10,40.76,244.94,232.12,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Content-based layouts for exploratory metadata search in scientific research data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bernard</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ruppert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Scherer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Schreck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL</title>
		<meeting><address><addrLine>New York, NY, USA, 2012</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,254.41,232.12,7.13;10,40.76,263.87,232.12,7.13;10,40.76,273.34,221.31,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">TimeSeries- Paths: projection-based explorative analysis of multivariate time series data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bernard</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Wilhelm</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Scherer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>May</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Schreck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of WSCG</title>
		<meeting><address><addrLine>Plzen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,282.80,232.12,7.13;10,40.76,292.26,232.12,7.13;10,40.76,301.73,232.12,7.13;10,40.76,311.19,118.36,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving visualization of large hierarchical clustering</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Bisson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Blanch</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 16th International Conference on Information Visualisation, IV &apos;12</title>
		<meeting>the 2012 16th International Conference on Information Visualisation, IV &apos;12<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="220" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,320.66,232.12,7.13;10,40.76,330.12,232.12,7.13;10,40.76,339.59,160.91,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualizing related metabolic pathways in two and a half dimensions</title>
		<author>
			<persName>
				<forename type="first">U</forename>
				<surname>Brandes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Dwyer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Schreiber</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Drawing</title>
		<editor>G. Liotta</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,349.05,232.12,7.13;10,40.76,358.52,232.12,7.13;10,40.76,367.98,81.25,7.13"  xml:id="b10">
	<monogr>
		<title level="m" type="main">DICON: interactive visual analysis of multidimensional clusters. Visualization and Computer Graphics</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gotz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Qu</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2581" to="2590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,377.45,232.12,7.13;10,40.76,386.91,232.12,7.13;10,40.76,396.37,222.10,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Querying and mining of time series data: experimental comparison of representations and distance measures</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ding</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Trajcevski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Scheuermann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1542" to="1552" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,405.84,232.12,7.13;10,40.76,415.30,232.12,7.13;10,40.76,424.77,89.22,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Cluster analysis and display of genome-wide expression patterns</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">B</forename>
				<surname>Eisen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">T</forename>
				<surname>Spellman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">O</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Botstein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Acad. of Sc</title>
		<imprint>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="9514863" to="14868" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,434.23,232.12,7.13;10,40.76,443.70,232.12,7.13;10,40.76,453.16,229.10,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Hierarchical aggregation for information visualization: Overview, techniques, and design guidelines</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Elmqvist</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-D</forename>
				<surname>Fekete</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="454" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,462.63,232.12,7.13;10,40.76,472.09,232.12,7.13;10,40.76,481.55,162.49,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time visual analytics for event data streams</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Fischer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Applied Computing, SAC &apos;12</title>
		<meeting><address><addrLine>New York, NY, USA, 2012</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="801" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,491.02,232.12,7.13;10,40.76,500.48,69.29,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">A review on time series data mining</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Fu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="181" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,509.95,232.12,7.13;10,40.76,519.41,205.87,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Visualization of multi-variate scientific data</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Fuchs</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hauser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1670" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,528.88,232.12,7.13;10,40.76,538.34,222.35,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of two-dimensional graph layout techniques for information visualisation</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Gibson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Vickers</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,547.81,232.12,7.13;10,40.76,557.27,232.12,7.13;10,40.76,566.74,212.05,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">A visualization system for space-time and multivariate patterns (vis-stamp)</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Guo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">M</forename>
				<surname>Maceachren</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Liao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1461" to="1474" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,576.20,232.12,7.13;10,40.76,585.66,232.12,7.13;10,40.76,595.13,145.22,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">iHAT: interactive hierarchical aggregation table for genetic association data</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Heinrich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Vehlow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Battke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Jger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nieselt</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,604.59,232.12,7.13;10,40.76,614.06,221.93,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Motion track: Visualizing variations of human motion data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Hu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Xia</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PacificVis</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,623.52,232.12,7.13;10,40.76,632.99,232.12,7.13;10,40.76,642.45,96.52,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated extraction and parameterization of motions in large data sets</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Kovar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gleicher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="568" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,651.92,232.12,7.13;10,40.76,661.38,69.29,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Kovar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gleicher</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Pighin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="473" to="482" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,670.84,232.12,7.13;10,40.76,680.31,232.12,7.13;10,40.76,689.77,232.12,7.13;10,40.76,699.24,73.49,7.13"  xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast local and global similarity searches in large motion capture databases</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Tautges</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Weber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zinke</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG- GRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,708.70,232.12,7.13;10,40.76,718.17,81.25,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering of time series data-a survey</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">T</forename>
				<surname>Liao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1857" to="1874" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,727.63,232.12,7.13;10,40.76,737.10,232.12,7.13;10,303.38,54.06,220.94,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">A symbolic representation of time series, with implications for streaming algorithms</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lonardi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Chiu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD workshop on DMKD</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,63.52,232.12,7.13;10,303.38,72.99,136.82,7.13"  xml:id="b26">
	<monogr>
		<title level="m" type="main">Information Retrieval for Music and Motion</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Müller</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, Inc., Secaucus, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,82.45,232.12,7.13;10,303.38,91.92,232.12,7.13;10,303.38,101.38,232.12,7.13;10,303.38,110.85,172.82,7.13"  xml:id="b27">
	<monogr>
		<title level="m" type="main">Documentation: mocap database HDM05</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Müller</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Röder</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Clausen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Eberhardt</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Weber</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Data. available at www.mpi-inf.mpg.de/resources/HDM05</note>
</biblStruct>

<biblStruct coords="10,303.38,120.31,232.12,7.13;10,303.38,129.78,232.12,7.13;10,303.38,139.24,213.19,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Content-based organization and visualization of music archives</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Pampalk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Rauber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Merkl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth ACM International Conference on Multimedia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="570" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,148.70,232.12,7.13;10,303.38,158.17,232.12,7.13;10,303.38,167.63,94.13,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">State of the art in example-based motion synthesis for virtual characters in interactive applications</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Pejsa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Pandzic</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="202" to="226" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,177.10,232.12,7.13;10,303.38,186.56,232.12,7.13;10,303.38,196.03,232.12,7.13"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion map: imagebased retrieval and segmentation of motion data</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Sakamoto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kuriyama</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kaneko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG- GRAPH/Eurographics symposium on Computer animation</title>
		<imprint>
			<biblScope unit="page">259</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,319.32,205.49,190.77,7.13"  xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">Switzerland</forename>
				<surname>Aire-La-Ville</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Association</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,232.12,7.13;10,303.38,233.88,232.12,7.13;10,303.38,243.35,94.09,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Composite density maps for multivariate trajectories</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Scheepens</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Willems</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Van De Wetering</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Andrienko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Andrienko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2518" to="2527" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,252.81,232.12,7.13;10,303.38,262.28,232.12,7.13;10,303.38,271.82,232.12,6.86;10,303.38,281.21,232.12,7.13;10,303.38,290.67,39.75,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Retrieval and exploratory search in multivariate research data repositories using regressional features</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Scherer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bernard</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Schreck</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th annual international ACM/IEEE joint conference on Digital libraries</title>
		<meeting>the 11th annual international ACM/IEEE joint conference on Digital libraries<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,300.14,232.12,7.13;10,303.38,309.60,232.12,7.13;10,303.38,319.07,93.51,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual cluster analysis of trajectory data with interactive Kohonen maps</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Schreck</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Bernard</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tekušová</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="14" to="29" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,328.53,232.12,7.13;10,303.38,337.99,107.03,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Treevis.net: A tree visualization reference</title>
		<author>
			<persName>
				<forename type="first">H.-J</forename>
				<surname>Schulz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="11" to="15" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,347.46,232.12,7.13;10,303.38,356.92,122.86,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Interactively exploring hierarchical clustering results</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Seo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Shneiderman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,366.39,232.12,7.13;10,303.38,375.85,232.12,7.13;10,303.38,385.32,232.12,7.13;10,303.38,394.78,72.16,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Musicgalaxy: A multi-focus zoomable interface for multi-facet exploration of music collections</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Stober</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Nürnberger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exploring Music Contents</title>
		<meeting><address><addrLine>Berlin / Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="273" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,404.25,232.12,7.13;10,303.38,413.71,232.12,7.13;10,303.38,423.17,117.77,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Simplified representation of vector fields</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Telea</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Visualization</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,432.64,232.12,7.13;10,303.38,442.10,232.12,7.13;10,303.38,451.57,33.87,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Interactive visual analysis of temporal cluster structures</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Turkay</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Parulek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Reuter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Hauser</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,461.03,232.12,7.13;10,303.38,470.50,232.12,7.13;10,303.38,479.96,232.12,7.13;10,303.38,489.43,94.09,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual analysis of large graphs: State-of-the-art and future research challenges</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Von Landesberger</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kuijper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Schreck</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-D</forename>
				<surname>Fekete</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">W</forename>
				<surname>Fellner</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="301719" to="1749" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,498.89,232.12,7.13;10,303.38,508.36,223.46,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">A taxonomy of glyph placement strategies for multidimensional data visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">O</forename>
				<surname>Ward</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="194" to="210" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,517.82,232.12,7.13;10,303.38,527.28,219.52,7.13"  xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual exploration of time-series data with shape space projections</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">O</forename>
				<surname>Ward</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Guo</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="701" to="710" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,536.75,232.12,7.13;10,303.38,546.21,232.12,7.13;10,303.38,555.68,122.51,7.13"  xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploratory search: Beyond the queryresponse paradigm</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">W</forename>
				<surname>White</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">A</forename>
				<surname>Roth</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Information Concepts, Retrieval , and Services</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,565.14,232.12,7.13;10,303.38,574.61,232.12,7.13;10,303.38,584.07,232.12,7.13;10,303.38,593.54,232.12,7.13;10,303.38,603.00,98.44,7.13"  xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual exploration and discovery of atypical behavior in financial time series data using twodimensional colormaps</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ziegler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Nietzschmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference Information Visualization</title>
		<meeting>the 11th International Conference Information Visualization<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
