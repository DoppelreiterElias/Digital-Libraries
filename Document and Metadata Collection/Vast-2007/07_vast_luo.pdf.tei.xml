<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Hangzai</forename>
								<surname>Luo</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Software Engineering Institute East China Normal University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jianping</forename>
								<surname>Fan</surname>
							</persName>
							<affiliation>
								<orgName type="institution">UNC-Charlotte Charlotte</orgName>
								<address>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Jing</forename>
								<surname>Yang</surname>
							</persName>
							<affiliation>
								<orgName type="institution">UNC-Charlotte Charlotte</orgName>
								<address>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">William</forename>
								<surname>Ribarsky</surname>
							</persName>
							<affiliation>
								<orgName type="institution">UNC-Charlotte Charlotte</orgName>
								<address>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<surname>Shin &apos;ichi Satoh</surname>
							</persName>
							<affiliation>
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Analyzing Large-Scale News Video Databases to Support Knowledge Visualization and Intuitive Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Video Classification, Knowledge Discovery,</term>
					<term>Knowledge Visualization</term>
					<term>Index Terms:</term>
					<term>I26 [Artificial Intelligence]:</term>
					<term>Learning—</term>
					<term>Concept learning; I36 [Computer Graphics]: Methodology and</term>
					<term>Techniques—Interaction Techniques</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge visualization. To relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports, a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently. Our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization techniques on semantic video retrieval systems. Our techniques on intelligent news video analysis and knowledge discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections. In addition, news video visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge discovery.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Broadcast video news is a very important information source to most people. To satisfy the extremely diverse demands of the public , it covers large amount of events everyday. As a result, it provides a picture of what is happening now at the local, national, and international levels. Broadcast news provides not only reports on events but insight into the social and political framework from which the broadcast originates. For these reasons, broadcast news is watched and closely analyzed by individuals, government organizations , and companies. However, with the rapidly increasing number of broadcasts, especially in developing countries, the fraction that can be successfully watched in detail or even monitored by any individual or entity is growing rapidly smaller. Therefore, there is an urgent demand for achieving intuitive and effective exploration of large-scale video news databases. However, automatic video news analysis still suffers from the following challenging problems. The first problem is how to extract the underlying semantics from the video clips. Before the system can provide an automatic video news exploration service, it must understand the underlying semantics of the input video clips. However, there exists a big semantic gap <ref type="bibr" coords="1,100.35,635.67,14.19,8.02" target="#b9">[10,</ref><ref type="bibr" coords="1,117.74,635.67,7.47,8.02" target="#b0"> 1] </ref> between the low-level visual features and the high-level semantic video concepts. Existing video exploration systems can only support the services based on low-level visual fea- tures <ref type="bibr" coords="1,338.21,205.50,9.52,8.02" target="#b2">[3]</ref> . Typical users, however, can only express their information needs via high-level semantics and concepts <ref type="bibr" coords="1,504.81,215.47,9.52,8.02" target="#b1">[2]</ref>. Semantic video classification approaches can extract limited video semantics from video clips, but they can hardly satisfy the requirements of semantic video news database exploration applications. With a limited video semantics, how to provide intuitive applications for video news database investigations is still an open problem. The second problem is how to extract the most useful knowledge from the large-scale video news database and display such knowledge to the users. Because the total amount of knowledge for a large-scale video news database is very large (e.g., thousands of hours of video), most of the information is irrelevant to the point of interest. If all information is delivered to the analysts or audiences , they may easily get lost and miss the important information. For example, " Bush is the president of the USA " is a piece of wellknown information. Disclosing this information to an analyst does not make sense, and most general audiences may not be interested in such kind of information. Abnormal information is more useful and interesting for the users. Thus, there is an interest gap <ref type="bibr" coords="1,529.54,384.91,14.94,8.02" target="#b13">[14] </ref>between the underlying information collection and the user's interest. The third problem is how to launch personalized knowledge retrieval upon receiving input from the users. Before the users submit any input to express their preferences and information needs, the system can only display a general overview of all knowledge. The general overview of all knowledge discloses the global overview of the database but does not disclose enough details to fit the user preferences or the user's current information needs. As a result, the knowledge structure must be reorganized after the system receives user input, so that more details related to the user input can be disclosed . Existing systems adopt retrieval techniques to extract a few most relevant items from the database. However, the traditional retrieval techniques can only provides a set of possibly relevant items. How to directly disclose the personalized knowledge structure via these relevant items is still an open problem. Researchers have proposed different approaches to resolve these problems. Semantic video classification is one of the potential solutions to bridge the semantic gap. To achieve video understanding via semantic classification, the semantic classification algorithms first extract features from the video clips and then classify the video clips to semantic concepts according to their feature vectors via machine-learning algorithms. However, they are optimized toward keyword-based search applications. As a result, they may not be optimal for video news exploration and analysis applications. Therefore , how to extract suitable semantics for video news exploration and analysis applications is still an open problem. Visualization approaches have been proposed to help the users explore in information spaces and find interesting parts intuitively. InSpire <ref type="bibr" coords="1,346.41,674.07,14.94,8.02" target="#b15">[16] </ref>transforms the text document collection of interest to a spatial representation for visualization and analysis. For example, statistical information of news reports <ref type="bibr" coords="1,458.00,694.00,10.45,8.02" target="#b8">[9] </ref>could be put on a world map to inform the audience of the " hotness " of regions and the relations among the regions. TimeMine <ref type="bibr" coords="1,453.72,713.92,14.94,8.02" target="#b12">[13] </ref>is able to detect the most S e m a n tic V id e o A n a l y s is In te re s tin g n e s s W e ig h tin g S e m a n tic In te rp re ta tio n V is u a liz a tio n K n o w le d g e In te rp re ta tio n F e e d b a c k <ref type="figure" coords="2,101.70,103.84,28.13,7.37">Figure 1</ref>: The workflow of the framework. important reports and organize them through a timeline with statistical models of word usage. Another system, called newsmap <ref type="bibr" coords="2,276.87,131.35,13.74,8.02" target="#b14">[15]</ref>, organizes news topics from Google news on a rectangle, where each news story covers a visualization space that is proportional to the number of related news pages reported by Google. News titles are drawn in the corresponding visualization space allocated to them. ThemeRiver <ref type="bibr" coords="2,133.71,181.16,10.45,8.02" target="#b3">[4] </ref>and ThemeView <ref type="bibr" coords="2,207.89,181.16,10.45,8.02" target="#b4">[5] </ref>can visualize a large collection of documents with keywords or themes over time or as aggregations of related themes. ThemeRiver and ThemeView can represent intuitively the distribution structure of themes and keywords of the database. However, all of these visualization systems cannot directly provide knowledge to the users. Rather, they disclose relevant information that the users must investigate to form their own conclusions. Although the tools provided disclose different distribution structures of the database, most of the distribution structures are uninteresting for many users. Only the unexpected events, such as the announcement of Osama bin Laden, can catch the eyes of these users. In addition, all existing algorithms address the above problems separately. Therefore, they may optimize the solution for different purposes. On the one hand, the semantic video classification algorithms are generally optimized for keyword-based video retrieval applications <ref type="bibr" coords="2,99.63,340.91,9.71,8.02" target="#b0">[1,</ref><ref type="bibr" coords="2,111.65,340.91,6.47,8.02" target="#b6"> 7]</ref> . As a result, the semantic video concepts implemented may be suitable for search but not suitable for visualization. On the other hand, the visualization approaches focus on providing new techniques for information representation <ref type="bibr" coords="2,221.99,370.80,14.94,8.02" target="#b13">[14] </ref>and assume the information is somehow available for immediate use. However, the most useful information can only be extracted by using state-of-theart semantic analysis algorithms. By addressing the two problems together, these mismatches can be avoided and the performance of the system can be improved significantly. To resolve the above problems, we offer here a knowledge visualization framework that can integrate achievements on semantic video analysis, information retrieval, knowledge discovery, and knowledge visualization. The framework is introduced in Section 2. Sections 3 through 5 introduce algorithms to implement different components of the framework. Finally we conclude in Section 7. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="107"></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">KNOWLEDGE VISUALIZATION FRAMEWORK</head><p>Based on the above observations, one finds that it is very difficult, if not impossible, to resolve the problems addressed in the above section independently in a single research area. A solution can only be achieved by integrating achievements on semantic video analysis, information retrieval, knowledge discovery, and knowledge visualization . In addition, all components must be optimized toward a single target to achieve intuitive and intelligent exploration of largescale video databases. Based on this understanding, the workflow of our framework is shown in <ref type="figure" coords="2,160.70,594.02,28.79,8.02">Figure 1</ref> . First, the semantic interpretation is extracted from raw video clips via semantic video analysis techniques. Second, the knowledge interpretation is extracted by weighting the semantic interpretation according to an interestingness measurement. Third, visualization techniques are adopted to represent the knowledge intuitively. Finally, the semantic interpretation and the knowledge interpretation can be improved through the user input received via the visualization interface. To establish the best visualization design, the knowledge interpretation must be carefully selected to satisfy as many user needs as possible. Therefore, the user needs must be carefully analyzed. Our system is targeted at two major types of users: analysts and general audiences. Their needs are discussed below. First, the purpose of both analysts and general audiences in using the system is to gain knowledge of the database. To disclose as much knowledge of the database as possible, the knowledge interpretation must be in a format that is intuitive and easy to visualize. Second, the users may not be interested in most knowledge in the database. Large-scale news video databases carry a large amount of knowledge, but much of it is common sense and thus not interesting for most users. As a result, the knowledge interpretation must be able to suppress general, uninteresting knowledge and emphasize abnormal, interesting knowledge. Third, the user's interest viewpoint may change during the exploration . When a user finds an interesting event, it is preferable to disclose the semantic structure of the event and other relevant events to the user, so that the most useful knowledge can be explored easily . Then, the knowledge interpretation must be in a format that is easy to modify according to the user's changing viewpoints. In addition , the knowledge interpretation must be able to automatically cluster relevant events together. Based on the above observations, we use a weighted news topic relation network as the knowledge interpretation. The network uses news topics (i.e., keywords and keyframes) as nodes and their relations as edges, and the edges are weighted according to their interestingness for the users. If we use D to represent the database of interest and K D to represent the knowledge interpretation of D (i.e., the weighted semantic network), K D can be represent as: </p><formula>K D = {(k i = (s a , s b ) , w U (k i )) | 1 ≤ i ≤ N} (1) </formula><p>where k i is a relation between a pair of news topics s a and s b , U is the user who is using the system, and w U (k i ) is the interestingness weight of k i based on U's preference. An example of the network is given in <ref type="figure" coords="2,357.20,372.32,29.02,8.02" target="#fig_0">Figure 2</ref>. s a and s b are defined as related when they occur in a closed caption or automatic speed recognition (ASR) script sentence simultaneously . By collecting all these relations together, the semantics of the whole database can be represented. Therefore, we use the pairs of news topics, k i , as the knowledge items to interpret the knowledge of video news databases. However, not all of these relations are interesting for the users. For example, the relation between " Bush " and " President " is not interesting because it is well known. On the contrary, the relation between " Iraq War " and " Gas Price " may be interesting for many users. To resolve this problem, we also compute an interestingness weight w U (k i ) for each knowledge item k i . Our knowledge interpretation extraction algorithm will extract the interestingness weights for the knowledge items. The algorithm will be introduced in the next section. The weighted news topic relation network is suitable for our system because of the following reasons. First, networks can be intuitively visualized, so that most information of the news topic relation network can be delivered to the users. Second, uninteresting knowledge can be suppressed by the interestingness weights used in the semantic network. Third, the interestingness weights can be easily modified to adapt to the user's changing viewpoint during the exploration. Finally, the analysis of closed caption sentences together with the news topic pairs provide strong relations and rich knowledge content, as shown in the networks. It is therefore quite worthwhile to use the weighted news topic relation network as the format of the knowledge interpretation and to optimize all components of our knowledge visualization framework toward a single target. The semantic analysis algorithm is optimized to extract news topics and their relations for knowledge interpretation and knowledge visualization. The interestingness weighting algorithm is optimized to weight the news topics and their relations extracted from the semantic analysis algorithm so that common, uninteresting knowledge can be suppressed. The visualization interface is optimized to disclose as much about the  news topic relation network as can be clearly shown. All these optimizations result in maximizing the amount of relevant knowledge delivered to the users during the exploration. To implement this optimized system, algorithms from different research areas must be integrated. The following sections of this paper introduce these optimizations.  where s I is the clicked item, ϖ (k i , s I ) is a boosting factor to emphasize items most relevant to s I , c ≥ 1 is the boosting constant, and r (s * , s I ) ∈ <ref type="bibr" coords="4,110.92,368.19,9.46,7.96">[0,</ref><ref type="bibr" coords="4,121.36,368.33,6.97,8.02" target="#b0"> 1] </ref> is the relevance between s * and s I . By applying Eq. (2) to the global network, irrelevant knowledge items with r (s * , s I ) = 0 stay unchanged and relevant knowledge items with r (s * , s I ) &gt; 0 have interestingness weights increased according to their relevance to the user input. As a result, more relevant knowledge items are selected for visualization on the new network. Constant c balances the local details and the global context. Larger c enables more local details to be included in the new network. Smaller c preserves more global context in the new network. To computé K D (s I ), r (s * , s I ) must be computed. Because the news topic relation network´Knetwork´ network´K D represents the relevance quantities among news topics, r (s m , s n ) can be computed by exploring´K exploring´ exploring´K D . Between a pair of news topics s m and s n , there may be several </p><formula>paths p x (s m , s n ) = (s m , ..., s l , ..., s n ) on´Kon´ on´K D . The interestingness of p x (s m , s n ) is defined as: w U (p x (s m , s n )) = min w U k j = (s a , s b ) (3) </formula><p>where k j is a segment of p x (s m , s n ). The shortest path is defined as: </p><formula>p min (s m , s n ) = arg max x {w U (p x (s m , s n ))} (4) </formula><p>The shortest path p min (s m , s n ) represents the most interesting route connecting s m and s n . Therefore, it is a good measure of the relevance between s m and s n : </p><formula>r (s m , s n ) = w U (p min (s m , s n )) (5) </formula><p>By combining Eq. (5) into Eq. (2), a new semantic network´K network´ network´K D (s I ) can be generated, which is relevant to the user input s I . In addition, relevant nodes are automatically laid out close to s I . Relevant events can then be easily checked. Furthermore, the global semantic context is still preserved, so that the user can quickly switch to new point of interest if she changes her mind. An example is given in <ref type="figure" coords="4,85.02,713.92,32.38,8.02" target="#fig_3">Figure 5.</ref>(a) Global semantic network (b) Focused semantic network for "schoolhouse" By integrating the focused network, the global network can be slimmed down to keep only the most important news topic relation structure. As a result, the users can learn more high-level semantics and are free from messy fine details. The slimmed global network for the same database as <ref type="figure" coords="4,429.47,479.94,30.39,8.02" target="#fig_1">Figure 3</ref>is given in <ref type="figure" coords="4,502.39,479.94,29.27,8.02" target="#fig_4">Figure 6</ref>. From <ref type="figure" coords="4,317.96,489.90,29.31,8.02" target="#fig_4">Figure 6</ref> , the users can derive two major events immediately without further exploration: the Foley's scandal and the Amish school shooting. <ref type="figure" coords="4,353.87,509.83,30.14,8.02" target="#fig_1">Figure 3</ref>does not have this nice property. Although the modified news topic relation network is able to disclose more details related to the user input, this is not enough for reasoning. To have best support for reasoning, the relevant original news reports must be retrieved and played for the users. Our system can retrieve from the news video database by using the user input s I as the query. The most relevant news stories are selected and returned to the users. The retrieved stories can be organized by timeline so that the users can easily learn the development procedure of the whole event, at the bottom of <ref type="figure" coords="4,438.40,599.94,29.02,8.02" target="#fig_5">Figure 7</ref>(a). In addition, the most relevant web news is also retrieved, as shown in 7(b). This feature is very important for audiences who want to know more details and related discussions of the event. This combined process is a good reasoning technique for news video exploration and analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KNOWLEDGE INTERPRETATION EXTRACTION</head><p>To implement the above visual interface, knowledge interpretations must be extracted from the database. As discussed in the previous section, an appropriate knowledge interpretation is composed of a set of knowledge items (i.e., keywords, keyframes, and their relations ) and their interestingness, as in Eq. (1). We now describe how to compute the interestingness of knowledge items: w U (k i ). The interestingness weight enables our knowledge interpretation to emphasize interesting knowledge and suppress uninteresting knowledge. However, it is a subjective quantity and cannot be quantified accurately without information from the user. Nevertheless, for news retrieval applications, the system generally needs to make recommendations and display something before users have any idea of the database. And for many applications involving large-scale video databases, disclosing the thoughts and and opinions of the public may be more important than satisfying the interestingness of the user. For example, a political consultant may want to know the real feeling of the public for a new policy. In this scenario, the personal preference of the consultant apparently should not be counted. Based on these observations, the system must provide a general interestingness measurement that is reasonable for most users. To resolve this problem, we propose to use the provider behavior model to quantify the interestingness. Any news provider should have an interestingness measure for each news report. This measure is used to guide the production of news reports, taking into account such factors as story selection, sequence, length of each story, etc. If we can quantify these factors, the interestingness of news stories given by the provider can be quantified. One potential problem of the provider behavior model is that the interestingness measure from providers may be biased. The solution is to integrate multiple channels of large-scale data. The multi-channel database is able to smooth the individual biases and achieve unbiased or near unbiased solutions. Google has used the PageRank <ref type="bibr" coords="5,241.68,621.30,14.94,8.02" target="#b10">[11] </ref>technique to quantify the interestingness of web pages according to their links. The PageRank technique is a successful provider behavior model. Because of the great success of Google's search engine, we believe a provider behavior model will also work in large-scale news video database exploration applications. To quantify the interestingness with the provider behavior model, we need to quantify the factors that news producers use to perform news editing. There are two types of factors can be quantified. The first type is the frequency δ (k i ) of knowledge item k i . More important news stories will certainly have a higher chance to be selected to broadcast by news producers. Thus, knowledge items repeated again and again on channel after channel may be more important than those that appear only one or two times. However, it may not be true all the time. For example, " George Bush ⇔President " is one of the most frequent knowledge items in recent news reports. But users are seldom interested in this knowledge item because it's already well known. This implies that the interestingness w U (k i ) is inversely proportional to the user's prior knowledge, µ U (k i ). Based on the above observations, w U (k i ) can be modeled by integrating the two quantities: </p><formula>w U (k i ) ∝ δ (k i ) w U (k i ) ∝ 1 µ U (k i ) ⇒ w U (k i ) = γ δ (k i ) µ U (k i ) (6) </formula><p> where γ is the normalization constant. Because the visualization algorithm uses only the relative ratios among w U (k i ), γ can be simply set to 1 or selected to optimize other targets. In Eq. (6), δ (k i ) can be computed by statistical analysis on K D : </p><formula>δ (k i ) = P K D (k i ) (7) </formula><p>where P K D (k i ) is the probability of k i in K D . However, µ U (k i ) is subjective and thus more complex to compute. In addition, U may have different prior knowledge of k i at different times. Therefore, µ U (k i ,t) must be used, where t is the time when U uses the system. There are two factors that may affect µ U (k i ,t): the learning factor and the forgetting factor. After k i is reported by a provider m l at time t l , U may not learn it immediately. As time passes, the probability that U knows k i increases. This means we can adopt a learning curve to compute µ U (k i ,t). In addition, U may forget k i if it is not repeated for a long time. By integrating both factors together, we can model the effect, ρ U (t, l), of one occurrence of k i to U at time t (i.e. the time when the user employs the system) as: </p><formula>ρ U (t, l) = σ U (m l ) ϕ U (t − t l ) φ U (t − t l ) = σ U (m l ) g U (t − t l ) (8) </formula><p>where ϕ U (t − t l ) is the learning curve, φ U (t − t l ) is the forgetting curve, l indicates the broadcast of k i by provider m l , σ U is the efficiency of m l , t l is the time of l, and g U = ϕ U × φ U is the combined effect of learning and forgetting curves. Then, the prior knowledge of U at time t, µ U (k i ,t), is the sum of all occurrence of k i : </p><formula>µ U (k i ,t) = ∑ σ U (m l ) g U (t − t l ) </formula><formula>(9) </formula><p>Eq. (9) can be represented as a convolution. If we define f U (t) = σ U (m l ) when t = t l and f U (t) = 0 otherwise, µ U (k i ) is the convolution of f U and g U : </p><formula>µ U (k i ,t) = ˆ f U (t l ) g U (t − t l ) dt l = f U @BULLET g U (t) </formula><formula>(10) </formula><p>For a specific exploration task, U may only focus on a relatively short period, such as one day, one week or one month. Therefore , K D covers video news reports only in the period of interest. However, U may learn a knowledge item at any time. For example , " George Bush ⇔ President " may have been learned for several years. As a result, µ U (k i ,t) must be computed by using a database covering a much longer period than D. We name this database as´D as´ as´D. Consequently, the knowledge interpretation of´Dof´ of´D is K ´ D . We use </p><formula>K ´ D to compute µ U (k i </formula><p>,t) in our system. Eq. (10) gives a good model of the user's prior knowledge. Even though f U and g U are still subjective, they may be approximated. By adopting different approximations of f U and g U , both general and personalized knowledge extraction can be implemented. For large-scale video database exploration and investigation applications , general knowledge that reflects the thinking of a general user is more interesting than the personalized knowledge extracted for a particular investigator. To achieve this purpose, f U and g U must approximate the property of a general not a specific user. In this scenario, g U (t) = 1 is a reasonable approximation. There are two reasons to support this point. First, the time interval from the happening of an event to the perception of the public is very short due to the responsiveness of the modern news industry. Because the time span of the database is much longer than this perception interval , ϕ U can be treated as a step function. Second, because we use simple items for knowledge interpretation, users can remember them easily for a long time. In addition, the " refreshing interval " of these items may be much shorter than the time that users can remember them. As a result, φ U = 1 is a reasonable assumption. Using this reasoning and the property of convolution, g U (t) = 1 is a suitable approximation for general knowledge extraction. As the audiences have the freedom of selecting programs for watching, the rough average effect of the public is that all news providers may have about the same influence. This means σ U = 1 is also a reasonable assumption. Certainly, particular news providers may influence different numbers of users. Consequently, more sophisticated models can be achieved by using market share, revenue or similar factors to compute σ U . Based on these observations, f U @BULLET g U (t) is equivalent to the weighted frequency of k i in K ´ D . Weights for occurrences of k i are σ U . σ U = 1 is a special case when all weights are equal. In our experiments we adopt </p><formula>σ U = 1. As a result, µ U (k i ,t) = P K ´ D (k i ). </formula><p>Based on this understanding we can compute the interestingness as: </p><formula>w U (k i ) = γ P K D (k i ) P K ´ D (k i ) (11) </formula><p> To simplify the post process and fusing with other factors, γ is selected to normalize w U (k i ) to the range of <ref type="bibr" coords="6,207.68,682.87,9.46,7.96">[0,</ref><ref type="bibr" coords="6,218.12,683.01,6.31,8.02" target="#b0"> 1]</ref>: </p><formula>1 γ = max k i ∈K D P K D (k i ) P K ´ D (k i ) (12) </formula><p>When personalized knowledge extraction is needed, f U and g U can be quantified by using usage history and user preference. For example, if U frequently watches CNN but seldom watches FOX News, we can assign higher weights to knowledge items from CNN. We can also assign a steeper learning curve to users watching news programs more frequently. Although personalized knowledge extraction is very interesting, how to implement it by using usage history and user preference information is complex. Therefore, we do not cover it in this paper and leave it for the future. Video production rules can also imply the importance assigned by the news producers. To enable more efficient visualization of large-scale news video databases, important visual features should be considered. Video production rules generally do not have the " prior knowledge " problem. However these video production rules are difficult to extract because they are at high-level semantics. In addition, the keyframes, keywords and their relations are all semantic information. To have a complete large-scale news video database exploration system we need to extract these semantic interpretations from the database. The next section introduces the algorithm for semantic interpretation extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SEMANTIC INTERPRETATION EXTRACTION</head><p> Not all news topics (i.e., keyframes and keywords) are equally interesting for the users. For example, " New York " is a keyword frequently mentioned in many news reports. Thus, many users may not be interested in it. As a result, the news topics also need to be weighted as we do for the knowledge items introduced in above section . In addition, video production rules related to news topics and knowledge items also need to be quantified so that better semantic interpretation and knowledge interpretation can be extracted. However, semantic video analysis and understanding are still very challenging for current computer vision technologies. The problem is caused by the semantic gap between the semantics of video clips from the human point of views and the low-level features that can be extracted by computers <ref type="bibr" coords="6,469.26,403.97,13.74,8.02" target="#b11">[12]</ref> . Nevertheless, supporting semantic video analysis plays an important role in enabling more efficient exploration of large-scale news videos. Without extracting the semantics from large-scale news video collections, it is very difficult to visualize them effectively. Based on this observation , we have developed novel algorithms to extract the multimodal news topics (i.e., from video, audio, text) and video production rules automatically. Weights are assigned automatically with a statistical video analysis algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Video Analysis</head><p> The basic unit for news video interpretation is the video shot. Unlike the keywords of text documents, a video shot may carry abundant information (i.e., an image is more than a thousand words). This specific property of the video shot makes it difficult to effectively achieve statistical analysis of its visual properties and assign importance weights for news video visualization. To overcome this, we have developed a novel framework for statistical video analysis. There are three types of semantic units that are critical to determine the importance weights for the corresponding video shots. The first one is the statistical properties of the shots. The second one is the special video objects in the shots. The last one is the semantic concepts that are associated with the shots. Because these three types of semantic units have different properties, different algorithms are needed to extract such multi-modal news topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Statistical Property Analysis of Physical Video Shots</head><p> The physical video shot is the basic unit for news video interpretation . Therefore, it can be used as a semantic item. However, unlike the keywords in text documents, the repetition of physical video shots cannot be detected automatically by using simple comparison between the shots. New techniques are needed for detecting the repeat of video shots in news videos <ref type="bibr" coords="7,186.01,253.52,9.52,8.02" target="#b7">[8]</ref>. News producers may repeat a certain shot in several ways. By detecting the repeat pattern of shots, we can infer the interestingness weights assigned by news producers. Consequently, we need to discriminate these patterns and assign appropriate weights to them. Through experiments, we found that most repeated shots can be weighted by an intra-program repetition weights and an interprogram repetition weights: </p><formula>w intra (i) = e − r intra (i)−2 2 2 2 w inter (i) = e − r inter (i)−5 8 2 2 (13) </formula><p>where r intra (i) is the intra-program repeating number of shot i, and consequently r inter (i) is the inter-program repeating number. More details can be found in <ref type="bibr" coords="7,137.42,411.12,9.52,8.02" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Video Objects Detection</head><p>For news videos, text areas and human faces may provide important clues about news stories of interest. Text lines and human faces in news videos can be detected automatically by computer vision techniques <ref type="bibr" coords="7,95.19,471.29,9.52,8.02" target="#b7">[8]</ref>. Then the detected objects can be used to quantify the importance of the shot: </p><formula>w textArea (i) = 1 1+e − max{αtext (i)−νtext ,0} λtext w f aceArea (i) = 1 1+e − max{α f ace (i)−ν f ace ,0} λ f ace </formula><formula>(14) </formula><p> where α is the ratio that the object is in the frame, ν and λ are parameters determined by experiments. Video objects detection examples are given in <ref type="figure" coords="7,125.86,580.25,29.02,8.02" target="#fig_7">Figure 8</ref>. More details can be found in <ref type="bibr" coords="7,266.10,580.25,9.52,8.02" target="#b7">[8]</ref>. By performing face clustering, face objects can be clustered to several groups and the human objects can be identified. The human object is similar to the knowledge items: too frequent items may not be interesting, such as the anchor person. Consequently, the same weighting algorithm introduced in Section 4 is adopted to compute the weight. The importance weight for human face of shot i is computed by: </p><formula>w f ace (i) = max x∈FACE(i) {w U (x)} FACE (i) = / 0 0.5 FACE (i) = / 0 (15) </formula><p>where FACE (i) is the set of face objects of shot i, and w U (x) is the weight of x computed by using Eq. (11). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Semantic Video Classification</head><p> The semantic concepts of video shots can provide valuable information to enable more efficient and effective visualization and retrieval of large-scale news video collections. Semantic video classification is one method that helps detect the semantic concepts for the video shots. We adopt a principal video shot-based semantic video classification algorithm <ref type="bibr" coords="7,390.19,185.79,10.45,8.02" target="#b1">[2] </ref>in our system. Two types of information about semantic concepts can be used for weight assignment. First, the users may have different preferences for different semantic concepts. Therefore, a prior weight can be assigned to each semantic concept according to the user preference . We adopt a scheme that approximates the preference of the public, as assigned in <ref type="figure" coords="7,397.44,245.56,25.43,8.02" target="#tab_1">Table 1</ref>. Where C (i) is the semantic concept of i, and w c (C (i)) is the weight assignment. Second, semantic concepts are similar to the knowledge items thus can be weighted by the algorithm of Section 4. Finally, the weight of semantic concept is determined by: </p><formula>w concept (i) = w c (C (i)) × w U (C (i)) (16) </formula><p>where w c (C (i)) is looked up from <ref type="figure" coords="7,449.48,321.34,26.27,8.02" target="#tab_1">Table 1</ref>, and w U (C (i)) is the weight of C (i) computed by using Eq. (11). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Multi-Modal Data Fusion</head><p> To enable more efficient visualization of large-scale news video collections , an overall weight is assigned with each video shot based on the weights described above. Our purpose of weighting is to detect the existence of some visual properties and emphasize those shots with interesting visual properties. The existence of one visual property may be indicated by different visual patterns. For example , the repeat property may be represented by w intra or w inter . To ensure we detect the existence of interesting visual properties and capture the patterns we are looking for, we first use max operation to fuse weights for the same visual property: </p><formula>w repeat (i) = max {w intra (i) , w inter (i)} w ob ject (i) = max w f aceArea (i) , w textArea (i) w semantics (i) = max w f ace (i) , w concept (i) </formula><formula>(17) </formula><p>Where w repeat measures the visual property of physical video shot repetition, w ob ject measures the visual property of salient objects, and w semantics measures the visual property of visual concepts. Then the overall visual importance weight for a given video shot is determined by the geometric average of the three weights: </p><formula>w video (i) = 3 w repeat (i) × w ob ject (i) × w semantics (i) (18) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Audio and Text Keywords Extraction</head><p>The keywords can be extracted from closed caption and ASR scripts. Advanced natural language processing techniques, such as named entity detection, coreference resolving, and part-of-speech (POS) parsing are used in our system to extract appropriate keywords for shots. More details can be found in <ref type="bibr" coords="7,481.70,644.54,9.52,8.02" target="#b7">[8]</ref>. Finally each shot is associated a set of keywords. The keyword weight of a shot is computed by: </p><formula>w keyword (i) = max x {w U (x) </formula><p>|x is a keyword of i} </p><formula>(19) </formula><p>In Eq. (19) we use the proposed provider behavior model to weight each keyword. </p><p>With the keyword weight and the visual weight computed above, the overall weight for a given video shot is determined by averaging w video and w keyword : </p><formula>w (i) = γ × w video (i) + (1 − γ) × w keyword (i) (20) </formula><p>In our current experiments, we set γ = 0.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>To evaluate the efficiency of our system, we compare our system with the state of the art news search engine, Google News. We ask users to evaluate the difficulty of answering several news related questions by using our system and Google News. Total 12 users participated the experiments. 10 of them are undergraduate students without any related background, and 2 of them are security experts. Half of the users evaluate our system first. Another half evaluate Google News first. Before a user evaluate our system, the user watches a two-minute introduction video. For each task, the users give out only the difficulty level to complete the task. The difficulty level is defined as a number between 1 and 10, where 1 is the lowest level and 10 is the highest level. The database used in the evaluation contains three channels (CNN, FOX, and MSNBC) of video news reports in the past month (which is <ref type="bibr" coords="8,233.09,274.41,48.45,8.02">October, 2006</ref>). The first task is to list several most important news reports in the past month. The average difficulty level for Google News is 9.2, and that for our system is 4.5. Most users said Google News provides little help on completing this task. The two security experts said our system is very helpful to complete this task, and this task is typical for their everyday work. The second task is to summary the whole event of North Korean nuclear weapon test. The average difficulty level for Google News is 6.6, and that for our system is 4.3. The users said that our system places relevant news topics immediately surrounding the point of focus, which is very helpful to figure out the rough aspects of the whole event. The third task is to answer when, where, why and how of the Amish school shooting. The average difficulty level for Google News is 4.1, and that for our system is 6.7. Google News outperforms our system in this task. The most two important reasons given by the users are: (1) The keyword-based search technique of Google News is significantly better than ours; (2) It is much easier to extract fine details from the web news reports than from the video news reports. Based on the above experiments, one can find that: (1) Our system provides valuable service when the users do not have detailed preference. (2) Sophisticated keyword-based search techniques perform better when the users have detailed preference and need to learn the fine details. Therefore, our system is able to guide the users to build their own preference effectively and efficiently. Then keyword-based search techniques can be adopted to disclose fine details after the system catches the user's fine preference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, a large-scale video database exploration and analysis system is proposed by integrating novel algorithms of visualization, knowledge extraction, and statistical video analysis. By optimizing all components toward a single target, the proposed system achieves more effective and intuitive video database mining and exploration. To implement our system, a knowledge interpretation extraction algorithm is created to extract interesting knowledge and suppress uninteresting knowledge. As a result, the users may find news reports of interest and moreover get overviews of the whole news space for any given time span without the burdensome of mining large volume of uninteresting and useless reports. The knowledge interpretation we presented is able to bridge the interest gap. Experiments disclose that the proposed system is able to help the users build their own preference effectively and efficiently, which is very difficult with systems based on keyword-based systems, such as Google News. As a result, the most system needs to integrate our proposed techniques as the front end to capture the user preference, and keyword-based search techniques as the back end to disclose fine details. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,54.00,306.67,158.40,7.37;3,54.00,316.13,158.40,7.37;3,54.00,325.60,158.40,7.37;3,54.00,335.06,158.40,7.37;3,54.00,344.52,158.40,7.37;3,54.00,353.99,45.11,7.37"><head>Figure 2: </head><figDesc> Figure 2: News topic relation network discloses interesting knowledge to the users. Links of the news topic " test " disclose details of the event and response of the international community during the North Korean nuclear weapon test. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,234.00,344.52,324.00,7.37;3,234.00,353.99,298.98,7.37"><head>Figure 3: </head><figDesc>Figure 3: Showing the entire news topic relation network is too messy. The database has news reports between Oct. 1, 2006 and Oct. 10, 2006 from CNN, FOX, and MSNBC. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,54.00,321.19,240.04,7.37;4,54.00,330.65,207.09,7.37"><head>Figure 4: </head><figDesc>Figure 4: Hyperbolic visualization of the news topic relation network. Local details are embedded in the global semantic context. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,317.96,403.50,240.04,7.37;4,317.96,412.97,240.04,7.37;4,317.96,422.43,110.63,7.37"><head>Figure 5: </head><figDesc>Figure 5: Global and focused news topic relation network. On the bottom, " schoolhouse " is now the center and more relevant news topics are displayed and linked. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,131.04,339.50,205.93,7.37"><head>Figure 6: </head><figDesc>Figure 6: Slimmed news topic relation network of Figure 3. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,416.59,339.50,138.82,7.37"><head>Figure 7: </head><figDesc>Figure 7: An example of search results. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,97.69,227.33,152.68,7.37"><head>Figure 8: </head><figDesc>Figure 8: Video objects detection examples </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false" coords="7,339.02,64.15,197.93,48.74"><figDesc coords="7,369.01,64.15,137.94,7.37">Table 1: Semantic Concept Importance</figDesc><table coords="7,339.02,73.65,197.93,39.24">Concept (C (i)) 
w c (C (i)) 
Concept 
w c (C (i)) 
Announcement 
0.9 
Report 
0.3 
Sports 
0.5 
Weather 
0.5 
Gathered People 
1 
Unknown 
0.8 

</table></figure>

			<note place="foot" n="3"> NEWS TOPIC RELATION NETWORK VISUALIZATION The purpose of the visualization interface is to represent as much of the knowledge interpretation (i.e., the weighted news topic relation network) as possible to the users clearly and intuitively. To achieve this purpose, the visualization interface could show as many nodes and edges as possible to the users. However, because the network may be very large for large-scale video databases, it may be too messy to show the entire network to the users simultaneously. An example is given in Figure 3. The user may focus on a certain local detail at any time, so uninteresting detail should not get in the way and perhaps be removed. On the other hand, the points of interest for the user may change rapidly. The user may also need to explore from one part of the database to another part frequently. As a result, the global semantic context must be displayed at the same time as the interesting details are shown. To enable users to examine the local details under the global semantic context, we use the hyperbolic browsing technique [6] to visualize the semantic network. The hyperbolic browsing technique lays out the network on a hyperbolic plane and then projects the hyperbolic plane to the 2D screen space. There is a nice property of the hyperbolic plane for network visualization: the space increases exponentially along the distance. Therefore, the hyperbolic plane is able to hold exponentially increasing nodes as the network expands in depth. When the hyperbolic plane is projected to the 2D screen space, an appropriate projection can be selected so that a fisheye effect can be automatically implemented to enlarge the nodes around the focus and shrink the nodes far from the focus. As a result, the local details of interest can be represented in the global semantic context. An example is given in Figure 4. When a user wants to examine details at other points, she can simply drag the network to move the focus to a new position. As a result, the details of the network at any position can be checked in the global semantic context . And much more nodes can be represented simultaneously than without the hyperbolic interaction. Other visualization techniques could be used here as long as they retain the ability to see details in a global context. But efficient design requires that the technique be highly interactive and that relevant portions of the network be visible because they retain such rich knowledge content and potential for inference. By using the hyperbolic browsing technique, hundreds or thousands of nodes and edges can be visualized on the screen simultaneously or navigated to coherently. However, the news topic relation network of a large-scale news video database may have tens of thousands of nodes and edges. Therefore, it&apos;s still untenable to display all the nodes and edges simultaneously. As a result, the network must be slimmed according to the user&apos;s current point of interest. To resolve this problem, we integrate an information retrieval algorithm in our system. At any time of the exploration, the user can express her special interest by double clicking the nodes of the network. Consequently, the system knows that the user&apos;s new point of interest is the selected node, s I . Then, the system must take two actions in response to the user input: (1) modify the network to disclose the relevant knowledge associated with s I , and (2) retrieve the most relevant news reports from the database and present them to the user. To perform Action (1), we must extract a new news topic relation network that is relevant to the user input but still preserves the global semantic context. To achieve this purpose, we &quot; boost &quot; the relevant knowledge items relevant to the user input on the global network by a relevance factor: ´ K D (s I ) = {(k i , w U (k i ) × ϖ (k i , s I )) | 1 ≤ i ≤ N} = k i , w U (k i ) × c max{r(s a ,s I ),r(s b ,s I )} (2)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="8,336.21,127.51,221.78,7.13;8,336.22,136.98,221.78,7.13;8,336.22,146.44,221.79,7.13;8,336.22,155.91,29.89,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Applications of videocontent analysis and retrieval</title>
		<author>
			<persName>
				<forename type="first">Nevenka</forename>
				<surname>Dimitrova</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Hongjiang</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Behzad</forename>
				<surname>Shahraray</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Lbrahim</forename>
				<surname>Sezan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Thomas</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Avideh</forename>
				<surname>Zakhor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="55" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,172.32,221.78,7.13;8,336.22,181.78,221.78,7.13;8,336.22,191.25,221.78,7.13;8,336.22,200.71,153.05,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Conceptoriented indexing of video database toward more effective retrieval and browsing</title>
		<author>
			<persName>
				<forename type="first">Jianping</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Hangzai</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ahmed</forename>
				<forename type="middle">K</forename>
				<surname>Elmagarmid</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IF: 2.715. Google Cite: 12. SCI Cite: 6)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="974" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,217.12,221.79,7.13;8,336.22,226.59,142.77,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual information retrieval</title>
		<author>
			<persName>
				<forename type="first">Amarnath</forename>
				<surname>Gupta</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ramesh</forename>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="70" to="79" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,243.00,221.78,7.13;8,336.22,252.47,221.78,7.13;8,336.22,261.93,145.16,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Themeriver: Visualizing theme changes over time</title>
		<author>
			<persName>
				<forename type="first">Susan</forename>
				<surname>Havre</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Beth</forename>
				<surname>Hetzler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Lucy</forename>
				<surname>Nowell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization (InfoVis)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,278.34,221.78,7.13;8,336.22,287.81,221.78,7.13;8,336.22,297.27,221.79,7.13;8,336.22,306.74,33.87,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-faceted insight through interoperable visual information analysis paradigms</title>
		<author>
			<persName>
				<forename type="first">Elizabeth</forename>
				<forename type="middle">G</forename>
				<surname>Hetzler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Paul</forename>
				<surname>Whitney</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Lou</forename>
				<surname>Martucci</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Jim</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,323.15,221.78,7.13;8,336.22,332.61,221.78,7.13;8,336.22,342.08,221.79,7.13;8,336.22,351.54,57.34,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">The hyperbolic browser: A focus+context technique based on hyperbolic geometry for visualizing large hierarchies</title>
		<author>
			<persName>
				<forename type="first">John</forename>
				<surname>Lamping</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ramana</forename>
				<surname>Rao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,367.95,221.78,7.13;8,336.22,377.42,221.78,7.13;8,336.22,386.88,53.80,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Confidence-based dynamic ensemble for image annotation and semantic discovery</title>
		<author>
			<persName>
				<forename type="first">Beitao</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Kingshy</forename>
				<surname>Goh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="195" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,403.30,221.78,7.13;8,336.22,412.76,221.78,7.13;8,336.22,422.22,221.79,7.13;8,336.22,431.69,61.98,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring large-scale video news via interactive visualization</title>
		<author>
			<persName>
				<forename type="first">Hangzai</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Jianping</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Jin</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">William</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Shin &apos;ichi</forename>
				<surname>Satoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="75" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,448.10,221.78,7.13;8,336.22,457.57,221.78,7.13;8,336.22,467.03,136.69,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial analysis of news sources</title>
		<author>
			<persName>
				<forename type="first">Andrew</forename>
				<surname>Mehler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Yunfan</forename>
				<surname>Bao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Xin</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Yue</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Steven</forename>
				<surname>Skiena</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="765" to="772" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,483.44,221.78,7.13;8,336.22,492.91,221.78,7.13;8,336.22,502.37,106.04,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">A probabilistic framework for semantic video indexing, filtering, and retrieval</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Milind</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Thomas</forename>
				<forename type="middle">S</forename>
				<surname>Naphade</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,518.78,221.78,7.13;8,336.22,528.25,221.79,7.13;8,336.22,538.14,184.21,6.15"  xml:id="b10">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName>
				<forename type="first">Lawrence</forename>
				<surname>Page</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Sergey</forename>
				<surname>Brin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Rajeev</forename>
				<surname>Motwani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Terry</forename>
				<surname>Winograd</surname>
			</persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1999" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,554.13,221.78,7.13;8,336.22,563.59,221.78,7.13;8,336.22,573.06,221.78,7.13;8,336.22,582.52,138.48,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Content-base image retrieval at the end of the early years</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">M</forename>
				<surname>Arnold</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Marcel</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Simone</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Amarnath</forename>
				<surname>Santini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Ramesh</forename>
				<surname>Gupta</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,598.93,221.78,7.13;8,336.22,608.40,221.79,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Timemines: Constructing timelines with statistical models of word</title>
		<author>
			<persName>
				<forename type="first">Russell</forename>
				<surname>Swan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">David</forename>
				<surname>Jensen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,624.81,221.79,7.13;8,336.22,634.27,80.36,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Bridging the gaps</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Jarke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,650.69,221.79,7.13;8,336.22,660.57,107.20,6.15"  xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">Marcos</forename>
				<surname>Weskamp</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Newsmap</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.21,676.56,221.78,7.13;8,336.22,686.03,221.78,7.13;8,336.22,695.49,221.78,7.13;8,336.22,704.96,221.78,7.13;8,336.22,714.42,17.93,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing the non-visual: Spatial analysis and interaction with information from text documents</title>
		<author>
			<persName>
				<forename type="first">James</forename>
				<forename type="middle">A</forename>
				<surname>Wise</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">James</forename>
				<forename type="middle">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Kelly</forename>
				<surname>Pennock</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">David</forename>
				<surname>Lantrip</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Marc</forename>
				<surname>Pottier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Anne</forename>
				<surname>Schur</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Vern</forename>
				<surname>Crow</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Information Visualization (InfoVis)</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
