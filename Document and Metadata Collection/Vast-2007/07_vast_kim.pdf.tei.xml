<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Analytics on Mobile Devices for Emergency Response</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Sungye</forename>
								<surname>Kim</surname>
							</persName>
							<affiliation>
								<orgName type="laboratory">Purdue University Regional Visualization and Analytics Center (PURVAC)</orgName>
								<orgName type="institution">Purdue Homeland Security Institute (PHSI) Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yun</forename>
								<surname>Jang</surname>
							</persName>
							<affiliation>
								<orgName type="laboratory">Purdue University Regional Visualization and Analytics Center (PURVAC)</orgName>
								<orgName type="institution">Purdue Homeland Security Institute (PHSI) Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Angela</forename>
								<surname>Mellema</surname>
							</persName>
							<affiliation>
								<orgName type="laboratory">Purdue University Regional Visualization and Analytics Center (PURVAC)</orgName>
								<orgName type="institution">Purdue Homeland Security Institute (PHSI) Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">David</forename>
								<forename type="middle">S</forename>
								<surname>Ebert</surname>
							</persName>
							<affiliation>
								<orgName type="laboratory">Purdue University Regional Visualization and Analytics Center (PURVAC)</orgName>
								<orgName type="institution">Purdue Homeland Security Institute (PHSI) Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Timothy</forename>
								<surname>Collins</surname>
							</persName>
							<affiliation>
								<orgName type="laboratory">Purdue University Regional Visualization and Analytics Center (PURVAC)</orgName>
								<orgName type="institution">Purdue Homeland Security Institute (PHSI) Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Analytics on Mobile Devices for Emergency Response</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>mobile visualization, visual analytics, emergency re-</term>
					<term>sponse</term>
					<term>Index Terms:</term>
					<term>I36 [Computer Graphics]: Methodology and</term>
					<term>Techniques—Interaction techniques; I38 [Computer Graphics]:</term>
					<term>Applications—Visual Analytics</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education, business, law enforcement, protective services, medical services, scientific discovery , and homeland security. In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our system provides visual analytics with locational scene data within a simple interface tailored to mobile device capabilities. In particular, we focus on processing and displaying sensor network data for first responders. To verify our system, we have used simulated data of The Station nightclub fire evacuation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Our goal is to make mobile devices valuable tools for emergency response by effectively visualizing relevant, selected information (e.g., images, 3D models, and sensor data streams) on devices with varying capabilities and resolutions. We are developing a mobile visual analytic system that processes and displays sensor, location, and video data for first responders to increase situational awareness and enable more effective response. Visual analytics is defined as the science of analytical reasoning facilitated by interactive visual interfaces <ref type="bibr" coords="1,221.70,496.19,13.78,8.97" target="#b15">[20]</ref>. Mobile visual analytics extends the visual analytics process using state-of-theart mobile devices to increase the effectiveness and interactivity of analysis on-site. Visual analytics provides a solution for first responders and public safety command personnel requiring advanced analytical insight by allowing them to analyze and understand onscene , active emergency situations through interactive, integrated data analysis and visualization. Several factors enable mobile visual analytics to be a valuable solution for first responders. First, the mobility of the handheld devices using wireless connectivity can minimize the " fog of war " environment allowing first responders to attain rapid and actionable * e-mail: {inside|jangy|amellema|ebertd}@purdue.edu † e-mail: {tfcollins}@purdue.edu on-site decision making. Hence, mobile visual analytic tools can provide improved situational awareness and support first responders in planning immediate life-saving responses and for prioritizing actions in emergency situations. Second, the rapidly growing capabilities of these mobile devices, such as PDAs and cell phones, make it possible for these devices to gain acceptance as useful tools in a variety of fields. In particular, 2D and 3D graphics-accelerated mobile devices have become important in the game market, often delivering PC quality rendered images. However, most mobile devices still have many limitations including small screens, limited user interfaces, a short battery life, low bandwidth of the system bus, slow CPU clock speed, limited storage capacity, and a lack of advanced graphics hardware. Despite these problems, many researchers and developers have been exploring the use of mobile devices in various applications. In particular, visualization on handheld devices has gained increasingly popularity due to their mobility and various functionalities. Previously, visual analytics of sensor data on mobile devices was presented by Pattath et al. <ref type="bibr" coords="1,421.88,356.15,13.69,8.97" target="#b11">[16]</ref>. They showed the analysis and visualization of network and sensor data on mobile devices utilizing football games as a testbed. In our work, we extend this mobile analysis and visualization approach to emergency training and planning, and present a scalable interactive visual analysis system for emergency response. This paper is organized as follows: Section 2 discusses the background and Section 3 summarizes visual analytics for emergency response. Section 4 describes the design of our system and Section 5 presents visualization and analytics on client mobile devices. Section 6 gives a brief summary of implementation and results of our system. Section 7 discusses the capabilities and potential of our system as a visual analytic tool for emergency response. Finally , Section 8 presents conclusions and discusses some possible extensions for visual analytics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p> In relation to emergency training and planning, we can classify related work into three categories: visualization of sensor data, 2D and 3D visualization on mobile devices, and visual analytics on mobile devices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visualization of sensor data</head><p>With the increase in applications for sensor networks, manipulation and visualization of sensor data streams have become a crucial component . Fan and Biagioni <ref type="bibr" coords="1,410.93,613.67,14.99,8.97" target="#b6">[11] </ref>described approaches to process and interpret data gathered by sensor networks for geographic information systems. The approaches combine database management technology , geographic information systems, web development technology , and human computer interactive design to visualize the data gathered by wireless sensor networks. Their work differs from ours in that our system is based on a mobile environment, whereas, their work is web-based. Koo et al. <ref type="bibr" coords="1,431.75,683.39,14.99,8.97" target="#b9">[14] </ref> implemented software to analyze multi-sensor data for pipeline inspection of gas transmission. The information gathered by sensors is parsed and converted before it is saved in a database. They intended to manage sensor data effectively using a database. However, the system was also based on a desktop PC environment. Pattath et al. <ref type="bibr" coords="2,218.04,67.20,14.89,8.97" target="#b11">[16] </ref>implemented an interactive visual analytic system to visualize sensor network data during football games on PDAs. However, they did not provide enough processes and structures for real-time streaming of sensor data from a server. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="35"></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">2D and 3D visualization on mobile devices</head><p> We often need to visualize complex 3D models for displaying an urban environment. Much research has been conducted in displaying urban environments effectively through the use of mobile devices. City models are important factors for visually communicating spatial information related to an urban area. Particularly, we can divide this issue into two components. First, simplification of a represen- tation <ref type="bibr" coords="2,80.77,204.83,9.69,8.97" target="#b4">[9,</ref><ref type="bibr" coords="2,93.61,204.83,11.25,8.97" target="#b5"> 10,</ref><ref type="bibr" coords="2,108.02,204.83,11.14,8.97" target="#b8"> 13,</ref><ref type="bibr" coords="2,122.32,204.83,12.00,8.97" target="#b13"> 18] </ref>makes it possible to visualize complex 3D models of the environment with a limited graphical capability. For instance, the simplest way is to extract and draw that feature lines of 3D models. Second, effective transmission <ref type="bibr" coords="2,230.75,234.71,14.24,8.97" target="#b12">[17,</ref><ref type="bibr" coords="2,248.63,234.71,11.89,8.97" target="#b13"> 18] </ref>between a server and a client enables mobile devices to visualize complex models. Dollner et al. <ref type="bibr" coords="2,114.47,265.67,15.01,8.97" target="#b5">[10] </ref>described visualizations to represent abstract and comprehensible drawings of 3D models providing line drawing to enhance edges, tone shading, and simulated shadows. The purpose of their work is to render a large number of models (e.g., a city scene) in real-time; however, it is not designed for mobile devices. Diepstraten et al. <ref type="bibr" coords="2,120.11,315.47,10.43,8.97" target="#b4">[9] </ref>proposed a remote line rendering technique between the server and client. The server extracts feature lines of 3D models and transmits them. The clients just draw the results that were transmitted from the server. The clients do not need to have high computational capabilities since they only draw 2D lines. Hekmatzada et al. <ref type="bibr" coords="2,121.18,365.27,14.89,8.97" target="#b8">[13] </ref>also described non-photorealistic rendering of 3D models based on a server and client environment. Their work provided transmission of meshes from a server progressively and level of detail (LOD) as well; therefore, it is possible to navigate in nearly real-time. Quillet et al. <ref type="bibr" coords="2,177.40,405.23,14.99,8.97" target="#b13">[18] </ref>presented two optimization methods to visualize an urban environment on mobile devices interactively . One optimization method extracts feature lines and then changes the lines into vector lines. The other splits the urban environment into cells in order to transmit them as a stream. Their work also provides an efficient LOD solution. Pouderoux and Mar- vie <ref type="bibr" coords="2,67.09,464.99,14.88,8.97" target="#b12">[17] </ref>proposed two levels of adaptivity to display a large amount of terrain data regardless of the devices. The terrain data is partitioned into regular tiles and the tiles around a viewer are transmitted as a stream. The tiles are rendered using a pre-computed triangle strip path. 2D visualization can be an alternative to the above two approaches if we compromise and take advantage of 2D visualization instead of 3D visualization. 2D graphics and visualization is just as efficient in the case of information visualization. Hence, there are many applications that utilize 2D capabilities of mobile devices in fields such as geographic information systems <ref type="bibr" coords="2,222.72,565.67,13.79,8.97" target="#b10">[15]</ref>, entertainment, education, business, and industry. Moreover, OpenVG <ref type="bibr" coords="2,250.65,575.63,10.55,8.97">[1] </ref> and Mobile 2D Graphics (M2G) are boosting the development of more 2D applications that are scalable across any screen size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Visual analytics on mobile devices</head><p> The creation of visual analytics for mobile devices has several challenges . It is different from visual analytics on common desktop systems because of the restricted display space and computing resources of mobile devices. Sanfilipa et al. <ref type="bibr" coords="2,211.92,663.47,15.00,8.97" target="#b14">[19] </ref> introduced InfoS- tar <ref type="bibr" coords="2,66.84,673.43,9.52,8.97">[2]</ref>, an adaptive visual analytics platform for mobile devices. Their work was applied at SuperComputing2004 (SC2004) to provide information such as maps, schedules, an exhibitor lists, and provided visual exploration to conference attendees. Similarly, the work by Pattath et al. <ref type="bibr" coords="2,132.12,713.27,14.88,8.97" target="#b11">[16] </ref> provided a visual analytic tool for the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VISUAL ANALYTICS FOR EMERGENCY RESPONSE</head><p>For emergency response, a well-designed visual analytics system is necessary. We need to tailor the display capability to the responders and their roles and provide a succinct, quickly understood display of relevant information extracted from all information acquired. For example, SWAT (Special Weapons And Tactics) teams are highly trained groups of police officers whose missions include hostage rescue, dignitary protection, and high-risk warrant services. These missions all require successful coordinated information collection and exchange. In the case of the SWAT team responding to an active shooter in a school, the first and most critical requirement of the team leader, as well as all responders, is the most accurate situational awareness possible: In addition, the capability to provide information back to the emergency operation center, such as indicating rooms cleared or information contradictory to current situational assessment, is also vital. As previously indicated, relevant information is specialtydependent . A firefighter responding to a fire at the same building would need some of the above information, as well as task-specific information, such as fire spread, potential toxic gases or locations of dangerous goods stored. Moreover, the first generation mobile analytics should target readily available technology, such as PDAs and smartphones. This display system will be useful in decision support during emergency response, as well as planning for event response. The system will allow responders to be free from time spent on information gathering and focus on response actions. Response actions, such as asset dispersal, will be assisted with a visual display of current information while after-action reviews will also be enhanced by having the ability to clearly see information such as evacuation routes that are not used efficiently. After-action review methods are also aided through real-time analysis of actions taken during response time. This system will enhance training for response to many unique emergency situations, rather than simply the current scenario discussed here. <ref type="figure" coords="2,318.00,693.36,31.04,8.97" target="#fig_0">Figure 1</ref>shows the abstraction of our system structure that has a server-client architecture. Our system focuses on the utilization of various types of datasets such as images/video, 3D models, sensor data, and text data. All of the streaming data are received from and are preprocessed by each server in the server group. In <ref type="figure" coords="3,260.86,233.88,29.78,8.97" target="#fig_0">Figure 1</ref>, the data converter in a server group converts all input data into the appropriate types for the mobile visual analytic client. This conversion is necessary for visualization on mobile devices and involves determining the appropriate representation of the data for rapid, in-field cognition on a small-screen mobile device. The data created for desktop systems cannot be used for mobile devices without further preprocessing because of the limits of mobile devices in terms of memory, bandwidth, and screen resolution. The converter preprocessing has components that are customized for each type of input data stream, but in general it uses a flexible structure to allow the input of a variety of data based on the given response situation. Moreover, the structure is designed to allow tailored processing of the same input data for different response situations and different roles in the response. For our initial prototyping, we are using pre-generated simulations so that all of the data can be transmitted to the client initially, similar to the approach of Pattath el al. <ref type="bibr" coords="3,193.59,403.43,13.70,8.97" target="#b11">[16]</ref> . However, the architecture is designed to allow real-time network pulling of data from the server in actual response and training scenarios. Our client visualization tool consists of four components: the 2D/3D visualization system, the visualization of streaming personnel location data and sensor data (initially simulated), visual analytics , and the user interfaces. These functions can be utilized for situational awareness and assessment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SYSTEM DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing in a data converter </head><p>In our current system, we are using four input datasets that can be roughly categorized by their real-time properties. The abstracted/simplified background images and font data as well as the 3D models are immutable data during processing for visual analytics ; whereas the other data sources, the sensor/video data, and location data are time-varying. Therefore, our data converters are designed to process both time-varying data and static data. For the static data (e.g., images/blueprints, 3D models, text files), conversion to a format that enables execution on a mobile device in real-time under the management of the server is performed. Using vector images gives our system a scalable capability independent of the device screen size and resolution. Hence, all images are converted into vector path data in scalable vector graphics (SVG) for- mat <ref type="bibr" coords="3,70.17,633.47,9.60,8.97" target="#b0">[3]</ref>. We provide both two-dimensional and three-dimensional visualization of incident location, since three-dimensional viewing can provide various first-person views of the situation that provide new perspectives to the responders. The two-dimensional view can often provide the best, overall situational awareness for the responders but may lack the visual cues necessary for navigation in-field. For the conversion of time-varying data (e.g., sensor/video data, location data), special processing is needed to provide proper synchronization of the temporal data streams in a networked  ment. Moreover, filtering and selection of large data streams is necessary to enable real-time use on processor-and memory-limited mobile devices. To effectively utilize large streaming data on mobile devices, we employ simple compression/transformations of the data to reduce both network bandwidth and local storage requirements. We also use data importance characteristics to determine update rates, data items to skip, and interpolation methods to maintain our performance requirements. Finally, level of detail, level of abstraction, and level of data aggregation are chosen not only to enable interactive performance but also to reduce visual clutter and enable effective visualization and analysis on the mobile device. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Management of streaming data on the client </head><p>To deal with the large size of time-varying data streams, we need to utilize an appropriate data structure for storage. We use a circular queue, as shown in <ref type="figure" coords="3,416.82,411.95,30.61,8.97" target="#fig_2">Figure 2</ref> , in order to minimize memory consumption. A circular queue is a particular implementation of a queue, where insertion and deletion are totally independent. Though our system focuses on client-based visualization and analytics , such queuing structures can be used for processing of streaming data in server-client architectures. In our work, the size of the circular queue is set to 30 to provide short-term reference data for visualization and still fit in the memory of PocketPC phones and PDAs. In <ref type="figure" coords="3,355.88,491.75,29.51,8.97" target="#fig_2">Figure 2</ref> , the simulation data manager serves as a communication handler in a server-client system and can be composed of several types of data managers. The data manager is responsible for updating the appropriate entry type in each element of the queue (e.g., sensor, location). The application in a main thread can then take the data from the queue by time stamp without suspension occurring due to any network traffic. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUALIZATION ON MOBILE DEVICES</head><p>The main concerns for visual analytics on mobile devices are the device limitations (screen size, memory) and the appropriate data aggregation/abstraction level to enable effective decision making. In our work, the device memory limitations are primarily solved on the server component during data conversion to an appropriate representation. Hence, our mobile visualization client mainly deals with the visual representation to enable visual analytics on mobile devices for emergency response. (NIST) <ref type="bibr" coords="4,81.67,413.52,14.99,8.97" target="#b7">[12] </ref>after the fire. We used two simulated datasets including fire data and evacuation data of 419 intelligent agents. The fire was simulated using the Fire Dynamics Simulator (FDS4) <ref type="bibr" coords="4,245.30,433.44,10.43,8.97" target="#b1">[4] </ref>developed by NIST. FDS is a computational fluid dynamics (CFD) model of fire-driven fluid flow and provides time-resolved temperature, carbon monoxide, carbon dioxide, and soot distribution throughout the building for the duration of the fire. These calculations show how the fire and smoke propagate through the building and the results were used in the evacuation model for movement of agents. Agents start to move towards the exits to find the nearest known exit at the time fire started. All simulation and 3D model data that we used for this work was provided by Purdue Homeland Security Institute (PHSI) <ref type="bibr" coords="4,81.67,533.03,9.52,8.97" target="#b3">[8]</ref>. <ref type="figure" coords="4,97.29,533.03,30.24,8.97" target="#fig_3">Figure 3</ref>shows the floor plan of The Station nightclub. The 3D model and the background image we used were generated with same scales and locales such as The Station nightclub based on the document <ref type="bibr" coords="4,116.03,562.91,14.88,8.97" target="#b7">[12] </ref>from NIST. <ref type="figure" coords="4,63.96,572.87,29.70,8.97" target="#tab_1">Table 1</ref>shows information included in the two simulated datasets. For the agent data, 'the number of steps an agent took' in <ref type="figure" coords="4,63.35,592.91,26.45,8.97" target="#tab_1">Table 1</ref> provides a time stamp for the simulation. The fire simulation files are combined into a solution dataset by a data converter, as mentioned in Section 4. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Example test scenario</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualization of simulated data</head><p>For this scenario, as shown in <ref type="figure" coords="4,164.75,643.43,25.67,8.97" target="#tab_1">Table 1</ref> , we visualize the two simulated datasets, agent and fire data, to provide efficient emergency information . These datasets have different characteristics that are representative of most emergency response datasets. The agent dataset is agent-centered (e.g., location, health), whereas, the fire dataset is global, time-centered information (e.g., time-centered fire spread, carbon monoxide data). graphics. The position is shown by a circle and the time-evolving path is drawn by line segments. Based on the size of data contained in the circular queue, the paths of the agent movement is visualized using ghosting as a method of temporal visualization. The color of each agent's path can be pre-assigned based on agent/team designation , it can be changed based on the agent's health, or it can be randomly assigned. In our dataset, each agent has two health indicators based on the fractional effective doses (FED) for heat exposure and for gas concentration. Agents become unconscious and cannot move anymore when either of these values reaches 0.5. We nominally visualize healthy agents as green and display unconscious agents using red. A health level between 0.0 and 0.5 is visualized with yellow to orangish colors predefined in a health level table. Since in this scenario, we are tracking the patrons evacuating and not responders entering an incident, <ref type="figure" coords="4,461.78,563.63,29.94,8.97" target="#fig_5">Figure 4</ref> shows the evacuation of agents visualized in a 2D visualization for global evacuation analysis. During the visualization, the number of agents with each health status is displayed in the information window. After analyzing , it is apparent that only four of 30 have become incapacitated. In addition, we provide a 3D perspective view of the agent movement within the environment to better understand factors that may have determined the evacuation paths chosen. Similarly, 3D navigation and observation can help train first responders to have effective and intuitive recognition of potential evacuation routes and visual building characteristics that may lead responders to probable alternative paths taken by people missing during an actual emergency incident. <ref type="figure" coords="4,318.00,683.27,30.18,8.97" target="#fig_7">Figure 5</ref> shows the movement of agents in our 3D environment using two views at different camera positions. To visualize this information, we use separate 16-element color and gray-level tables. Temperature and HRR are displayed using colors , while smokes, CO 2 and CO are drawn using gray-levels. The visualization of each is overlaid on the 2D or 3D environment. <ref type="figure" coords="5,279.09,489.48,15.00,8.97;5,54.00,499.44,33.29,8.97" target="#fig_9">Fig- ure 6 and</ref><ref type="figure" coords="5,89.52,499.44,30.16,8.97" target="#fig_10">Figure 7</ref> show the results of the visualization of temperature and CO data at different time steps. In <ref type="figure" coords="5,212.92,509.40,30.52,8.97" target="#fig_9">Figure 6</ref>and <ref type="figure" coords="5,261.40,509.40,29.26,8.97" target="#fig_10">Figure 7</ref>, we use 7×7 pixels to interpolate and visualize temperature and CO since the data was transformed on a coarse grid for performance. We also encountered a problem with this data because the fire simulation data we have is not exactly aligned to the provided 2D floor map because the simulation was conducted for the bounding area of the building of the 2D map. Figures confirming this misalignment can be found in the Rhode Island Nightclub Investigation Image Archive <ref type="bibr" coords="5,84.72,589.07,10.55,8.97" target="#b2">[5] </ref>of NIST. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Mobile Visual Analytics</head><p>There is general analytic information that is commonly required for most emergency response situations. Such information helps first responders suggest response priorities and plan actions based on their evaluation of effective situational awareness common operating picture data. <ref type="figure" coords="5,119.14,662.99,27.28,8.97" target="#tab_2">Table 2</ref>classifies and lists analytic methods for our simulated agent and fire data in terms of personnel (agent) and environment categories. The most basic information is the location and movement of people and assets (see <ref type="figure" coords="5,124.04,703.31,30.67,8.97" target="#fig_5">Figure 4</ref>and <ref type="figure" coords="5,173.03,703.31,29.42,8.97" target="#fig_7">Figure 5</ref> ). In our scenario, to analyze the effectiveness of the evacuation, the number of agents at each status (alive, unconscious, dead) and the number of agents at each exit are provided in an information window. <ref type="figure" coords="5,497.15,474.12,30.17,8.97" target="#fig_11">Figure 8</ref>shows a few visual analytic results with numerical values. It shows the number of agents in each health condition and the number of agents at each exit used for evacuation. When the fire started, most agents started to run for the exit that was most familiar to them or was closest to them. However, this is not the exit most heavily used: most agents ran towards the bar exit. <ref type="figure" coords="5,457.55,534.00,30.79,8.97" target="#fig_12">Figure 9</ref> shows the congestion near the main exit that caused the heavy usage of the bar exit. In <ref type="figure" coords="5,327.72,553.91,29.04,8.97" target="#fig_9">Figure 6</ref>, <ref type="figure" coords="5,362.42,553.91,29.04,8.97" target="#fig_10">Figure 7</ref>, and <ref type="figure" coords="5,412.36,553.91,29.04,8.97" target="#fig_11">Figure 8</ref>, we obtain an unexpected result from the analysis. Although the kitchen area was safer than others were in terms of temperature and carbon monoxide, nobody used this area for the evacuation. <ref type="figure" coords="5,422.64,583.79,30.55,8.97" target="#fig_12">Figure 9</ref>shows the rates (the number of agents per second) of evacuation during the fire. The slope of the data line decreased when the main exit became blocked by the crowd. Thus, many agents chose the other exit (bar exit) for evacuation instead of the main exit. This happened 90 seconds after the fire occurred. The kitchen exit was not used as an evacuation exit because of its unfamiliarity. <ref type="figure" coords="5,422.82,643.55,34.94,8.97" target="#fig_0">Figure 10</ref>shows the information of a specific agent selected by a user. The selected agent is displayed using magenta. In <ref type="figure" coords="5,387.98,663.47,33.83,8.97" target="#fig_0">Figure 10</ref> , the agent with ID = 21 became unconscious near bar exit before evacuation (left). The FED of CO of the agent is over 0.5, whereas the agent with ID = 23 who evacuates through the stage exit shows the FED of heat and gases at low levels (right). Moreover, obtaining the change in health levels of each agent at each time can help first responders (e.g., fire fighters) to establish rescue priorities. <ref type="figure" coords="6,152.68,468.24,35.05,8.97" target="#fig_0">Figure 11</ref>shows different health levels of 100 agents by using distinct colors in the middle of the duration of the visualization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">IMPLEMENTATION AND RESULTS</head><p>We have implemented and tested our tool on a Dell Axim X51v PDA that uses the Intel 2700G graphics processor and 16MB of video RAM and on a Sprint PCS Vision SM smart device PPC-6700, which uses the Window Mobile 5.0 and a 416 MHz Intel processor . However, our tool will be run on any PDA using Pocket PC with sufficient processing capabilities. We use the Hybrid Ras- teroid3 for OpenGL|ES and OpenVG library provided by Hybrid Graphics, which is a reference implementation and provides functions by OpenGL|ES 1.1, OpenVG 1.0 and EGL 1.3 specification announced by Khronos <ref type="bibr" coords="6,142.11,613.07,10.43,8.97">[1] </ref>group. All images in this paper were captured with the Win32 version of our system. <ref type="figure" coords="6,233.66,623.03,35.31,8.97" target="#fig_0">Figure 14</ref>shows our system running on a Dell Axim X51v PDA and a Sprint PCS smartphone. We set the main screen as a 2D orthogonal projection of a building model for global situational awareness since the visualized entities are all in the same 2D plane. In addition, our system does provide 3D perspective views of all the data within the 3D building model. All of the user interfaces are represented with transparency in order to provide a non-invasive interface. The main menu and information window can be also be hidden to not interfere with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>situational </head><p>awareness visualizations. Therefore, this interface can always guarantee a full main view of the situation to the user. Due to the small screen size of mobile devices, the problem of an efficient user interface is another challenge for visualization on mobile devices. GLUT|ES <ref type="bibr" coords="6,390.12,503.76,10.43,8.97">[6] </ref>has been developed for WinCE and Win32 systems based on OpenGL|ES as the open source implementation. However, it can be space-consuming for the visualization of information . Therefore, we implement the API for a user interface based on OpenVG. Currently, it provides a button, a time slider, a hiding window, a line graph, and vector fonts. <ref type="figure" coords="6,463.55,553.55,34.98,8.97" target="#fig_0">Figure 12</ref> shows the overall user interfaces and camera changes in the 2D environment. All buttons can be activated and deactivated. We use the buttons for play, pause, stop, speed-up/down, toggle of 2D/3D, display of exits , viewing of fire spread (e.g., temperature, HRR, CO 2 and CO), and selection mode. The time slider shows the progress of the overall simulation. Menu windows are opened with their own button. Our tool has two menu windows: one is used for displaying text information and the other is used for visualizing additional graphic information, such as graphs. <ref type="figure" coords="6,422.98,643.32,34.75,8.97" target="#fig_0">Figure 13</ref>shows some of the viewing options including a wireframe and a bounding box of a 3D model in our tool. As a prototype for mobile visual analytic tools for emergency response, our tool presents efficient and interactive visual analytic methods and provides visualization of various types of data. For situations requiring rapid decisions, such as emergency response analysis and services, our system can be used as an efficient testbed.  Based on the overall visualization and analysis for our test simulation datasets, we observed that some agents evacuated using the stage exit at the beginning of the fire. Most agents ran about in confusion while they moved to the exits located opposite to the source of the fire near the stage. Some agents who tried to evacuate out of the main exit failed because of congestion near the corridor between the main exit and the main bar. Hence, many agents moved to the bar exit. As a result, 21 out of 100 agents became incapacitated and 24 agents did not find an evacuation exit. Most of the agents who became unconscious were found near the main exit. At the completion of the simulation, 35% exited via the bar exit, 0% via the kitchen exit, 16% via the main exit, 8% via the stage exit, and 17% of the agents used a window for their evacuation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CAPABILITIES &amp; POSSIBILITIES FOR MOBILE VISUAL ANALYTICS FOR EMERGENCY RESPONSE</head><p>Visual analytic systems for emergency response can be used not only during actual emergency events, but also during training, and for hotwash <ref type="bibr" coords="7,99.26,693.36,10.42,8.97">[7] </ref>and After Action Reviews (AAR) of exercises and incidents. The hotwash is an immediate debriefing and critique conducted immediately after the exercise and incident. The AAR is a detailed and extensive assessment and comment on the exercise with written evaluations that takes place several days or weeks after the exercise. The AAR does not judge success or failure but rather focuses on learning what happened, why things happened, and what tasks and goals were or were not accomplished. Mobile visual analytics adds mobility to common visual analytics and can provide enhanced situational awareness on site during the hotwash. Such rapid and appropriate awareness leads to " lessons learned, " which is intended to guide future response direction in order to avoid repeating errors made in the past. Hence, the effectiveness of analysis and evaluation to identify strengths and weaknesses of the response to a given situation can be enhanced. The capabilities of visual analytics needed for the hotwash include providing integrated visual analytics of additional data. Visual analytics of correlated 3D, 2D, video, and audio data will be extremely beneficial to created lessons learned from exercises and enable new insight from the interactive exploration and analysis of all information captured during the exercise. Replaying by time slider and location, by exercise plan as chapters, and at increased/decreased speeds is demanded. Pause, fast-forward, and rewind functions are also required. Evaluators and analysts will request to display various video, 2D/3D scene reconstruction, and statistical results of the exercise. For requirements of the above aspects , our system has been equipped with enough functionalities and extendibility to allow its expansion for these activities. To evaluate the effectiveness and capacity of our system to be used in real emergency situations or emergency training, we had several current and former emergency response personnel (state police , fire) use our system and provide informal feedback. Their feedback indicated that they felt our system could be useful in real emergency situation when integrated with real time personnel tracking data because accurate situational awareness is a crucial issue in real situations. Responders also felt that our system will be very useful for personnel training, including pre-planning scenarios and site inspections. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="2,390.84,183.46,94.19,7.96"><head>Figure 1: </head><figDesc>Figure 1: System overview </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="2,328.32,380.52,144.67,11.66;2,328.32,398.28,187.23,11.66;2,328.32,415.92,223.86,11.66;2,328.32,433.68,218.08,11.66"><head>@BULLET </head><figDesc>Where are all team members located? @BULLET Where are the locations of responding personnel? @BULLET Where are the secure, neutral and hot zones of the incident? @BULLET What locations provide opportunity or threat information? </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="3,97.68,192.70,152.68,7.96"><head>Figure 2: </head><figDesc>Figure 2: Data structure for streaming data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="3,318.00,211.18,240.00,7.96;3,318.00,220.66,155.92,7.96"><head>Figure 3: </head><figDesc> Figure 3: Floor plan of the Station nightclub. Image courtesy: National Institute of Standards and Technology. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="3,318.00,683.40,240.11,8.97;3,318.00,693.36,240.05,8.97;3,318.00,703.32,239.91,8.97;3,318.00,713.28,239.81,8.97"><head></head><figDesc> We employ the scenario of The Station nightclub fire of West Warwick , Rhode Island, that took place on the February 20, 2003. The scenario has been completed from an investigation and a computer simulation by the National Institute of Standards In Technology </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="4,54.00,225.34,240.08,7.96;4,54.00,234.82,240.12,7.96;4,54.00,244.30,162.38,7.96"><head>Figure 4: </head><figDesc>Figure 4: Visualization of sensor agents in 2D environment; (left) 30 agents in alive status (green) and their movement path by fading line, (right) four agents in unconscious status (red). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="4,63.96,713.27,230.02,8.97"><head></head><figDesc>Agent Data: The agent information is displayed using 2D vector </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="4,329.88,224.62,216.15,7.96"><head>Figure 5: </head><figDesc>Figure 5: Visualization of sensor agents and 3D environment. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="4,327.96,703.31,230.06,8.97;4,318.00,713.27,240.08,10.03"><head></head><figDesc> Fire Data: The fire simulation data includes the level of temperature , HRR (Heat Release Rate), smoke, CO 2 and CO in the fire. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="5,54.00,225.34,239.98,7.96;5,54.00,234.82,21.05,7.96"><head>Figure 6: </head><figDesc>Figure 6: Visualization of the temperature spread at different time steps. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10" coords="5,63.00,427.42,221.90,7.96"><head>Figure 7: </head><figDesc>Figure 7: Visualization of the CO spread at different time steps. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="5,318.00,225.34,239.95,7.96;5,318.00,234.82,240.01,7.96;5,318.00,244.30,68.81,7.96"><head>Figure 8: </head><figDesc>Figure 8: Visualization of information related to agents: (left) the number of agents in each health condition, (right) the number of agents at each exit. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12" coords="5,331.20,428.86,213.52,7.96"><head>Figure 9: </head><figDesc>Figure 9: Visualization of the rates for evacuation and crowd. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13" coords="6,54.00,225.34,240.06,7.96;6,54.00,234.82,240.10,7.96;6,54.00,244.30,141.08,7.96"><head>Figure 10: </head><figDesc>Figure 10: Information of a selected agent: (left) the agent located in front of a bar exit has not yet evacuated, (right) the agent positioned at a stage exit succeeded in evacuation. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14" coords="6,82.08,435.94,183.75,7.96"><head>Figure 11: </head><figDesc>Figure 11: Change of the health condition of agents. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15" coords="6,318.00,224.14,240.03,7.96;6,318.00,233.50,239.89,7.96;6,318.00,242.98,45.88,7.96"><head>Figure 12: </head><figDesc>Figure 12: Overall user interfaces for 2D: (left) main menu, time slider, graph window and text window, (right) change of view direction and zoom-in. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16" coords="6,330.96,432.34,214.00,7.96"><head>Figure 13: </head><figDesc>Figure 13: Views of the agents in 3D wireframe environment. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17" coords="7,54.00,470.26,240.01,7.96;7,54.00,479.74,53.89,7.96"><head>Figure 14: </head><figDesc> Figure 14: Photos of our system running on PDA (top) and smartphone (bottom) </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false" coords="4,74.88,273.94,197.92,105.79"><figDesc coords="4,84.96,273.94,177.99,7.96;4,83.04,294.50,14.94,7.97;4,118.20,294.50,37.85,7.97;4,74.88,304.46,188.95,7.97;4,118.20,313.82,110.21,7.97;4,118.20,323.30,154.60,7.97;4,118.20,332.78,41.27,7.97;4,84.12,342.62,12.78,7.97;4,118.20,342.62,91.98,7.97;4,118.20,352.10,68.20,7.97;4,118.20,361.58,73.98,7.97;4,118.20,371.06,81.52,8.67">Table 1: Information involved in simulated datasets Data Information Personnel Identifier of the agent (e.g. customer number) The number of steps an agent took Position in 2D space (with 640×480 coordinate) Health levels Fire Temperature at each position HRR at each position Smoke at each position CO 2 , CO at each position</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true" coords="4,334.08,254.38,207.73,135.91"><figDesc coords="4,334.08,254.38,207.73,7.96">Table 2: Information required for analysis of fire emergency</figDesc><table coords="4,337.32,276.74,201.31,113.55">Object 
Information 
Personnel 
Agent identifier 
Path of evacuation 
Health condition (alive, unconscious, dead) 
Change of health level 
Exit identifier used for evacuation 
The number of evacuated agents per each time 
Environment 
Structure of an environment (building) 
Exit area 
The number of agents at each exit 
Distribution of temperature, HRR 
Distribution of toxic gas (CO, CO 2 , Smoke) 

</table></figure>

			<note place="foot" n="8"> CONCLUSION AND FUTURE WORK We have shown a flexible prototype mobile visual analytics system for emergency response and demonstrated its use for a building fire evacuation. For situations requiring rapid decisions such as placement and location of public safety assets during a critical incident, our system can be used as an efficient prototype and testbed. In the future, we will extend this work to include more analytic functions to enhance emergency situational awareness and support rapid decision making. For example, a tool for interactively selecting specific personnel groups and comparing information within and between them can improve analysis of response asset allocation and training effectiveness. Moreover, our system can be extended for actual first responder 3D tracking, visualization, and video information for training and in-field deployment support. The integration of RSS data and social network data (e.g., family, friend group, local community, police, fire station, hospital, and government department) will provide interesting visual representation and interrogation challenges, and will further increase the usefulness of our system for emergency response.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS </head><p>We wish to thank the Purdue Homeland Security Institute (PHSI) for supplying the simulation data. This work has been funded by the U.S. National Science Foundation (NSF) under grants 328984 and 0121288, and by the U.S. Department of Homeland Security Regional Visualization and Analytics Center (RVAC) Center of Ex- cellence. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,72.24,76.94,220.02,7.97;8,72.24,86.40,150.79,7.96"  xml:id="b0">
	<monogr>
		<title level="m" type="main">Scalable Vector Graphics (SVG) XML Graphics for the Web</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.28,95.90,179.87,7.97;8,72.24,105.24,121.99,7.96"  xml:id="b1">
	<monogr>
		<title level="m" type="main">NIST Fire Dynamics Simulator (FDS) and Smokeview</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.22,114.74,221.64,7.97;8,72.24,124.20,220.61,7.96;8,72.24,133.68,97.99,7.96"  xml:id="b2">
	<monogr>
		<title level="m" type="main">Rhode Island Nightclub Investigation Image Archive</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.20,180.98,221.86,7.97;8,72.24,190.46,221.79,7.97;8,72.24,199.94,221.59,7.97;8,72.24,209.42,187.50,7.97"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Dddas for fire and agent evacuation modeling of the rhode island nightclub fire</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Chaturvedi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mellema</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Filatyev</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Gore</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCS &apos;06: Workshop on Dynamic Data-Driven Application Systemse</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.21,218.90,221.85,7.97;8,72.24,228.38,221.35,7.97;8,72.24,237.86,69.92,7.97"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Remote line rendering for mobile devices</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Diepstraten</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gorke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Ertl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Graphics International</title>
		<meeting>the Computer Graphics International</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.19,247.22,221.74,7.97;8,72.24,256.70,221.80,7.97;8,72.24,266.18,139.51,7.97"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time expressive rendering of city models</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Dollner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Walther</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Information Visualization</title>
		<meeting>the 7th International Conference on Information Visualization</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="245" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.19,275.66,221.77,7.97;8,72.24,285.14,221.84,7.97;8,72.24,294.62,221.48,7.97;8,72.24,304.10,221.72,7.97;8,72.24,313.58,78.32,7.97"  xml:id="b6">
	<analytic>
		<title level="a" type="main">An approach to data visualization and interpretation for sensor networks</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">S</forename>
				<surname>Biagioni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HICSS &apos;04: Proceedings of the 37th Annual Hawaii International Conference on System Sciences</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="30063" to="30064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.23,322.94,221.72,7.97;8,72.24,332.42,36.03,7.97;8,127.52,332.42,166.46,7.97;8,72.24,341.90,78.56,7.97;8,170.65,341.90,123.37,7.97;8,72.24,351.35,211.13,7.96;8,72.24,360.83,172.39,7.99"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Report of the technical investigation of the station nightclub fire</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">L</forename>
				<surname>Grosshandler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">P</forename>
				<surname>Bryner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Madrzykowski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kuntz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">[NIST NCSTAR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005-06" />
		</imprint>
	</monogr>
	<note>nist. .gov/public_affairs/releases/ RI_finalreport_june2905.htm</note>
</biblStruct>

<biblStruct coords="8,72.21,370.34,221.69,7.97;8,72.24,379.82,221.70,7.97;8,72.24,389.30,221.42,7.97;8,72.24,398.66,126.56,7.97"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Non-photorealistic rendering of complex 3d models on mobile devices</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hekmatzada</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Meseth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Klein</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 8th Annual Conference of the International Association for Mathematical Geology</title>
		<meeting>8th Annual Conference of the International Association for Mathematical Geology</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="93" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,408.14,221.82,7.97;8,72.24,417.62,221.70,7.97;8,72.24,427.10,221.47,7.97;8,72.24,436.58,61.89,7.97"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Visualization for a multi-sensor data analysis</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">O</forename>
				<surname>Koo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">D</forename>
				<surname>Kwon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G</forename>
				<surname>Yoon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">S</forename>
				<surname>Seo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">K</forename>
				<surname>Jung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Graphics, Imaging and Visualization</title>
		<meeting>International Conference on Computer Graphics, Imaging and Visualization</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="57" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.20,446.05,221.78,7.97;8,72.24,455.53,221.45,7.97;8,72.24,464.89,221.48,7.97;8,72.24,474.37,161.76,7.97"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Visualization of travel itinerary information on pdas</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Masoodian</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Budd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AUIC &apos;04: Proceedings of the fifth conference on Australasian user interface</title>
		<meeting><address><addrLine>Darlinghurst, Australia, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Australian Computer Society, Inc</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="65" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.23,483.85,221.80,7.97;8,72.24,493.33,221.86,7.97;8,72.24,502.81,221.83,7.97;8,72.24,512.29,221.67,7.97;8,72.24,521.77,45.20,7.97"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactive visualization and analysis of network and sensor data on mobile devices</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pattath</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Bue</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Jang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">S</forename>
				<surname>Ebert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Zhong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ault</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Coyle</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VAST &apos;06: Proceedings of IEEE Symposium on Visual Analytics Science and Technology</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.22,531.25,221.82,7.97;8,72.24,540.61,221.85,7.97;8,72.24,550.09,221.53,7.97;8,72.24,559.57,221.58,7.97;8,72.24,569.05,129.32,7.97"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive streaming and rendering of large terrains using strip masks</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Pouderoux</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-E</forename>
				<surname>Marvie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GRAPHITE &apos;05: Proceedings of the 3rd international conference on Computer graphics and interactive techniques in Australasia and South East Asia</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.18,578.53,221.42,7.97;8,72.24,588.01,221.44,7.97;8,72.24,597.49,79.61,7.97"  xml:id="b13">
	<monogr>
		<title level="m" type="main">Client-server visualization of city models through non photorealistic rendering</title>
		<author>
			<persName>
				<forename type="first">J.-C</forename>
				<surname>Quillet</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J.-E</forename>
				<surname>Marvie</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.25,606.97,221.47,7.97;8,72.24,616.33,221.71,7.97;8,72.24,625.81,221.53,7.97;8,72.24,635.30,221.83,7.97;8,72.24,644.78,221.85,7.97;8,72.24,654.26,118.17,7.97"  xml:id="b14">
	<analytic>
		<title level="a" type="main">An adaptive visual analytics platform for mobile devices</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Sanfilippo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>May</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Danielson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Baddeley</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Riensche</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kirby</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Collins</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Thornton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Washington</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Schrager</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">V</forename>
				<surname>Randwyk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Borchers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Gatchell</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;05: Proceedings of the 2005 ACM/IEEE conference on Supercomputing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.21,663.74,221.95,7.97;8,72.24,673.22,151.75,7.97"  xml:id="b15">
	<monogr>
		<title level="m" type="main">Illuminating the Path: The R&amp;D Agenda for Visual Analytics</title>
		<editor>J. J. Thomas and K. A. Cook</editor>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
