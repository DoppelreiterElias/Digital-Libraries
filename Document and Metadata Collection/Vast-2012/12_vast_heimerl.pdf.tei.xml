<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Classifier Training for Text Document Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Florian</forename>
								<surname>Heimerl</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Steffen</forename>
								<surname>Koch</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Harald</forename>
								<surname>Bosch</surname>
								<roleName>Student Members, Ieee</roleName>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Thomas</forename>
								<surname>Ertl</surname>
								<roleName>Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">Visual Classifier Training for Text Document Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Visual Analytics methods and techniques are often applied in tasks in which large amounts of information should be analyzed, but neither computational nor human effort alone is sufficient to solve the prob- lem <ref type="bibr" coords="1,48.17,646.68,14.19,8.02" target="#b19">[20,</ref><ref type="bibr" coords="1,65.59,646.68,10.64,8.02" target="#b42"> 43]</ref>. Almost all scenarios adhering to that scheme include @BULLET All authors are with the Institute for Visualization and Interactive Systems, Universität Stuttgart, e-mail: {firstname.lastname}@vis.uni-stuttgart.de. For information on obtaining reprints of this article, please send e-mail to: tvcg@computer.org. the subtask of searching or filtering the data to be analyzed. This is even the case if tasks are unknown at the beginning of the analysis, or it starts with rather vague objectives. Eventually, a more concrete information need (c.f. <ref type="bibr" coords="1,376.62,646.16,14.34,8.02" target="#b23">[24]</ref>) will manifest during exploration in such analysis processes and lead to these subtasks. According to Pirolli and Card <ref type="bibr" coords="1,314.65,666.09,13.74,8.02" target="#b29">[30]</ref>, sensemaking processes in general incorporate search and filtering in several stages and at different levels of data aggregation (c.f. <ref type="bibr" coords="1,311.07,686.01,13.74,8.02" target="#b14">[15]</ref>, chapter 3.6 and 7.4). Another observation regarding search and filter tasks in analytics and sensemaking scenarios is the difference in analyst's information needs compared to other, more common search tasks such as casual web-search. Very often this information need is recall-biased, meaning that as many relevant objects as possible should be retrieved, while still maintaining good precision. </p><p> In typical retrieval scenarios, analysts have to translate their information need into a keyword search query or a combination of filtering constraints. This implies the ability of deriving such explicit queries or constraints either from a set of examples small enough to be manageable for an analyst, or the skill to guess useful keywords. Generally, the creation of queries and constraints works well, especially if the analysts are experienced with respect to selecting keywords that are relevant to their tasks and domain. However, it can still be difficult to achieve good coverage of relevant documents if not all important keywords are considered. Therefore, we suggest the interactive creation of binary classifiers as an additional, complementary method for improving information retrieval during analytic tasks. Here, the purpose of the classifier is to separate a corpus into relevant and non-relevant documents and to improve recall through generalization. To demonstrate the applicability of the general approach, we employed linear support vector machine (lSVM) classification in text retrieval scenarios while foreseeing its integration with keyword search based methods. In this work, we compared three different methods for training such lSVMs for text classification tasks in a user study. If classification is to be used during search tasks, two basic requirements have to be met. First, the classifier must be adaptable to capture an analyst's information need as well as possible. As a consequence, analysts must be able to perform this adaptation, ideally without being machine learning experts. We suggest an approach that enables analysts to build classifiers primarily on the observation level <ref type="bibr" coords="2,231.55,304.31,13.74,8.02" target="#b9">[10]</ref>, which is also referred to as black box model <ref type="bibr" coords="2,156.26,314.27,9.52,8.02" target="#b3">[4]</ref>. Hence, we employ views for presenting text documents on different levels of abstraction, as well as views depicting the classifier's state. Second, analysts must be enabled to assess the creation of a classifier by observing and judging its quality, e.g, via providing suitable interaction techniques on these views. This is vital for detecting classification problems such as overfitting, or too broad generalization while adapting the classifier to the analytic task. In our evaluation we compare three modes for labeling, all exploiting active learning either directly or indirectly as part of semi-guided visual interactive labeling. Method 1 is rather straightforward, providing a text-based interface, while the second and third provide interactive visual access to the classifier state. Method 3 offers additional degrees of freedom regarding document labeling. There are further demands that are important for creating classifiers more efficiently. In order to build a classifier, labeling effort is needed which can only be justified if an important information need exists. The employment of classifiers for search should therefore be an optional step. If analysts decide on building one, its creation should be efficient. For the methods compared during our evaluation we foresaw measures to lower the entry barrier in building such a classifier in terms of labeling effort. Significant speedup is achieved by exploiting active learning (AL) approaches, which aim at making users label those instances that provide the potentially highest impact in improving the classifier's performance during the next training iteration. As described in Section 3, we transfer the labeling initiative in Method 3 to the analyst, while still giving hints on selecting good labeling candidates . We accomplish this by providing analysts with feedback on the informativeness of their current labeling choice. Apart from picking the most informative documents, we let analysts select multiple instances for labeling at once when using Method 3. An integration of binary classifiers with key term search can be exploited in two ways. First, the key term query can be used to bootstrap an initial classifier in order to relieve analysts of the burden to build it from scratch. This idea is part of our approach. Second, the built classifier can be applied in conjunction with classical search and filter techniques. The latter forms an interesting research problem on its own, which we consider as future work. We see our approach as a visual analytics technique that can be employed as part of larger visual text analysis tasks, but also for creating dedicated classifiers that can be exploited in batch mode processing of text document collections. It is scalable with respect to the size of the document collection. Additionally, we provide a mechanism for capturing provenance information of a classifier's evolution. This information is made available in form of an interactive history graph enabling users to start over at any intermediate state of the classifier in case subsequent labeling actions lead to unsatisfactory results. The following section provides an overview of related work for interactive classifier creation. Section 3 describes our interactive visual approach for training lSVMs for document classification, including details on all three employed methods. Subsequently, we describe the evaluation of these methods through a user study in Section 4 and discuss its outcome in Section 5. Finally, the conclusion and ideas on future work can be found in Section 6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK &amp; BACKGROUND</head><p> Supervised classifier learning methods are widely used for text classification tasks and have shown to be very effective. An overview of the learning frameworks used is given in <ref type="bibr" coords="2,438.86,206.75,14.94,8.02" target="#b35">[36] </ref>and <ref type="bibr" coords="2,473.01,206.75,13.74,8.02" target="#b23">[24]</ref>. Supervised machine learning methods derive models from labeled training data that allow for the automatic labeling of previously unseen data with user-defined classes. Clustering, in contrast is a data-driven, unsupervised method that tries to find classes of documents by grouping them according to their similarities. Because this work focuses on methods that are capable of capturing an analyst's specific information need, clustering is not discussed in this section in detail. We built three prototypes for the interactive labeling of text documents to efficiently create training sets for text classifiers. There are many systems that allow for the exploration of large document collections and that feature multiple views (e.g. SWAPit <ref type="bibr" coords="2,475.17,316.67,9.52,8.02" target="#b2">[3]</ref>, Jigsaw <ref type="bibr" coords="2,518.31,316.67,13.74,8.02" target="#b40">[41]</ref>, IN-SPIRE <ref type="bibr" coords="2,325.04,326.63,13.74,8.02" target="#b47">[48]</ref>, Bead <ref type="bibr" coords="2,366.95,326.63,9.41,8.02" target="#b6">[7]</ref>). We see our tools as great candidates for an integration into such systems. This would allow users to train their own classifiers to add self-defined document categories to those sys- tems. Seifert et al. <ref type="bibr" coords="2,339.76,366.81,14.94,8.02" target="#b37">[38] </ref>employ a document map visualization method that has been generated through advanced clustering. Their approach supports the user in exploring document collections for letting them find label candidates for classification. Moehrmann and Heidemann <ref type="bibr" coords="2,520.55,396.70,14.94,8.02" target="#b25">[26] </ref> present a systems that uses unsupervised learning to facilitate fast labeling of image datasets for classification. We think, however, that combining supervised classification methods with unsupervised methods , such as clustering of a document collection or automatic topic extraction may lead to two competing approaches. If there are automatic methods that can already identify clusters the way users need them, it is better to use those unsupervised methods, because there is no need to label training data. Only when users need their own, self-defined categories that are not reflected by any automatic clustering the work invested in the creation of a training set will pay off, and supervised learning methods are the right tool to use. The major drawback of supervised classification methods is their need for labeled training data. AL <ref type="bibr" coords="2,417.00,526.55,14.19,8.02" target="#b38">[39,</ref><ref type="bibr" coords="2,434.39,526.55,11.95,8.02" target="#b26"> 27] </ref>has been proposed as a method to reduce this drawback. Its central idea is to give the training procedure active control over the instances that it uses for training. AL starts out with a very small initial training set and a much larger set of unlabeled examples. It sets up a training/labeling loop that, during each iteration, presents unlabeled examples to a teacher, who is typically a human annotator. The learner will choose those examples that are maximally informative for it, thus trying to reduce the overall labeling effort of the teacher. We apply a technique called uncertainty sampling introduced by Lewis and Gale <ref type="bibr" coords="2,431.65,616.21,14.94,8.02" target="#b21">[22] </ref> which selects the example as the most informative one that receives the lowest confidence (or probability) rating for the label assigned by the classifier. In this work we use lSVMs <ref type="bibr" coords="2,396.19,646.43,14.94,8.02" target="#b45">[46] </ref>for classification. They have been successfully applied to text classification <ref type="bibr" coords="2,438.38,656.39,14.19,8.02" target="#b16">[17,</ref><ref type="bibr" coords="2,455.75,656.39,11.95,8.02" target="#b17"> 18] </ref>and are known to achieve high classification accuracy with the vector space model. An SVM <ref type="bibr" coords="2,307.64,676.32,9.71,8.02" target="#b4">[5,</ref><ref type="bibr" coords="2,320.45,676.32,7.47,8.02" target="#b7"> 8] </ref>in its basic form is a linear, binary classifier. It tries to find a separating hyperplane between two classes in the training data in such a way that its margin is maximized, where the margin is the distance to the closest training data points on both sides of the decision boundary. Schohn and Cohn <ref type="bibr" coords="2,363.48,726.46,14.94,8.02" target="#b34">[35] </ref>as well as Campbell et al. <ref type="bibr" coords="2,480.11,726.46,10.45,8.02" target="#b5">[6] </ref> propose selecting the unlabeled example closest to the hyperplane (uncertainty sampling) during each AL iteration. They also provide theoretical justification that this strategy is a good heuristic for effectively reducing the version space (the set of hyperplanes that separate the training data) in each iteration. Tong et al. <ref type="bibr" coords="3,156.92,83.27,14.94,8.02" target="#b43">[44] </ref> propose two additional strategies with higher performance for some datasets but with considerably increased computational effort. There is some work that tries to give humans more control during the labeling process, which is related to our User-Driven Method (see Section 3.3). Settles <ref type="bibr" coords="3,110.02,134.20,14.94,8.02" target="#b39">[40] </ref>presents an innovative AL system for text classification that lets users produce classifiers with only a couple of minutes labeling time. Although the system largely follows the traditional concept of AL, users can label terms that they get to choose themselves. Seifert et al. <ref type="bibr" coords="3,86.35,185.13,14.94,8.02" target="#b36">[37] </ref>take an approach that is similar to our third system by reversing the initiative in the labeling loop from the machine to the user. They propose a visualization that displays unlabeled examples in a multi-classification scenario and evaluate their proposed method for an image dataset as well as text datasets. There are three main differences to our work. First, their visualization is solely based on the confidence of the classifiers' decisions. The radial layout concentrates uncertain documents in the middle which might make interacting with them difficult. Contrastingly, we reflect document similarity in our layout and intentionally relax the layout of documents along the decision boarder in order to help users in labeling those having the highest potential for classifier improvement. Furthermore, our approach offers multiple linked views that provide additional information about the underlying document collection as well as the current classification model. Second, they used a standard seed set for their experiments, while we propose a bootstrapping approach. Third, we conducted a user evaluation comparing three methods for labeling documents with each other. Seifert et al. evaluate their visualization by simulating user behavior, and report encouraging results for that. Real users, however, tend to show highly diverse and unpredictable behavior, as becomes obvious from our user evaluation. In our opinion, it is thus unclear whether results from Seifert et al. can be generalized to training scenarios with real users. Höferlin et al. <ref type="bibr" coords="3,97.24,415.39,14.94,8.02" target="#b15">[16] </ref>present a system that allows users to build a cascade of linear classifiers for image classification. Their implementation features a scatter plot depicting the decision boundary and the classification uncertainty of each example. The system is tailored to ad-hoc training of classifiers in video surveillance applications, e.g. for the automatic detection of certain objects in a scene and features deep interaction with the model allowing the analyst to remove learned features if they judge them to be spurious based on their world knowl- edge. Various ways of directly incorporating a user into classifier training and model evaluation have been discussed. Ankerst et al. <ref type="bibr" coords="3,248.48,516.13,10.45,8.02" target="#b1">[2] </ref>argue that the goal of such a cooperation between user and computer is to fully exploit the capabilities of both. While users are able to provide their domain knowledge and analytic capabilities for steering the algorithm into the desired direction, the computer follows the guidance of the user by deriving patterns from the user's input and by providing visualizations that enable the user to judge its current state. Bertini and Lalanne <ref type="bibr" coords="3,63.04,585.87,10.45,8.02" target="#b3">[4] </ref> term this cooperation between user and computer " Interactive Machine Learning " and identify two different approaches of integrating user feedback and interaction into the classifier training process . These are white-box integration, where a user is directly involved in the model building process and can intervene at any time during this process, and black-box integration, which restricts users to tuning parameters of the training algorithm and providing the training set. The model, however, is learned automatically only giving the user visual feedback about its properties. Both concepts fit the classification of Endert et al. <ref type="bibr" coords="3,77.72,675.53,13.74,8.02" target="#b9">[10]</ref>, who coin the terms parameter level interaction and observation level interaction, respectively. Our approach mainly adheres to the concept of black-box integration, with the exception of the Term Weight View (see Section 3.2). May and Kohlhammer <ref type="bibr" coords="3,127.04,716.50,14.94,8.02" target="#b24">[25] </ref>describe a general approach to create customized classifiers in an interactive visual manner and sketch it as an extension of the information visualization pipeline. They exemplify the approach by enabling users to build and iteratively refine a classifier based on a decision tree. Interactive decision tree construction systems are a commonly described variant for visualizing interactive machine learning concepts (<ref type="bibr" coords="3,398.59,83.27,9.52,8.02" target="#b0">[1,</ref><ref type="bibr" coords="3,411.50,83.27,11.21,8.02" target="#b46"> 47,</ref><ref type="bibr" coords="3,426.10,83.27,11.21,8.02" target="#b22"> 23,</ref><ref type="bibr" coords="3,440.70,83.27,10.31,8.02" target="#b44"> 45]</ref> ). All those systems enable users to directly manipulate their models (e.g. by creating new tree nodes and selecting attributes and split points for them), making them representatives of the white-box approach. One possible explanation for their popularity is that their model can be transfered to a well-understood visual analogy easily. Poulet <ref type="bibr" coords="3,329.32,145.27,14.94,8.02" target="#b30">[31] </ref>presents multiple methods for visualizing classification models, some of which also allow for user interaction. The first one is a decision tree building system using an SVM algorithm to help users find good split points of their data. He then presents some methods of visualizing SVM decision boundaries using histograms to display the distribution of data around the hyperplane, scatter plot matrices and parallel coordinates. We also display data points around the decision boundary in our Main View, but we display all points to allow the user to label them directly. We cannot use scatter plot matrices or parallel coordinates, as proposed by Poulet, because of the high dimensionality of our document vectors. Approaches employing other supervised learning frameworks mostly follow the black-box integration paradigm, giving the user no or very little direct influence on the model. Interaction possibilities are typically reduced to labeling new data instances or correcting previous classification decisions of the algorithm in those cases. Such an approach is used by Eaton et al. <ref type="bibr" coords="3,426.92,306.90,10.45,8.02" target="#b8">[9] </ref> to iteratively refine a regression model that provides scores for the importance of network security events. Fails and Olsen <ref type="bibr" coords="3,379.81,326.83,14.94,8.02" target="#b10">[11] </ref> present an approach for interactive learning that lets users quickly build accurate classifiers for image classification . Here, the user points out misclassified instances to the training algorithm, which subsequently adapts the model. Fogarty et al. <ref type="bibr" coords="3,529.55,356.71,14.94,8.02" target="#b12">[13] </ref> develop an image retrieval system that helps users in creating classifiers for certain concepts (e.g. scenic image, or images with faces). Users steer the classifier training by providing labeled examples of the concept that they have in mind, and stop as soon as they judge that the algorithm performs well enough. The resulting classifier can then be used to search for images e.g. in combination with a keyword search. This is related to the idea of a vertical search engine (c.f. <ref type="bibr" coords="3,497.13,426.45,13.74,8.02" target="#b23">[24]</ref>, p. 235). The user-interaction in our approach has similarities with that of an IR system that supports explicit relevance feedback. Users first start with a query, for which they are presented with initial results. By choosing relevant and non-relevant documents from the query's result set, the users give feedback to the IR system which is then able to improve the result set. Ruthven and Lalmas <ref type="bibr" coords="3,455.45,488.45,14.94,8.02" target="#b32">[33] </ref>give an overview of relevance feedback techniques in IR. Steed et al. <ref type="bibr" coords="3,476.18,498.42,14.94,8.02" target="#b41">[42] </ref> present a system that re-ranks retrieval results and finds new relevant documents according to user feedback. In our approach, users do not give relevance feedback regarding a specific query. The initial query is rather a means to bootstrap a classifier that can then be refined in subsequent iterations and we aim to support the construction of a classification model, which is stored and can be used many times after it has been trained once. <ref type="figure" coords="4,22.50,198.41,18.44,7.37">Fig. 2</ref>. The user queries the document collection to define bootstrapping examples in order to initially train the classifier. The classifier is then further trained by an active learning oriented interactive procedure. on the corpus. For this we rely on the Apache Lucene 1 framework. The initial training set is assembled by taking up to 50 of the most relevant results returned by Lucene and assigning them a positive label. Afterwards , we pick an equal number of random documents not returned by the search as non-relevant examples and train the initial lSVM from both sets. Bootstrapping text classification using keywords that describe document classes has been explored in the literature, e.g. by Jones et al. <ref type="bibr" coords="4,63.37,320.15,14.94,8.02" target="#b18">[19] </ref>who use it to bootstrap classifiers in a semi-supervised learning setting. <ref type="figure" coords="4,32.46,340.98,20.53,8.02">Fig. 2</ref> depicts the main feedback loop that applies to all of our methods . Analysts request an initial result set from the document repository by formulating their information need in a query. After the bootstrapping step, they are allowed to refine the model by iteratively assigning labels to previously unlabeled documents, thus actively assembling the training set of the classifier. In order to compute the optimal margin hyperplane for a set of labeled examples we used the LibLinear 2 library <ref type="bibr" coords="4,193.45,411.63,13.74,8.02" target="#b11">[12]</ref>, which is capable of solving lSVMs for large data vectors and huge training sets fast. Hence, it is perfect for dealing with text documents that are represented as large vectors in a so-called 'bag of words model'. The integration of Lucene and LibLinear thus ensures high scalability of our tools with respect to the corpus size. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method 1: The Basic Method</head><p> The Basic Method realizes textual AL without employing sophisticated visualization. Analysts have to label the text presented to them as relevant or non-relevant according to the given information need. The tool (see <ref type="figure" coords="4,85.40,528.87,21.47,8.02">Fig. 3</ref>) provides a Search Bar, where analysts can enter an initial query and start the classifier building procedure with the button next to it. The headline following the text field has been specifically introduced for our user evaluation and informs the participants about the current search task (see Section 4). The statement below gives feedback to the analyst whether the lSVM would classify the shown document as relevant or non-relevant in its current training state. In this way, analysts can see if and how often they agree with the classifier. The text area displays the content of the document that is judged as the most informative by the AL training procedure. After the analyst has labeled the document, it is added to the training set and the classifier is retrained. A new iteration starts by displaying the next document judged as most informative and updating the feedback line. There are three buttons that allow analysts to assign the displayed document a label (relevant or non-relevant), or to reject the document if they are unsure about its correct label. In case a document is rejected, an alternative, but also highly informative document is selected and displayed. <ref type="figure" coords="4,285.12,235.35,19.30,7.37">Fig. 3</ref>. The tool for the Basic Method features a text field for the initial query, a text label containing the classifier rating of the currently displayed document, a text field for the document and buttons to label or reject the document. The depicted version of the tool was employed in our user study. It therefore shows a hint informing participants about their current task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Method 2: The Visual Method</head><p>The Visual Method provides users with feedback on the classifier's state and lets them explore the classification context by representing the whole set of documents including labeled and unlabeled ones. AL is still enforced in the same way as in the Basic Method, meaning that users cannot influence which document is chosen next for labeling . Therefore, the interface of the Basic Method is also part of this method, as can be seen in <ref type="figure" coords="4,382.75,385.97,20.41,8.02">Fig. 4</ref>. The Visual Method's tool features multiple linked views that provide analysts with insights on the data and the classifier's state. Since most of the views are also employed in the tool for Method 3, the shared ones are explained by referring to <ref type="figure" coords="4,285.12,425.82,19.94,8.02">Fig. 1</ref>. The color mapping in our tools can be switched to address the needs of color-blind persons, as can be seen from <ref type="figure" coords="4,464.22,435.78,36.11,8.02">Fig. 4 and</ref><ref type="figure" coords="4,502.57,435.78,19.81,8.02">Fig. 1</ref>. Again, the Visual Method's tool contains the Search Bar for entering the initial query and a button to generate the initial classifier (see <ref type="figure" coords="4,285.12,465.86,24.00,8.02">Fig. 1a</ref>). The Main View (<ref type="figure" coords="4,384.70,465.86,24.67,8.02">Fig. 1b</ref>) shows a graphical representation of the current state of the classifier in form of a scatter plot. In the middle, a white separator is depicted, which represents the decision boundary of the SVM model. On both sides of this boundary, two colored regions are visible that contain documents, depicted as dots. The left part contains all the documents that are classified as non-relevant and the right one holds all documents that are classified as relevant. In <ref type="figure" coords="4,285.12,535.60,21.53,8.02">Fig. 1</ref>dots colored blue are the ones that have no label yet, whereas the purple dots represent the training documents. The scatter plot can be zoomed and panned to inspect regions of interest in detail. The horizontal axis of the scatter plot represents the confidence value for each document. This means that the actual distance from the separating hyperplane in the high-dimensional space is mapped to the distance from the separator depicted in the Main View. The vertical axis represents the diversity of the documents closest to the decision boundary. The vertical position of each document is computed as follows . Let U be the set of the 100 documents closest to the decision boundary. For each document in U, the projection of the vector on the first principal component of all document vectors in U is assigned as its vertical coordinate. For all the other documents d i further away from the decision boundary, the following weighted sum is used: </p><formula>v(d i ) = ∑ d∈U i cos(d i , d) · v(d) ∑ d∈U i cos(d i , d) </formula><p> where v gives the vertical position of a document, U i are the ten documents from the set U closest to d i , and cos is the cosine similarity between two documents in vector space. </p><p> The primary aspect of the Main View is the classification confidence of each document, which is represented by the horizontal axis. The vertical layout strategy emphasizes the diversity of the documents closest to the decision boundary, as those are the potentially most informative ones for subsequent classifier training. Computing the first principal component of only 100 document vectors rather than of all document vectors also significantly reduces computational effort, and thus waiting time for the user. An important interaction feature visible in (<ref type="figure" coords="5,206.20,136.73,25.28,8.02">Fig. 1b</ref>) is the Term Lens (the purple circle with the list of terms to its right). It can be activated by pressing the Shift button and its size can be adjusted via the mouse wheel. When hovered over documents, it displays at most ten terms sorted according to their document frequency (i.e. the number of documents under the lens that they occur in). The terms are displayed in form of a list, sorted according to term document frequency from top to bottom. The font size of each term also indicates its relative document frequency. Each term in the list is annotated with its absolute document frequency. If the Term Lens is hovered over areas of the scatter plot that contain none or exactly one document, its list stays empty. This lens enables analysts to quickly explore certain regions of the scatter plot and get an idea of the contents and the diversity of the documents in this region. The Cluster View (<ref type="figure" coords="5,112.62,279.85,24.46,8.02">Fig. 1c</ref>) contains a clustering of the set U. In contrast to the Main View, this view puts special emphasis on representing document similarity, but ignores classification uncertainty. The clustering is computed using the bisecting k-means algorithm. The subsequent projection into 2D is accomplished with the LSP algo- rithm <ref type="bibr" coords="5,53.50,329.66,13.74,8.02" target="#b27">[28]</ref>. We use the respective implementations of the Projection Explorer (PEx) project <ref type="bibr" coords="5,115.49,339.62,14.94,8.02" target="#b28">[29] </ref>for this. The classes of the documents are indicated by the colors of the circles (see <ref type="figure" coords="5,186.40,349.59,20.43,8.02">Fig. 1</ref> ). This view is supposed to give insight into how clusters of very similar documents are classified by the current model. This is described in more detail with the user-driven approach. The Term Lens is also available in this view. </p><p>The Content View, as can be seen in <ref type="figure" coords="5,182.18,393.08,24.99,8.02">Fig. 1d</ref> , displays the documents in focus. It either shows the document currently hovered by the mouse, or all documents under the lens. If only one document is in focus, the full text of this document is displayed in this view. If multiple documents have the focus, a list of all titles is displayed to give analysts an overview of the set of focused documents. </p><p>The only view that deviates from the black box approach is the Term Weights View (<ref type="figure" coords="5,88.56,466.46,23.49,8.02">Fig. 1e</ref>). It provides the user with direct information about the classification model. The first row of bar charts shows the ten terms with the highest changes compared to the classification model of the previous iteration. The second row displays the ten terms that have the highest positive weights in the current model, and the third row shows the ten terms with the highest negative weights in the current model. This view gives analysts an idea of what the training algorithm has learned from the training data. Each of the bars can be hovered by the mouse, and all the documents containing the respective term are highlighted in the Main View and the Cluster View. The Manual View (<ref type="figure" coords="5,113.55,569.73,23.61,8.02">Fig. 1f</ref>) was designed specifically for our user evaluation. It contains a task for users defining an artificial information need for the evaluation tasks, followed by a short legend of the colors and a list of available keyboard shortcuts. View (g) and </p><p>(i) shown in <ref type="figure" coords="5,31.50,609.58,20.93,8.02">Fig. 1</ref>are not used in this method. The Labeled Documents View (<ref type="figure" coords="5,159.72,623.19,24.85,8.02">Fig. 1h</ref>) contains four lists of the documents that have been assigned a label so far. The tabs 'relevant' and 'non-relevant' contain the documents labeled directly by the analyst , where the tabs 'relevant lucene' and 'non-relevant lucene' contain the documents added to the training set due to the bootstrapping. The documents in the lists can be highlighted in the Main View and the Cluster View by clicking on them. All described views are connected through brushing and linking, i.e. documents that are hovered in one view are highlighted in all the others. Highlighted documents are represented by filled black dots. The currently selected document for labeling is marked in the Main View and the Cluster View with a black outline. <ref type="figure" coords="5,294.12,201.39,18.87,7.37">Fig. 4</ref>. The tool for the Visual Method features the text field, the Search Bar, the Main View, the Cluster View, the Content View, the Term Weight View, the Labeled Document View, and the Manual View from <ref type="figure" coords="5,514.63,220.32,18.96,7.37">Fig. 1</ref>. It additionally includes the interface from the Basic Method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Method 3: The User-Driven Method</head><p>The User-Driven Method no longer uses AL directly, but incorporates some ideas derived from it. Here, analysts have full control over the selection of the documents they want to label. It also allows the labeling of multiple documents at once. The UI for Method 3 is depicted in <ref type="figure" coords="5,431.62,316.46,19.82,8.02">Fig. 1</ref>. Views 1a to f and 1h are similar to those in the tool for the Visual Method, but the Main View, the Cluster View, and the Term Weights View differ with respect to their interaction capabilities. The Main View and the Cluster View now provide interaction mechanisms for selecting single documents, by clicking on them, and multiple documents with a rectangular selection mechanism. When the Term Lens is activated, it can also be used to select multiple documents. In the cluster view heterogeneously classified but similar documents identify suitable regions for detailed inspection, since the chance that some of them are classified incorrectly is high. Accordingly, it is more likely to find candidates for labeling in such areas. The Term Weights View now not only allows highlighting the documents containing certain terms, but also selecting them by clicking on the respective bars. The Classifier History (<ref type="figure" coords="5,387.53,456.45,23.48,8.02">Fig. 1g</ref> ) provides global undo and redo functionality . It displays a tree showing all classification models trained by the analyst so far. Each training iteration gets a unique number by which it can be identified (second column) and users can give each model a name (first column). Furthermore, the number of negative and positive training instances (third column) as well as negative and positive classifications (forth column) is displayed. The user can go back to any of those models at any time and start new training iterations from a previous classifier state without losing an already trained model. In addition to providing undo functionality the Classifier History can be seen as a means for capturing classification provenance. This can be very useful if the classifier has to be adapted at a later time, even by a different analyst. The Labeling Controls (<ref type="figure" coords="5,389.47,586.47,22.14,8.02">Fig. 1i</ref>) provide two different functionalities with the progress bar and the four buttons below. The first three buttons let users label selected documents as relevant or non-relevant, and remove labels previously assigned during the same training iteration. The possibility to remove labels provides a local undo function, complementary to the global undo provided by the Classifier History. The fourth button trains the classifier with the new training set, to which the newly labeled documents have been added, and starts a new training iteration. Consequently, the Main View, the Cluster View, the Term Weights View and the Classifier History are updated. During each training iteration, the progress bar displays the training impact of the currently labeled instances. When documents are labeled by the analyst, they change their shape to triangles pointing upward in case they are labeled relevant, or downward in case they are labeled non-relevant. They also change their color respectively (see <ref type="figure" coords="5,377.35,736.42,19.90,8.02">Fig. 5</ref> ). This change of shape and color is <ref type="figure" coords="6,22.50,173.07,19.31,7.37">Fig. 5</ref> . Preview of classification results after labeling. Previously unlabeled documents (a) are assigned with a positive (b) or negative label (d). Resulting changes in classification are highlighted by the color of the corresponding region (c and e). The currently selected document is marked with a black outline (d). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ible </head><p> in the Main View and the Cluster View. As soon as some documents are labeled by the analyst, a preview is computed of what would happen if a new model were trained on a training set including the newly labeled documents. The unlabeled documents to which the new classification model would assign a different label are highlighted in color (see <ref type="figure" coords="6,60.74,292.78,20.14,8.02">Fig. 5</ref> ). Documents that change their color due to the preview of the next training step keep their circular shape. The preview is again visible in the Main View and the Cluster View. As the Cluster View already displays classification results by color, different colors have to be used for the preview. In our case the preview colors are more saturated and thus clearly distinguishable from the classification colors. Inspired by AL we foresaw three methods to support analysts in carrying out labeling actions that have the potential to speed up classifier training. As described before, the Main View's layout acknowledges the diversity of the documents closest to the decision boundary, letting analysts inspect them more easily. Furthermore, the cluster view only depicts the 100 most uncertain documents. This ensures that any labeling action in this view has considerable impact on the classifier's evolution during the next training step. The third measure informs analysts about the potential training progress of their labeling actions. Accordingly, a progress bar is displayed above the Labeling Controls informing analysts about the impact of their labeling. The impact is derived from the reduction of the margin size of the new SVM model compared to the old model based on the fact that in traditional AL, the margin can be maximally reduced to half of its previous size by selecting one example during each iteration. We are aware that for large training sets, i.e. training sets with about 50,000 and more documents the scatter plot in the Main View would become cluttered. The bootstrapping step applied at the beginning of our method should guarantee that even in cases where corpora contain a very small fraction of relevant documents, enough of them will be available for labeling positive examples. Afterwards, we can add a representative subset of the full corpus in such a case for classifier training with visualization-centric methods. The simplest method of creating such a subset would be random sampling. If necessary, that sampling step could be repeated for each training iteration. An alternative strategy for handling large training sets is to restrict the Main View to a subset of the whole corpus by displaying only the most uncertain documents (e.g. 10,000 of them) together with the training documents, and to hide all other documents. Since all other views either aggregate or restrict the amount of displayed information, the depicted visual methods are highly scalable regarding the amount of information to be classified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We carried out a user study in order to find out if a classifier can be trained visually and interactively. Additionally, we compared three different methods for classifier training in a test procedure aiming at qualitative as well as quantitative results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup and Procedure</head><p>For the qualitative evaluation we used three different data sets, two of which are well-known and widely used benchmark corpora with gold labels for text document classification. The problem with using gold labeled corpora is that an artificial information need has to be created that fits the existing gold labels of the corpus. This information need was derived from the gold labels in the corpus and communicated to the participants to ensure that their labels fit the original gold labels. By introducing this artificial information need, we achieve that the results of different participants can be compared and the classifiers' performances can be measured equally. A drawback of using standard test corpora in a user evaluation is that the participants' motivation can be expected to be dampened because they have no connection to the data set, an effect that Saraiya et al. <ref type="bibr" coords="6,418.36,187.56,14.94,8.02" target="#b33">[34] </ref>report. We thus expect that the reported results would be better if text corpora directly provided by an analyst would be used in combination with a real information need. Our first corpus is called 20 newsgroups 3 (henceforth 20ng), consists of usenet postings from 20 different newsgroups. The corpus was assembled by the author of the Newsweeder system <ref type="bibr" coords="6,471.08,248.12,14.94,8.02" target="#b20">[21] </ref>and has often been used as a benchmark corpus since then. For our evaluation we used the version of the 20ng corpus in which all the duplicate postings have been removed resulting in about 19,000 remaining postings. As the example task for this corpus we chose to consider all computer related posts as relevant and all the others as non-relevant (i.e. we assigned all posts from the comp.* newsgroups the gold label relevant, and all others the gold label non-relevant). The second corpus is a subset of Reuters RCV1 <ref type="bibr" coords="6,476.38,328.61,13.74,8.02" target="#b31">[32]</ref>, henceforth RCV1. The original corpus consists of over 800,000 Reuters newswire articles. We decided to use a subset of 12,000 documents to keep the size of the document pool that each user is presented with at a manageable size. All articles in the RCV1 corpus are labeled according to their topics, the geographical regions, and the industry branches referred to in an article. Hence, an article typically has multiple labels. The example task we chose for this corpus was to separate all sport news as the relevant documents from all other news. The third corpus was assembled by the authors and contains the abstracts of VisWeek publications (i.e. paper abstracts from VIS, InfoVIS , and VAST contributions), which amounts to about 1,200 documents . There are no gold labels for this corpus. We therefore used it in combination with an exercise task to give each participant some time to practice with each of the tools before the real labeling tasks were started. The exercise task we chose was to identify all abstracts as relevant , whose respective papers contain a natural language processing component. As all of our participants had a visualization background, we expected this to be a rather straightforward task for them. Each corpus underwent the standard preprocessing steps, i.e. documents were tokenized and transformed to length-normalized tf-idf vectors (see <ref type="bibr" coords="6,330.92,539.41,13.44,8.02" target="#b23">[24]</ref>). We did not apply any stemming algorithm on the tokens of the corpus to avoid confusing the participants by displaying terms that are hard to interpret. The participants were twelve PhD students from our visualization department. By asking them to take part in our evaluation, we expected to reduce the training effort for specifically the Visual Method and the User-Driven Method, because all of them were experienced in interactive visualizations. While some of them had a background in machine learning, most were inexperienced in this respect. Each participant was asked to create a classifier with two of our three methods. In order to diminish effects of becoming familiar with the data sets, the users were presented different corpora for their first and second task. Additionally, we permuted the order of the methods for each participant to diminish effects of becoming familiar with the tools. Those effects were likely to occur because of the similarities between our three tools. Each combination of method and labeling task has thus been executed exactly four times by four different participants in different order. During the evaluation the participants were encouraged to think aloud about all aspects of the tool or the task that came to their minds. All comments during the evaluation sessions were recorded on paper . In addition, the participants' actions were observed and interesting behavior, including mistakes and employed strategies of usage, were logged. The evaluation procedure for each participant comprised all of the following ten steps: i) initial instruction: Participants were instructed about the evaluation process and informed that they could stop the evaluation at any time and without giving any reasons for stopping. They were also informed about which data was recorded during the evaluation sessions and that all of the recordings were fully anonymized. ii) colorblindness test: We tested each participant for colorblindness with the Ishihara color plates. This was necessary because our tools contained red-green color differences by default, but could be switched to another color mapping if necessary. iii) tutorial for the first task: Participants were introduced to the tool for first method and received a short tutorial. Afterwards, they were able to use the tool with the exercise task until they felt confident enough to start the real labeling task with the next corpus. Questions about the tool or the labeling tasks were answered at any time. iv) tool evaluation for the first task: The first task was started. The participants had a maximum time of 15 minutes to accomplish this task, but they were also allowed to stop any time for any reason (e.g. because they were content with the classifier's performance or the training did not make any progress). v) questionnaire for the first task: Participants were asked to complete a questionnaire about the first task. vi) -viii): Tutorial , tool evaluation, and questionnaire for the second task. ix) final questionnaire: Participants were asked to complete a questionnaire with questions about their age and previous knowledge. x) comments and discussion: Finally we discussed the two methods and tasks with the participants and asked for their comments on both. The evaluation with each participant took about one to one and a half hours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Evaluation</head><p>As we aimed at comparing the classifiers' performance built by each participant, we used predefined queries for each task. The queries were preselected and kept constant for each participant and corpus in order to guarantee the same starting situation and to explicitly rule out suboptimal starting configurations caused by problematic initial queries. We expressed the artificial information need to the participants in terms of the initial query and in form of a short task description displayed in each of our three tools. We executed the queries once and kept the resulting initial training sets constant. Accordingly, all participants started with the exact same configuration. Our preselected initial queries were as follows: 20ng: computers network motherboard graphics RCV1: sports baseball basketball tennis game VisWeek: text For measuring the performance of the classifiers trained on the selections of the participants, we split each of our corpora randomly into an 80% training set and a 20% test set. The evaluation task was performed on the training set entirely. The smaller test set was used to evaluate the performance of the classifiers with respect to the gold la- bels. <ref type="figure" coords="7,41.46,622.33,20.63,8.02" target="#fig_2">Fig. 6</ref> shows the results of the qualitative evaluation for the two corpora with our three methods. Each of the diagrams depicts the classifier's evolution curve 4 for one of the six combinations of method and task. Hence, each diagram contains four classifier evolution curves for four different users. The classifier evolution curves generated by the participants are compared to a random sampling baseline (in blue) and simulated AL (in black). The random sampling curve depicts the average evolution (ten simulation runs) of a learner that randomly picks a document from the training set and assigns its gold label. The simulated AL curve has been generated with the Simple Method as proposed for interactive systems by Tong and Koller <ref type="bibr" coords="7,480.97,73.31,13.74,8.02" target="#b43">[44]</ref> . The resulting classifiers' performance was then measured on the test set of the respective corpus. Our participants assigned labels to documents according to the given task. We simulated learning with a perfect labeler using the gold labels to have a common baseline that all curves generated by the participants could be compared to. For the simulation of AL with the help of the gold labels, the same initial training documents as for the user evaluations were used to keep the results comparable. This is visible in the plots, because all of the classifier evolution curves are identical up until 100 instances before they diverge. For this reason, we have cut off much of the curves before 100 labeling actions. The dashed line in each of the diagrams indicates the performance of the classifier when trained on the complete training set with the gold labels. Some of the classifier evolution curves dominate the final performance value in some regions of the plots. This is a known phenomenon discussed in <ref type="bibr" coords="7,388.51,233.09,13.74,8.02" target="#b34">[35]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Evaluation</head><p> Qualitative feedback was requested from the users through a questionnaire on an optional basis after they carried out each evaluation task. The first six questions were the questions of the standard NASA-TLX test <ref type="bibr" coords="7,308.95,294.68,13.74,8.02" target="#b13">[14]</ref>, which measures the task load on the participants for each of the tasks. In contrast to the original NASA-TLX questionnaire, however , we transformed its original 21 point Lickert scale to a seven point scale. The seventh question asked the participants about the trust they had in the classifier they created. The eighth question asked for the reason why the participant stopped the task (e.g. classifier was trained well enough). The ninth question asked the participants about how useful the different views were for them when executing the task. As the tool for the Basic Method contained only one view, this question was omitted from the corresponding questionnaire. The users were given a list of all the titles of the windows containing the views and were asked to rate their usefulness. After completing the two tasks and the respective questionnaires the participants were asked to complete the final questionnaire. It requested the following information of the participants in this order: age, gender, the expertise in using web search engines, machine learning, using interactive visualizations, and carrying out information finding tasks. All questions (except the two regarding age and gender) were to be answered on a seven point Lickert scale. Finally, the users could state their opinions about both methods and which one they would prefer for building a classifier. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>As we already expected before conducting the evaluation, the small number of four participants for each task/tool combination did not yield statistically significant results. We therefore give a mostly informal report of our results and discuss the flaws and benefits of our three approaches. We also present all the performance curves for the trained classifiers, not only the mean values. This allows to differentiate between the single participant and to reference their performance curves directly from the text. Our participants are between 25 and 55 years old, most of them are in their late twenties or early thirties. Eleven of them were male, and one was female. They rated themselves as quite experienced using web search engines (6.15 on average). The average level of expertise in machine learning of our participant was much lower (3.31 on average) with half of the participants scoring themselves lowest. The average experience in using interactive visualization among our participants was 5.92. The expertise in carrying out exhaustive search tasks showed an average of 5.46. The performance of the trained classifier and the level of experience in machine learning are not correlated, which we see as an indication that our methods can be used by non-machine learning experts successfully.  Results from the NASA TLX questions indicate that the combination of the User-Driven Method with the 20ng corpus was the most frustrating task for the participants. This is certainly due to the complexity of the 20ng labeling task. Here, it was more difficult to select the documents for labeling as compared to the automatic AL selection process. Additionally, the participants put higher trust in the classifiers for the RCV1 task created with the User-Driven Method than in the ones created with the other two methods. The level of trust in the classifiers created for the 20ng task was much lower than the one in the classifiers for RCV1. For the RCV1 task, which was easier with respect to the composition of the relevant class, the User-Driven Method was the one that put the least workload on the participants. Most of the participants stopped training because the time limit was reached. The Main View was rated as being most beneficial for the task, followed by the Content View. Some users found the Term Weights View quite helpful, while others hardly ever used it. The same is true for the Term Lens. The positive effect in speeding up the creation of a high-quality classifier by applying AL can be clearly perceived from all diagrams with the exception of the diagram for the 20ng task with the User- Driven Method in <ref type="figure" coords="8,90.01,602.68,20.26,8.02" target="#fig_2">Fig. 6</ref> . We can also report that it is possible to effectively train an lSVM with all three methods producing comparable results. The best results for each method are as follows: All performance values are measured in F 1 , which is the harmonic mean of precision and recall: F 1 = 2·precision·recall precision+recall . The all docs column specifies the performance of a classifier trained on all gold labeled documents of the training set (the dashed lines of <ref type="figure" coords="8,202.84,716.50,19.88,8.02" target="#fig_2">Fig. 6</ref>). Classic AL and Method 3 learn slower on the 20ng task compared to RCV1, which is due to the greater diversity of the relevant class in the 20ng corpus. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>i) </head><p>The evaluation indicates that methods with less degrees of freedom in labeling, are much safer against labeling actions that hurt the trained classifier's performance severely. Examples for such disadvantageous labeling actions can be seen in <ref type="figure" coords="8,295.23,444.09,21.82,8.02" target="#fig_2">Fig. 6</ref>(20ng, user-driven). Here, user 1, depicted in red, labeled vast amounts of the negatively classified documents as positive in one training iteration. This resulted in a strongly skewed classifier. Unfortunately , the undo functionality was not used afterwards, but other mass labeling actions were performed in order to 'repair' the classifier. This led to over 2,000 labeling actions and a slight improvement ending with an F 1 score of 0.42. User 9 made selections mainly using the Term Weights View without much further refinement. This resulted in many incorrectly labeled instances and thus in a bad classifier performance . Since the number of labeling actions in these two cases was very high, we cut most of them in favor of preserving details of the other participants' actions. The affiliation of the test persons as well as their expertise in computer science/visualization may raise questions regarding their representativeness . However, in addition to their experience in the field of visualization, they matched the scheme of being specialists in their field. Our exercise task for finding VisWeek abstracts describing work that involves text analysis or visualization of text documents, has been chosen accordingly. It still became obvious through the participants' questions and loudly spoken thoughts that not all of them were able to develop a meaningful interaction workflow for training a classifier with the the User-Driven Method. We observed that some participants were still learning and exploring certain aspects of the tools, while they were carrying out the tasks for Methods 2 and 3. This is one potential reason for the two quite unsuccessful attempts in training the lSVM with the User-Driven Method. These findings indicate that the learning curve for successfully exploiting this method is high. The relatively small numbers of labeled instances of the Visual Method (second column of <ref type="figure" coords="8,387.77,726.46,20.57,8.02" target="#fig_2">Fig. 6</ref>). is primarily caused by the time needed to compute the next PCA projection of our SVM view. Since the process for the Visual Method is similar to that of the Basic Method – label the document suggested by AL, then perform the next training step automatically – the PCA projection is performed after each newly labeled example. The time needed for inspecting the classifier's state in between labeling actions adds on to this time. The effect is considerably larger than the delay occurring when performing a training step in the User-Driven Method after multiple examples have been labeled. Taking the results from the quantitative evaluation, Method 1 outperforms Method 2 and 3 with respect to efficiency. However, this comes at the cost that users can not judge the quality of the produced classifier very well, which was one aspect participants complained about most when using the Basic Method. A solution to this problem could be provided by creating a setup that uses Method 1 and provides an update on the visual interface only on demand or after a certain amount of label/training iterations. Further comments indicate that inspecting a considerable number of documents sequentially as required with the Basic Method is tiring and the participants are getting bored quickly. Two of them even reported physical load in the TLX questions and mentioned eyestrain and weariness in the later discussions. Getting visual feedback is preferred by most of the participants. In summary, most participants achieved good classification performance . Since almost all of them used the whole 15 minutes for their labeling tasks, the number of labeled instances in the diagrams shows that participants were faster by tendency using the Basic Method. The influence of a task's difficulty on the amount of labeled instances can be seen in <ref type="figure" coords="9,71.43,313.40,20.42,8.02" target="#fig_2">Fig. 6</ref>, when comparing the results of the RCV1 task and the 20ng task. From our observation, it is likely that the time foreseen for the tutorial was insufficient for mastering the complexity of the User-Driven Method. This has an additional negative influence on the number of labeling actions. Better user training would most probably diminish this influence. Despite the initial need for more user training when using Method 3 we think that there are situations in which the User-Driven Method should be employed. In our evaluation, users only had to deal with relatively 'good-natured' initial classifiers. Depending on the task and predefined query this cannot be guaranteed in general. If many documents are incorrectly labeled due to the initial query, it can be difficult to create a good classifier without relabeling these documents. Relabeling documents, however, is not foreseen in 'classic' AL approaches as applied in Method 1 and 2. In the context of dynamic data sources such as blogs and forums, visual control of the classifier performance is important. Otherwise, new relevant subtopics that were not encountered during classifier training are not detected and the need for retraining the classifier cannot be recognized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p> We presented an approach for building classifiers interactively and visually in order to complement classical search and filter techniques, which often show weaknesses regarding recall and generalization. A first user evaluation indicated that our approach is effective with all three tested methods. We see our main contribution in suggesting the described approach as a whole, starting from bootstrapping an initial classifier using a keyword query, to classifier training employing active learning methods, and its future re-integration into keyword queries. One of our hopes is that the design of the described evaluation further fuels the discussion on employing baselines for comparative test procedures for visual interactive classifier creation. The integration of classifiers into keyword queries raises exciting research questions. How can classifiers be combined with Boolean and vector search backends? How does their position or sequence within classical search query structures influence precision and recall? How can analysts be supported in employing classifiers successfully in such a context? Despite the negative outliers in our evaluation, we see great potential in additional research on how classifiers can be trained in a visual and interactive manner to improve recall and generalization for search and filter tasks in visual analytics scenarios. Future work will therefore include a broader evaluation of the presented approach regarding the number of users and tasks. Generalization and high performance are two remarkable characteristics of linear support vector machines, making them an interesting candidate for live monitoring of dynamic text data, such as social network data for example, besides using it in post analysis scenarios as in this work. We will also apply the approach to tasks different from searching/classifying text documents and will experiment with other classifiers than lSVMs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,33.49,699.32,244.39,6.86;1,31.50,708.78,144.33,6.86"><head></head><figDesc>Manuscript received 31 March 2012; accepted 1 August 2012; posted online 14 October 2012; mailed on 5 October 2012. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="8,22.50,371.27,410.99,8.22"><head>Fig. 6. </head><figDesc>Fig. 6. Evaluation results as F 1 scores over labeled instances for datasets i) 20ng and ii) RCV1 and for each method. </figDesc></figure>

			<note place="foot" n="3"> THREE METHODS FOR CLASSIFIER CREATION This section introduces three methods that let users create classifiers by labeling instances, thus assembling the classifier&apos;s training set. As opposed to traditional AL the labeling is not performed by an annotator , who remains uninformed about the training procedure, but by an analyst, who wants to observe the training progress of the classifier . Hence, in all of our approaches analysts receive different forms of feedback about the current state of the classifier, putting them in a position to decide which documents to label next, and when to stop labeling. Giving this ability to users is vital for our usage scenario of text document classifier creation. For each of the classifier building methods, we present a corresponding software prototype implementing it. In each of them, the initial state of the classifier is bootstrapped by executing a text search</note>

			<note place="foot" n="1"> http://lucene.apache.org 2 during our evaluation we used the L2-regularized L2-loss dual solver with C = 1, and ε = 0.01</note>

			<note place="foot" n="3"> The corpus itself can be downloaded freely on the web, e.g. at http: //people.csail.mit.edu/jrennie/20Newsgroups/</note>

			<note place="foot" n="4"> In the literature such diagrams are known as learning curves. To avoid confusion, we will term such curves evolution curves in the context of classifiers instead, because we use the term learning curve in context of users&apos; ability to familiarize themselves with a task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p>We would like to thank the German Science Foundation (DFG) for funding this work as part of the priority program (SPP) 1335 'Scalable Visual Analytics' and the European Commission for funding it as part of the FP 7 project PESCaDO. Additionally, we owe special thanks to our SPP project partners at the Institute for Natural Language Processing of the Universität Stuttgart (IMS), Charles Jochim, Florian Laws, and Hinrich Schütze, for sharing their expertise and for fruitful discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,312.38,254.41,232.12,7.13;9,312.38,263.87,232.12,7.13;9,312.38,273.34,232.12,7.13;9,312.38,282.80,33.87,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Visual classification: An interactive approach to decision tree construction</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ankerst</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Elsen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ester</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-P</forename>
				<surname>Kriegel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. Conf. on Knowledge Discovery and Data Mining, KDD&apos;99</title>
		<meeting>. 5th Int. Conf. on Knowledge Discovery and Data Mining, KDD&apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,292.26,232.12,7.13;9,312.38,301.73,232.12,7.13;9,312.38,311.19,232.12,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards an effective cooperation of the user and the computer for classification</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ankerst</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ester</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H.-P</forename>
				<surname>Kriegel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>. 6th Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="0" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,320.66,232.12,7.13;9,312.38,330.12,232.12,7.13;9,312.38,339.59,232.12,7.13;9,312.38,349.05,59.68,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">Swapit: a multiple views paradigm for exploring associations of texts and structured data</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Becks</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Seeling</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Working Conf. on Advanced Visual Interfaces, AVI &apos;04</title>
		<meeting>. of the Working Conf. on Advanced Visual Interfaces, AVI &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="193" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,358.52,232.12,7.13;9,312.38,367.98,232.12,7.13;9,312.38,377.53,232.12,6.86;9,312.38,386.91,123.68,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Surveying the complementary role of automatic data analysis and visualization in knowledge discovery</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Bertini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lalanne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery , VAKD&apos;09</title>
		<meeting>. of the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discovery , VAKD&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,396.37,232.12,7.13;9,312.38,405.84,204.17,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">J</forename>
				<surname>Burges</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="121" to="167" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,415.30,232.12,7.13;9,312.38,424.77,232.12,7.13;9,312.38,434.23,103.75,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Query learning with large margin classifiers</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Campbell</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cristianini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Smola</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Int. Conf. on Machine Learning, ICML&apos;00</title>
		<meeting>. of the 17th Int. Conf. on Machine Learning, ICML&apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,443.70,232.12,7.13;9,312.38,453.16,232.12,7.13;9,312.38,462.63,221.53,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Bead: Explorations in information visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chalmers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Chitson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th annual int. conf. on research and development in information retrieval, ACM SIGIR&apos;92</title>
		<meeting>. of the 15th annual int. conf. on research and development in information retrieval, ACM SIGIR&apos;92</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,472.09,232.12,7.13;9,312.38,481.56,232.12,7.13;9,312.38,491.02,62.31,7.13"  xml:id="b7">
	<monogr>
		<title level="m" type="main">An Introduction to Support Vector Machines and Other Kernel-based Learning Methods</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cristianini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Shawe-Taylor</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,500.48,232.12,7.13;9,312.38,509.95,232.12,7.13;9,312.38,519.41,176.88,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive learning using manifold geometry</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Eaton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Holness</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Mcfarlane</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Fall Symposium on Manifold Learning and Its Applications, AAAI&apos;09</title>
		<meeting>. of the AAAI Fall Symposium on Manifold Learning and Its Applications, AAAI&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,528.88,232.12,7.13;9,312.38,538.34,232.12,7.13;9,312.38,547.81,232.12,7.13;9,312.38,557.27,104.30,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Observation-level interaction with statistical models for visual analytics</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Endert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Han</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Maiti</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>House</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Leman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>North</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Visual Analytics Science and Technology</title>
		<meeting>. of the Conf. on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,566.74,232.12,7.13;9,312.38,576.20,232.12,7.13;9,312.38,585.66,17.93,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Interactive machine learning</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Fails</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">R</forename>
				<surname>Olsen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Jr</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 8th Int. Conf. on Intelligent User Interfaces, IUI&apos;03</title>
		<meeting>. of the 8th Int. Conf. on Intelligent User Interfaces, IUI&apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,595.13,232.12,7.13;9,312.38,604.59,232.12,7.13;9,312.38,614.06,128.43,7.13"  xml:id="b11">
	<analytic>
		<title level="a" type="main">LIB- LINEAR: A library for large linear classification</title>
		<author>
			<persName>
				<forename type="first">R.-E</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-W</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-J</forename>
				<surname>Hsieh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X.-R</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C.-J</forename>
				<surname>Lin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,623.52,232.12,7.13;9,312.38,632.99,232.12,7.13;9,312.38,642.45,156.95,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Cueflik: interactive concept learning in image search</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fogarty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Tan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kapoor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Winder</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th Conf. on Human Factors in Computing Systems, CHI&apos;08</title>
		<meeting>. of the 26th Conf. on Human Factors in Computing Systems, CHI&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,651.92,232.12,7.13;9,312.38,661.38,232.12,7.13;9,312.38,670.85,232.12,7.13;9,312.38,680.31,17.93,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">G</forename>
				<surname>Hart</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">E</forename>
				<surname>Stavenland</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Mental Workload</title>
		<editor>P. A. Hancock and N. Meshkati</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,312.38,689.77,232.12,7.13;9,312.38,699.24,44.06,7.13"  xml:id="b14">
	<monogr>
		<title level="m" type="main">Search User Interfaces</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Hearst</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>1st. edition</note>
</biblStruct>

<biblStruct coords="9,312.38,708.70,232.12,7.13;9,312.38,718.17,232.12,7.13;9,312.38,727.63,232.12,7.13;9,312.38,737.10,17.93,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Inter-active learning of ad-hoc classifiers for video visual analytics</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Netzel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Höferlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Weiskopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heidemann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Visual Analytics Science and Technology</title>
		<meeting>. of the Conf. on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,54.06,232.12,7.13;10,40.76,63.52,232.12,7.13;10,40.76,72.99,137.08,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Joachims</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Machine Learning, ECML&apos;98</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,82.45,232.12,7.13;10,40.76,91.92,232.13,7.13;10,40.76,101.38,49.81,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Joachims</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning, ICML&apos;99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,110.85,232.12,7.13;10,40.76,120.31,232.12,7.13;10,40.76,129.78,154.30,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Bootstrapping for text learning tasks</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Jones</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mccallum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Nigam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Riloff</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Text Mining: Foundations, Techniques and Applications, IJCAI&apos;99</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="52" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,139.24,232.12,7.13;10,40.76,148.70,232.12,7.13;10,40.76,158.17,48.92,7.13"  xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ellis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mastering The Information Age: Solving Problems with Visual Analytics. Eurographics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,167.63,232.12,7.13;10,40.76,177.10,198.93,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Newsweeder: Learning to filter netnews</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Lang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Int. Machine Learning Conf., ICML&apos;95</title>
		<meeting>. of the 12th Int. Machine Learning Conf., ICML&apos;95</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,186.56,232.12,7.13;10,40.76,196.03,232.12,7.13;10,40.76,205.49,166.57,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lewis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Gale</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Int. Conf. on Research and Development in Information Retrieval, SIGIR&apos;94</title>
		<meeting>. of the 17th Int. Conf. on Research and Development in Information Retrieval, SIGIR&apos;94</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,214.96,232.12,7.13;10,40.76,224.42,232.12,7.13;10,40.76,233.88,83.02,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Design and evaluation of visualization support to facilitate decision trees classification</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Salvendy</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,243.35,232.12,7.13;10,40.76,252.81,145.44,7.13"  xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Manning</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Raghavan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Schütze</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,262.28,232.12,7.13;10,40.76,271.74,232.12,7.13;10,40.76,281.21,125.67,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards closing the analysis gap: Visual generation of decision supporting schemes from raw data</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>May</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="911" to="918" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,290.67,232.12,7.13;10,40.76,300.14,232.12,7.13;10,40.76,309.68,232.12,6.86;10,40.76,319.07,139.61,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient annotation of image data sets for computer vision applications</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Moehrmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Heidemann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st Int. Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications , VIGTA&apos;12</title>
		<meeting>. of the 1st Int. Workshop on Visual Interfaces for Ground Truth Collection in Computer Vision Applications , VIGTA&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,328.53,232.12,7.13;10,40.76,337.99,232.12,7.13;10,40.76,347.46,80.58,7.13"  xml:id="b26">
	<monogr>
		<title level="m" type="main">A literature survey of active machine learning in the context of natural language processing</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Olsson</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,356.92,232.12,7.13;10,40.76,366.39,232.12,7.13;10,40.76,375.85,232.12,7.13;10,40.76,385.32,187.62,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Least square projection: A fast high-precision multidimensional projection technique and its application to document mapping</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">V</forename>
				<surname>Paulovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">G</forename>
				<surname>Nonato</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Minghim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Levkowitz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,394.78,232.12,7.13;10,40.76,404.25,232.12,7.13;10,40.76,413.71,232.12,7.13;10,40.76,423.17,187.65,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">The projection explorer: A flexible tool for projection-based multidimensional visualization</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">V</forename>
				<surname>Paulovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">C F</forename>
				<surname>Oliveira</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Minghim</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th Brazilian Symposium on Computer Graphics and Image Processing, SIBGRAPI&apos;07</title>
		<meeting>. of the 20th Brazilian Symposium on Computer Graphics and Image essing, SIBGRAPI&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,432.64,232.12,7.13;10,40.76,442.10,232.12,7.13;10,40.76,451.57,174.91,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pirolli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Card</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Intelligence Analysis</title>
		<meeting>. of Int. Conf. on Intelligence Analysis</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="2" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,461.03,232.12,7.13;10,40.76,470.50,232.12,7.13;10,40.76,479.96,17.93,7.13"  xml:id="b30">
	<monogr>
		<title level="m" type="main">Visual data mining. chapter Towards Effective Visual Data Mining with Cooperative Approaches</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Poulet</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="389" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,489.43,232.12,7.13;10,40.76,498.89,232.12,7.13;10,40.76,508.36,232.12,7.13;10,40.76,517.82,41.84,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">The reuters corpus volume 1 from yesterday&apos;s news to tomorrow&apos;s language resources</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Rose</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Stevenson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Whitehead</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Int. Conf. on Language Resources and Evaluation, LREC&apos;02</title>
		<meeting>. of the 3rd Int. Conf. on Language Resources and Evaluation, LREC&apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="29" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,527.28,232.12,7.13;10,40.76,536.75,232.12,7.13;10,40.76,546.21,33.87,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">A survey on the use of relevance feedback for information access systems</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Ruthven</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Lalmas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="145" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,555.68,232.12,7.13;10,40.76,565.14,232.12,7.13;10,40.76,574.61,110.79,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">An insight-based longitudinal study of visual analytics</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Saraiya</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>North</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Lam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Duca</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1511" to="1522" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,584.07,232.12,7.13;10,40.76,593.54,232.12,7.13;10,40.76,603.00,103.75,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Less is more: Active learning with support vector machines</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Schohn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Cohn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 17th Int. Conf. on Machine Learning, ICML&apos;00</title>
		<meeting>. of the 17th Int. Conf. on Machine Learning, ICML&apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,612.47,232.12,7.13;10,40.76,621.93,113.99,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Sebastiani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,631.39,232.12,7.13;10,40.76,640.86,196.43,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">User-based active learning</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Seifert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Granitzer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Data Mining, Workshops, ICDMW&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="418" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,650.32,232.12,7.13;10,40.76,659.79,232.12,7.13;10,40.76,669.25,161.48,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Classifier hypothesis generation using visual analysis methods</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Seifert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Sabol</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Granitzer</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second Int. Conf. on Networked Digital Technologies</title>
		<meeting>. of the Second Int. Conf. on Networked Digital Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="98" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,678.72,232.12,7.13;10,40.76,688.18,185.98,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">Active learning literature survey</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Settles</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Sciences</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,697.65,232.12,7.13;10,40.76,707.11,232.12,7.13;10,40.76,716.57,232.12,7.13;10,40.76,726.04,57.78,7.13"  xml:id="b39">
	<analytic>
		<title level="a" type="main">Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Settles</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Empirical Methods in Natural Language Processing</title>
		<meeting>. of the Conf. on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1467" to="1478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,40.76,735.50,232.12,7.13;10,303.38,54.06,232.12,7.13;10,303.38,63.52,33.87,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Jigsaw: supporting investigative analysis through interactive visualization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Görg</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="132" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,72.99,232.12,7.13;10,303.38,82.45,232.12,7.13;10,303.38,91.92,111.48,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">Guided text analysis using adaptive visual analytics</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Steed</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">D</forename>
				<surname>Symonsa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">A</forename>
				<surname>Denap</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">E</forename>
				<surname>Potok</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conf. on Visualization and Data Analysis</title>
		<meeting>. of Conf. on Visualization and Data Analysis</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,101.38,232.12,7.13;10,303.38,110.85,110.78,7.13"  xml:id="b42">
	<analytic>
		<title level="a" type="main">Challenges for visual analytics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kielman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Visualization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,120.31,232.12,7.13;10,303.38,129.78,232.12,7.13;10,303.38,139.24,95.70,7.13"  xml:id="b43">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Koller</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,148.70,232.12,7.13;10,303.38,158.17,232.12,7.13;10,303.38,167.63,184.82,7.13"  xml:id="b44">
	<analytic>
		<title level="a" type="main">Baobabview: Interactive construction and analysis of decision trees</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Van Den Elzen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conf. on Visual Analytics Science and Technology</title>
		<meeting>. of the Conf. on Visual Analytics Science and Technology</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,177.10,164.70,7.13"  xml:id="b45">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Vapnik</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,186.56,232.12,7.13;10,303.38,196.03,232.12,7.13;10,303.38,205.49,130.17,7.13"  xml:id="b46">
	<analytic>
		<title level="a" type="main">Interactive machine learning: letting users build classifiers</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Ware</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Frank</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Holmes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Hall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">H</forename>
				<surname>Witten</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal on Human- Computer Studies</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,303.38,214.96,232.12,7.13;10,303.38,224.42,232.12,7.13;10,303.38,233.88,232.12,7.13;10,303.38,243.35,79.96,7.13"  xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">C</forename>
				<surname>Wong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Hetzler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Posse</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Whiting</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Havre</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Cramer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Shah</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Singhal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Turner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Symposium on Information Visualization , INFOVIS&apos;04</title>
		<meeting>. of the IEEE Symposium on Information Visualization , INFOVIS&apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
