<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Deshredder: A Visual Analytic Approach to Reconstructing Shredded Documents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Patrick</forename>
								<surname>Butler</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science and Discovery Analytics Center</orgName>
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<postCode>24061</postCode>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Prithwish</forename>
								<surname>Chakraborty</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science and Discovery Analytics Center</orgName>
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<postCode>24061</postCode>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Naren</forename>
								<surname>Ramakrishan</surname>
							</persName>
							<affiliation>
								<orgName type="department">Department of Computer Science and Discovery Analytics Center</orgName>
								<orgName type="institution">Virginia Tech</orgName>
								<address>
									<postCode>24061</postCode>
									<settlement>Blacksburg</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Deshredder: A Visual Analytic Approach to Reconstructing Shredded Documents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: I75 [Document and Text Processing]: Document Capture—Graphics recognition and interpretation</term>
					<term>I47 [Image Pro- cessing and Computer Vistion]: Feature Representation—</term>
					<term>H5 [In- formation Interfaces and Presentation]: User Interfaces—Graphical *</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Figure 1: Overview of Deshredder. (Left) original document; (Middle) Document shreds to be reconstructed and preview of matchings identified by Deshredder; (Right) image reconstructed from the shredded pieces. Some parts of the figure courtesy of [1]. ABSTRACT Reconstruction of shredded documents remains a significant challenge. Creating a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries&apos; attempts to gain access to information. Existing approaches to reconstructing shredded documents adopt either a predominantly manual (e.g., crowd-sourcing) or a near automatic approach. We describe Deshredder, a visual analytic approach that scales well and effectively incorporates user input to direct the reconstruction process. Deshredder represents shredded pieces as time series and uses nearest neighbor matching techniques that enable matching both the contours of shredded pieces as well as the content of shreds themselves. More importantly, Deshred-der&apos;s interface support visual analytics through user interaction with similarity matrices as well as higher level assembly through more complex stitching functions. We identify a functional task taxonomy leading to design considerations for constructing deshredding solutions , and describe how Deshredder applies to problems from the DARPA Shredder Challenge through expert evaluations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p> Putting back shredded pieces of documents into a whole is an important problem in many domains, such as intelligence, security informatics , health informatics, and insurance claim analysis. Engineering a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries' attempts to gain access to information . Because significant domain knowledge can be exploited in the reconstruction process, human input is crucial. At the same time, the scale at which shreds have to be processed necessitates automated algorithmic support. It is hence natural to view deshredding as a visual analytics problem where human judgment and automated algorithmic assistance can be fruitfully combined. However, designing a semi-automatic deshredding system is quite a challenge. A range of research problems manifest, from the need to address imperfections caused by the actual shredding process, to designing best algorithmic strategies for matching shreds, to supporting novel problem solving strategies for human users. The span of backgrounds from which techniques have to be drawn is also broad: image matching, computer vision, and picture reconstruction, to name a few. Here, we present a systematic pipeline of processing stages that address practical issues in analyzing shredded pieces and also provides significant leverage to users in directing the reconstruction process. We present Deshredder, a visual analytic framework for reconstructing shredded documents. Our specific contributions are: 1. A novel time series approach to represent shreds that enable matching both the contours of shredded pieces as well as the content of shreds themselves. </p><p> 2. Interfaces that support both matching shreds through interaction with similarity matrices as well as higher level assembly through more complex stitching functions. 3. Evaluations with two expert users that successfully illustrate both the capabilities of Deshredder and also reveal different problem solving strategies of users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND DESIGN CONSIDERATIONS 2.1 DARPA Shredder Challenge</head><p>The DARPA Shredder Challenge <ref type="bibr" coords="2,180.06,146.16,10.46,8.12" target="#b0">[1] </ref> can be credited with the recent spurt of interest in the deshredding problem. This was a contest where participants were required to re-assemble five sets of shredded documents, with increasing levels of difficulty, and answer specific questions that pertain to the contents of the documents. Different teams adopted widely varying strategies and it is instructive to review these strategies. Interaction with these winning teams enabled us to understand the strategies of those that were placed first, second, third, fifth, and sixth (strategies of the fourth wasn't available.) The winning solution ('All Your Shreds Are Belong To U.S.') used a custom-coded computer vision algorithm that suggested fragment pairings to human assemblers for verification <ref type="bibr" coords="2,216.07,250.26,9.52,8.12" target="#b0">[1]</ref> . Among other techniques , the team employed a scoring mechanism to evaluate their matching algorithm <ref type="bibr" coords="2,126.83,269.20,13.75,8.12">[10]</ref>. Also, the team found repeating patterns of yellow dots which, according to the team, were a characteristic of the problem set and used the dots to guide the shred matching stage. Similar ideas were adopted by the second ('Shroddon'), third ('wasabi') and fifth ('mmbd') placed teams. The 'Shroddon' team also noted that the shredders can shear the papers in depth, and used this information in their final solution <ref type="bibr" coords="2,198.75,325.98,13.75,8.12">[10]</ref>. In addition the team searched for the most probable character that can follow the set of characters recognized by the human eye, during the process of shred assembly, through dictionary search. The 'wasabi' team <ref type="bibr" coords="2,266.29,354.37,14.94,8.12" target="#b18">[20] </ref>employed a number of computer vision algorithms such as image rotations , trimming, and computed similarity matrices between shreds based on a few simplifying assumptions. These matrices were based on line connections, edge similarity, color similarity, and pen connections . For the initial puzzle sets, the pieces were assembled using picture editing software (e.g., gimp). However, according to <ref type="bibr" coords="2,276.76,411.16,14.94,8.12" target="#b18">[20] </ref>a custom assembly tool was used for the latter problems. An interactive query based system was used by the 'mmbd' team <ref type="bibr" coords="2,74.27,439.56,13.75,8.12" target="#b12">[14]</ref>. According to their public webpage <ref type="bibr" coords="2,225.76,439.56,13.75,8.12" target="#b12">[14]</ref>, this team used features such as line connectivity and stroke connectivity as the basis to connect shreds. A Google Drawing powered interactive framework was next used where best possible matches for the pieces were presented for perusal and selection by the user. The sixth team ('UCSD') adopted a crowdsourcing approach <ref type="bibr" coords="2,287.98,486.88,10.46,8.12" target="#b6">[7] </ref>that was effective at solving the first few problems. This team was plagued by player fraudulence and the designers had to implement a new security feature to qualify users. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Commercial Tools</head><p>The field of commercially available tools to address the deshredding problem is surprisingly sparse. To the best of our knowledge, only one system exists, viz. Unshredder <ref type="bibr" coords="2,188.33,563.16,13.75,8.12" target="#b16">[18]</ref>. Unshredder is primarily targeted at traditional shredding, while our approach is applicable to cross-cut shredding (traditional shredding cuts in one direction, whereas cross-cut shredding yields parallelogram-shaped pieces). From the limited information available online, this approach makes most of its matches using computer vision algorithms, offering limited opportunities for user interaction. However, multiple users have the ability to collaborate in solving a deshredding problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Functional Task Taxonomy</head><p>Before we introduce our Deshredder approach, it is helpful to cast the above deshredding solutions along an axis that emphasizes the 'mixing' between automated analysis and human/visual input: @BULLET Near-automated solutions: Unshredder. @BULLET Backend analysis with visual front-ends: 'All Your Shreds Are Belong To U.S.', 'wasabi' and 'mmbd'. </p><p>@BULLET Visual analytic frameworks: Deshredder. @BULLET Predominantly manual (e.g., crowdsourcing) approaches: 'UCSD'. </p><p> In our visual analytic approach, we emphasize automated algorithms for shred matching, the interactivity enabled by users to critique matches, and most importantly the ability to incorporate user feedback to improve the reconstruction process. It is this closed loop framework that sets Deshredder apart from the other solutions above. A functional taxonomy of tasks that must be supported by a visual analytic solution involves four aspects as shown in <ref type="figure" coords="2,527.77,146.98,26.87,8.12" target="#tab_1">Table 1</ref>. Shreds can be represented in numerous ways that emphasize the features crucial for effective matching. Deshredder uses a time series representation that aids in identifying matches quickly with low underlying computational complexity. Second, we must decide on the matching algorithm for comparing shreds and identifying nearest neighbors; here we show how the use of the Chamfer distance measure enables us to expressively take multiple factors into account, including brightness, physical shape, and color. More importantly, it enables the ready incorporation of user feedback. Third, the strategy for shred assembly can be automated to different levels. Here we demonstrate how the use of a similarity matrix visualization enables a truly visual analytic approach that can incorporate user feedback continually in the process to identify and discard bad matches. This enables the user to follow different strategies of interest in reconstructing documents, e.g., 'go for the low hanging fruit first' (i.e., identify easy matches and rule out other fragments based on these matches) or a 'toddler approach' (i.e., identify random matches across the board and then piece them together). Finally, higher-level functions are crucial as the number of shreds increases. We demonstrate the use of features such as constraint propagation, and state capture and reuse, to enable the creation of more complex problem solving strategies. A comparison of Deshredder with other systems alongside some system features is given in <ref type="figure" coords="2,410.57,374.12,25.30,8.12" target="#tab_2">Table 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p> The design of Deshredder draws upon work from many related backgrounds , which we survey below. Computer Vision: Stitching images together based on overlapping regions, i.e., panoramic stitching, is a well studied problem in the field of computer vision <ref type="bibr" coords="2,408.92,600.26,14.19,8.12">[21,</ref><ref type="bibr" coords="2,426.36,600.26,6.72,8.12" target="#b7"> 8,</ref><ref type="bibr" coords="2,436.31,600.26,6.47,8.12" target="#b2"> 3]</ref>. One of the seminal works in this field is attributed to Lowe <ref type="bibr" coords="2,421.94,609.72,13.75,8.12" target="#b15">[17]</ref>. This paper proposed SIFT (scale invariant feature transform), an algorithm to detect and describe local features of images, which has become one of the mainstays of major image stitching algorithms. SIFT has been used by Brown et al. <ref type="bibr" coords="2,325.28,647.58,10.46,8.12" target="#b5">[6] </ref>for automatic panoramic stitching. In 2006, Ward <ref type="bibr" coords="2,525.63,647.58,14.94,8.12" target="#b29">[32] </ref>presented a HDR-photography stitching algorithm and in 2010 Xiong et al. <ref type="bibr" coords="2,325.26,666.51,14.94,8.12" target="#b32">[35] </ref>presented a mobile and faster panoramic stitching method. A good review of the major stitching algorithms can be found in Woeste <ref type="bibr" coords="2,342.09,685.44,13.75,8.12" target="#b30">[33]</ref>. A key lesson from these works is the choice of feature representation and their capabilities for supporting rapid and accurate stitching. In our work, significant knowledge exists in the detailed shapes and content of pieces which are often non-overlapping  and hence we focus on capturing them in a suitable time series representation . There have been several preliminary attempts at academic solutions to document reconstruction, however they either deal with large pieces <ref type="bibr" coords="3,99.51,307.23,9.71,8.12" target="#b8">[9,</ref><ref type="bibr" coords="3,111.01,307.23,11.21,8.12" target="#b22"> 25,</ref><ref type="bibr" coords="3,123.99,307.23,10.65,8.12" target="#b26"> 29]</ref>, non-cross cut shreds <ref type="bibr" coords="3,214.13,307.23,13.75,8.12" target="#b17">[19]</ref> , or they do not provide user guided iterative machine learning techniques <ref type="bibr" coords="3,256.12,316.70,13.75,8.12" target="#b20">[23]</ref>. Some other approaches can be found in <ref type="bibr" coords="3,175.01,326.16,14.20,8.12" target="#b13">[15,</ref><ref type="bibr" coords="3,191.45,326.16,11.21,8.12" target="#b19"> 22,</ref><ref type="bibr" coords="3,204.89,326.16,11.21,8.12" target="#b23"> 26,</ref><ref type="bibr" coords="3,218.34,326.16,11.21,8.12" target="#b25"> 28,</ref><ref type="bibr" coords="3,231.80,326.16,11.95,8.12" target="#b27"> 30] </ref> Motif Mining and Time Series Modeling: Concurrent to advancements in image processing tools, there have been extensive research in representing images through 1-dimensional time series and mining motifs from these time series in order to match image objects <ref type="bibr" coords="3,272.59,364.02,9.71,8.12" target="#b3">[4,</ref><ref type="bibr" coords="3,284.25,364.02,10.65,8.12" target="#b9"> 11]</ref>. These methods work with image boundaries in general and hence do not require overlapping regions to compare two images. Keogh et al <ref type="bibr" coords="3,63.33,392.41,14.94,8.12" target="#b14">[16] </ref>in 2006 detailed the process of converting a 2-D shape into a time series and matching two shapes based on the motifs discovered in these time series, allowing for rotations. Yankow et al <ref type="bibr" coords="3,283.50,411.34,14.94,8.12" target="#b33">[36] </ref>and Xi et al. <ref type="bibr" coords="3,104.23,420.81,14.94,8.12" target="#b31">[34] </ref>extend this framework further to more complex scenarios of matching and similarity search. In 2007, Yankow et al. <ref type="bibr" coords="3,65.92,439.74,14.94,8.12" target="#b34">[37] </ref>presented a uniform scaling approach to match differently scaled shapes. Rakthanmanon et al. <ref type="bibr" coords="3,183.58,449.20,14.94,8.12" target="#b21">[24] </ref> matched near duplicate figures found in historical documents using a time series approach. Our work is motivated by the above techniques but, as we will show, we require significantly specialized pipelines to accommodate shredded documents. Similarity matrix interaction and visualization: Similarity matrices are the underlying basis for many data mining and visual analytic algorithms, e.g., clustering <ref type="bibr" coords="3,153.39,515.45,13.75,8.12" target="#b11">[13]</ref>. There is significant work on matrix visualization and interaction, in general (e.g., <ref type="bibr" coords="3,226.19,524.91,14.34,8.12" target="#b24">[27]</ref>) and similarity matrix visualization, in particular (e.g., <ref type="bibr" coords="3,199.32,534.38,13.45,8.12" target="#b28">[31]</ref>). Much of these works are focused around problems like social network analysis, bioinformatics , and network traffic. Here, we demonstrate the use of similarity matrix visualization and interaction as a primary mechanism for users to understand the landscape of possible shred matches and how they can systematically prune this space to identify key shreds that can (or cannot) form important segments in the reconstructed image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OVERVIEW</head><p>We introduce Deshredder by identifying three of its salient themes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representing Shapes as Time Series</head><p>The first step in Deshredder is to process the individual shreds and extract two time series from each of these shreds. Although it might appear unconventional to represent fragments as time series, as introduced in the related work section there is precedence for such a representation and it provides significant benefits in matching as we shall see. The two extracted time series correspond to the left and right side of the shreds, respectively, and closely follows the relevant contours of the raw shreds as shown in <ref type="figure" coords="3,458.32,63.41,29.48,8.12" target="#fig_0">Figure 2</ref>. Using a perfectly vertical line as reference, the time series captures the distance of the shred boundary from the vertical line. Subsequently, the time series are used to compute different similarity metrics between shreds which in turn form the basis of the assembly process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Visualizing and Interacting with Similarity Matrices</head><p>One of the basic interfaces available in Deshredder is the similarity matrix interaction capability (e.g., see <ref type="figure" coords="3,452.68,354.30,21.15,8.12" target="#fig_2">Fig. 4</ref>(A)), which provides a high level overview of the best matches between all pairs of shreds (assuming all possible orientations). It enables the user to spot patterns that occur over large sets. One such useful pattern occurs with blank or nearly blank pieces, which are composed of primarily the background color and therefore yield relatively good matches with most other shreds. This can be seen by prominent rows and columns of blue pixels in <ref type="figure" coords="3,377.67,420.56,31.13,8.12" target="#fig_2">Figure 4</ref>. The user can use this view to choose a threshold ('Threshold' slider) of what equates to a good match, and then automatically discount any pieces that have too many good matches ('Max Positives' slider). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Constraint Propagation via User Interaction</head><p>Constraint propagation is an important tool for reducing the number of possible matches to the user. During the process of reconstructing documents in the physical world humans employ constraint propagation in order to reduce the number of choices remaining. Deshredder automatically employs several forms of constraint propagation. The first is enabled when a user matches all possible spaces along a single piece's side. Deshredder then knows not to show any further matches that include the 'used side' of that piece. Deshredder is also aware of orientation possibilities (see <ref type="figure" coords="4,168.92,72.88,20.27,8.12" target="#fig_1">Fig. 3</ref>). Once a match is identified, Deshredder will remove from consideration other candidate matches whose orientations do not align with the identified match(es). Furthermore if a user matches an unoriented piece with an oriented piece it propagates the orientation information to the unannotated piece, leading to a cascade of simplifications, that often results in a significant reduction to the number of matches that a user must consider. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Example workflow</head><p>An example of a typical work flow of a user is provided in <ref type="figure" coords="4,266.01,161.07,29.06,8.12" target="#fig_2">Figure 4</ref>. A user's attention typically focuses on only a few shreds. At the start, it is important to be able to quickly sort through the difficult or uninteresting pieces to find the easiest pieces to match – this is analogous to focusing first on the border pieces in a jigsaw puzzle. In step A of <ref type="figure" coords="4,102.16,208.39,21.76,8.12" target="#fig_2">Fig. 4</ref>the user is presented with all possible matches and a visualization of the similarity matrix with known constraints resulting from the chosen match algorithm. As mentioned before, one of the largest sources of false positives is due to the presence of blank pieces that are seemingly good matches to many other pieces. In Deshredder, the user can observe these pieces as creating long rows and columns of blue pixels in the visualization. The user sets the threshold slider and the max positives slider to first decide what makes a good match and then discounts any piece that matches with too many other pieces. In step B, we see the results of discounting these pieces in the similarity matrix, where invalid matches are marked in black and the corresponding matches shown in red. In step C, the user annotates the orientation of several pieces, which has the result of updating the constraints (notice the greater presence of black lines in the similarity matrix view). When the orientations of both pieces in a possible match are known, it allows us to remove two out of four possible matches between the two shreds (see <ref type="figure" coords="4,272.61,359.82,19.36,8.12" target="#fig_1">Fig. 3</ref>). (When the orientation as well as the match is confirmed, it allows us to remove three out of the four possible matches.) These prunings enable the removal of displayed matches that have incompatible orientations . this removes one of the matches displayed because the match represents two pieces with incompatible orientations. In step D, the user confirms two matches which involve the right side of the shred containing 'V'. This further constrains any matches that might aim to match that side of the shred, but not ones that use the left side of that shred. Furthermore, the orientation annotation is propagated to matched pieces, which again reduces the number of pieces under consideration. In step E, the user confirms two more matches which again propagate their orientations to other pieces, as well as serving to remove matches that overlap the confirmed matches. In step F, the user returns to the overview display to observe the fruit of his interactions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DESHREDDER IN DETAIL</head><p>The overall algorithmic pipeline of Deshredder is given in <ref type="figure" coords="4,265.98,534.00,29.10,8.12">Figure 5</ref>. Deshredder takes as input a single image containing all the shreds from the original document. Then using computer vision algorithms outlined below we separate each shred into a single image, and apply a straightening algorithm to guarantee that each shred has a known orientation. Next, we apply a feature matching technique to find the best matches between each pairwise set of shreds. The features can be based on shape, brightness, or a particular color. This information is then used in the Deshredder interface where a user is presented with the best of matches for a given shred, and visualization of the similarity to see overall progress and to help weed out unhelpful shreds. The user can then choose the correct matches and further eliminate possibilities by annotating the shreds with a correct orientation. Deshredder takes the verified matches and orientation information and performs constraint propagation to further eliminate potential matches. If the users wishes, at any time they can change the features to match on, either focusing on a color using the color selector or choosing shape, or brightness. These steps are repeated until the original document is reconstructed. More details are provided below. We now detail each of the stages below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Segmentation, Filtering, and Straightening</head><p>To begin the reassembly, the first step is to typically place all pieces face down on a scanner and then algorithmically separate them from <ref type="figure" coords="5,179.46,299.65,29.15,8.12">Figure 5</ref>: The pipeline of the Deshredder algorithm and user interface. the background. A noise filtering step accounts for many shred defects such as non-straight cuts, bleaching, tearing, and crumpling. This step involves discounting areas near the top and bottom of the edges, and considering an average value of colors near the edges to account for degradations. If it is assumed that the shreds are cut into strips which are taller than they are wider and relatively straight, pieces, there are several simplifying assumptions to help determine the correct orientation. One team ('wasabi'; <ref type="bibr" coords="5,135.27,616.35,14.34,8.12" target="#b18">[20]</ref>) in the DARPA Challenge noted that in a perfect rectangle, a correctly oriented shred will have a maximum number of edge pixels oriented along a single vertical line. In practice , however, this is not the case because each piece is not perfectly cut, some cuts can be slightly curved, some may have mangled cuts, and these could lead to errors in using this metric. Instead we build a vertical edge time series as mentioned earlier, in <ref type="figure" coords="5,235.28,673.14,19.80,8.12" target="#fig_0">Fig. 2</ref>and V r (y) respectively). An image that is well oriented should seek to minimize the average distance between each edge pixel and a vertical line. Therefore in finding the correct orientation for each shred we seek to find the optimum θ * such that: </p><formula>θ * = arg min θ (V (y) − E) 2 </formula><p>where E is the average location of the vertical edge that we are trying to orient (Note that V (y) is implicitly a function of θ ). In <ref type="figure" coords="5,333.76,434.16,29.54,8.12" target="#fig_4">Figure 7</ref> , we study the performance of this approach for Puzzle 1 of the DARPA Shredder Challenge. We can see that our orientation approach performs with ≤ 2% error for the vast majority of shreds; the greater errors are predominantly confined to shreds that are not completely separated from their neighbors. The visual analytics approach described later brings in crucial user input that alleviates some of these outlier cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Image Analysis</head><p> In matching two pieces together, there are several pieces of information which are available in order to find the optimum matches between them. The first has already been discussed as a tool for finding the correct orientations, viz. the shape of the vertical edges. This information is encoded as the vertical edge time series and used further below. The second piece of information is the content of the shreds themselves. This information is encoded in the Luma (a measure of brightness) time series: the datasets we use here contain full color information (and thus three channels of data, one for each primary color), we have found that using Luma <ref type="bibr" coords="6,157.44,205.23,13.75,8.12" target="#b10">[12]</ref>, defined as L = .3R + .59G + .11B was sufficient. While both Luma and edge shape information exist for the top and bottom edges, it is important to note that our algorithm focuses on primarily the left and right edges. This is because the cross cut shredding action creates large numbers of deformities on the cross-cuts and often mangle the top and bottom edges. Furthermore, because the top and bottom edges are much smaller, there is less information to match on and consequentially much harder to match effectively. Each of the two encoded data sources pose different advantages and disadvantages. The vertical edge time series, while extremely useful for jigsaw style pieces that had exaggerated features in the edges, did not by itself provide enough information to identify good matches. It resulted in false negatives when pieces were mangled on one side of a cut but not on the other, or when both pieces were mangled in different ways. Furthermore, false positives were created as a result of cuts being too straight and thus matching every other straight cut. The Luma time series, on the other hand, provides a better source of data, but brought its own problems. The Luma channel allows for matching content but is susceptible to false positives such as regularly spaced features e.g., the background lines on ruled paper, as can be seen in <ref type="figure" coords="6,140.58,403.99,29.03,8.12" target="#fig_3">Figure 6</ref>. Furthermore, the data provided by the Luma channel is mostly discontinuous, as the features contained in the shredded documents tend to be sharp and distinct. This means that, in comparing two time series, near misses will generate as much error as complete misses. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Color Targeting</head><p> While by default the Deshredder feature matching algorithm observes the luminosity of the shreds for points of interest to match with, Deshredder also allows the user to filter out a specific color to match pieces against. For instance, in <ref type="figure" coords="6,195.04,694.90,34.63,8.12">Figure 9a</ref>the user desires to begin to match by the colored ruling on the page. The user then uses an eye-dropper tool to select that color from an example piece and Figure 9: Deshredder allows a user to guide the matching algorithm to focus on a particular color in comparing shreds. can use a threshold slider to set the variability of colors to consider. Deshredder responds by highlighting the colors of interest and then rerunning the matching algorithm in the background focusing on the specific colors chosen. In <ref type="figure" coords="6,409.70,243.29,33.58,8.12">Figure 9b</ref>, conversely, the user wishes to match against the black pen color; in response Deshredder highlights the black pen strokes and focuses on matching the pen strokes as it reruns the matching algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Matching Shreds</head><p>Figure 10: The process for converting an edge, first to a Luma time series, then finding the edges, and finally creating a Chamfer distance distribution. The red lines denote the locations of the features. While being able to compare shreds based on their content and their shape is useful it is still not sufficient to make good matches. In order to make the best use of the information in the Luma channel, we developed a nearest neighbor matching algorithm based upon the notion of Chamfer similarity, described next. For each side of each image we build a Luma time series, and from this time series we use a simple one dimensional convolution kernel with weights <ref type="bibr" coords="6,362.04,540.81,32.38,8.97">[−1, 0, 1] </ref>to find the edges along the Luma time series. Next, we filter the Luma time series and note the largest peaks in the graph. to find the largest such edges in the image and mark these as our features. Finally, we build a Chamfer distance distribution which denotes the distance of a pixel to the nearest feature. <ref type="figure" coords="6,499.83,579.22,34.17,8.12">Figure 10</ref>shows each step of the process of building the Chamfer distance distribu- tion <ref type="bibr" coords="6,329.68,598.15,9.52,8.12" target="#b4">[5]</ref> . After building a Chamfer distance distribution for each vertical edge of the shred, we can then use these distributions to find the most suitable match between two edges. Suitable matches are found using the Chamfer similarity of two Chamfer distance distributions c 1 and c 2 defined as: </p><formula>ChamferSim(c 1 , c 2 ) = c 1 · c 2 max(c 1 · c 1 , c 2 · c 2 ) </formula><p> Here, c 1 and c 2 are distributions defined over the common boundary of the shreds. The entries of these distributions denote distances from the nearest feature in the corresponding shred. c 1 · c 2 denotes the scalar (dot) product of the vectors. We compute the Chamfer similarity four times for every pair of shreds, once for every possible orientation (see <ref type="figure" coords="6,487.24,713.83,20.13,8.12" target="#fig_1">Fig. 3</ref> ). These </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">STITCHING WORKFLOWS IN DESHREDDER</head><p>The Deshredder interface, at its core, provides an interactive approach for the user to explore the matches extracted from the algorithmic phase and helps organize them into a composite image. We used a checkered background to provide contrast to the gaps in the shreds. The framework can be broken down into a number of interdependent processes which are explained below: Match Selector: The matching facility is organized along two columns. The first column (called the parent column) helps the user select a specific shred to be matched. The set of best matches identified for this shred are presented as a stream of shreds in the second column (called the chooser column). Shreds other than the one(s) being currently matched are grayed out. In the simplest mode a user can select the most appropriate match from the chooser column. <ref type="figure" coords="7,347.31,167.52,33.99,8.12" target="#fig_0">Figure 12</ref>(a) gives an example of this mode of operation. In contrast, the Deshredder also provides an advanced mode, called the long mode, where the user can choose to view all the matches as an ordered matrix . Example of such an operation is shown in <ref type="figure" coords="7,313.56,205.38,33.90,8.12" target="#fig_0">Figure 12</ref>(b). On one hand this mode enables the user to quickly scan through all the possible matches to focus on an active shred in the parent column; at the same time it aids the user to be efficient about the match making process by providing semi non-serial access to the matches.Once the user is satisfied with a suggested match, Deshredder provides the capability to store this match in a palette to the right (as in <ref type="figure" coords="7,367.49,262.17,32.86,8.12" target="#fig_0">Figure 12</ref>(c)) referred to as the reconstruction palette Reconstruction Palette: The reconstruction palette provides a number of functionalities to the user, including (1) the ability to rotate the complete match, (2) drag either shred participating in the match for fine tuning of the placement of the match, (3) drag the entire match to another region of the reconstruction palette, (4) delete the match, (5) zoom in/out of the reconstruction palette. We now explain how each of these capabilities are organized into the workflow of a deshredding session. Functionality (1) is required because the alignment of the shreds can be inverted with respect to other matches in the reconstruction palette. Regarding functionality (2), giving the user the ability to fine tune the placement of the match is an important design requirement as this places less stringency on the alignment matching part of the algorithm. It also helps to better incorporate user experience directly into the interface so that future matches for a participating shred can be displayed from this modified placement. An example of the user activating functionalities (1) and </p><p>(2) can be seen in <ref type="figure" coords="7,378.71,432.53,32.81,8.12" target="#fig_0">Figure 12</ref>(d). As the user is traversing through the stitching process, he/she may begin to develop some idea about the relative placements of disconnected regions of matches developed till that point of the time. Functionality (3) enables the user to organize the matches in the reconstruction palette to represent these ideas and accelerate the stitching process and an example can be seen in <ref type="figure" coords="7,342.96,489.31,33.71,8.12" target="#fig_0">Figure 12</ref>(e). Finally, any match chosen by the user need not be the best when placed relative to other pieces and hence functionality (4) enables the user to explore a number of matches from the chooser column without forcing the user to adopt stringent requirements on the allowed number of trials. Functionality (5) is perhaps one of the most important feature as the zoom in/out feature allows the user to closely inspect a match by zooming in and also view the entire palette in a modestly-sized screen by zooming out (refer <ref type="figure" coords="7,335.71,565.03,31.75,8.12" target="#fig_0">Figure 12</ref>(i,j). Other Features: There are a few additional features we implemented with respect to the parent column. Similar to the simple mode of the chooser, the parent column can also be traversed oneat-a-time in either direction. For each traversal, the active element is changed and the chooser reflects the matches corresponding to this element. An advanced feature of this column is a 'hide' mode (<ref type="bibr" coords="7,543.65,631.28,14.35,8.12;7,313.56,640.74,32.88,8.12">Figure 12(g)</ref>) in which the user can decide on a piece to be trivial and hide it from all the suggested matches in the chooser column (for all the remaining matches in the current run). Further, in this column one can rotate a piece and this state can be locked. This feature was implemented as it took away the burden of trying to suggest orientations from the user and instead enables the user recognize the orientation most suitable to him/her (<ref type="figure" coords="7,450.07,697.53,31.98,8.12" target="#fig_0">Figure 12</ref>(f)). Also, when the active element in either the parent or the chooser column matches a piece, already matched and saved in the reconstruction palette, the pieces in the palette are highlighted(<ref type="figure" coords="8,188.19,63.44,32.30,8.12" target="#fig_0">Figure 12</ref> (h)). Finally, we implemented a state save-loader mechanism, whereby a user can save his current state of the reconstruction palette and load it for future runs. This is a very important feature as it enables the user to safely keep track of the progress made and at the same time allows collaborative stitching with other users. This feature also opens up the possibility of parallelism with respect to the user as a team can divide up the pieces to be matched among themselves and which can be merged at a later time (<ref type="figure" coords="8,148.74,139.15,31.91,8.12" target="#fig_0">Figure 12</ref>(k).) Aside from these features, Deshredder also provides tools like color selector (<ref type="figure" coords="8,109.59,158.08,29.11,8.12">Figure 9</ref>) and similarity matrix visualizer (<ref type="figure" coords="8,265.59,158.08,29.11,8.12" target="#fig_2">Figure 4</ref>) which are designed to pop-out on user command. All of the above features can be used by a user any number of times in a stitching session. One such sample run where the user is able to reconstruct part of the document is shown in <ref type="figure" coords="8,240.06,195.94,34.42,8.12" target="#fig_0">Figure 12</ref>which depicts all the functionalities being used to make the match. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS</head><p>We describe results of using Deshredder on Puzzle 1 of the DARPA Shredder Challenge. From the image containing the shreds (<ref type="figure" coords="8,284.10,257.90,14.35,8.12;8,54.00,267.36,16.66,8.12">Fig- ure 1</ref>), 225 pieces were extracted for reconstruction. In the challenge some questions about the document were asked which could be answered only after proper reconstruction <ref type="bibr" coords="8,197.67,286.30,9.52,8.12" target="#b0">[1]</ref>. <ref type="bibr" coords="8,125.47,512.09,9.52,8.12" target="#b0">[1]</ref>. First, we outline some quantitative results to assess the effectiveness of our matching algorithms. For each edge, we considered all possible matches, and then compared where each correct match occurred in the ordered ranking for each edge. We used four different evaluations for the matches: 1) Chamfer similarity, 2) the RMSE of the vertical edge time series of the match, 3) the RMSE of the Luma time series of the match, and 4) a combination of 1) and 2). Of these, the combination metric worked the best. The graphical results of this exercise are depicted in <ref type="figure" coords="8,140.37,609.72,33.12,8.12" target="#fig_1">Figure 13</ref>. The Luma evaluation performed the worst because as stated above, the discontinuous nature of the Luma time series function decreases the accuracy of the evaluation. Observing the behavior of the Chamfer+Vertical line we see that 50% of the correct matches will show up in the first 20% of the recommendations, ideal for quick visual inspection by analysts. Deshredder was then used by two experts to reconstruct the first puzzle from scratch. One expert was a trained knowledge discovery professional. The second expert was a practicing professional in the national security industry. We outline the overall strategies employed by them respectively. Figure 14: Expert user rejecting apparently good matches. (a) The words formed are meaningless, or (b) the words seem out of context. </p><formula>(a) (b) (c) </formula><p>Figure 15: Stitching strategy used by the Expert. (a) Stitching shreds to a single region, (b) using the long mode while stitching the image (c) avoiding matching an already matched piece </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Expert 1 Strategy</head><p>The knowledge discovery expert usually favored shreds that have traces of handwriting in them rather than empty shreds of legal pad. Nevertheless, apparently good matches (with respect to the continuation of boundaries) were rejected by this user because the words formed either did not make sense or appeared out of context. An example run is shown in <ref type="figure" coords="8,405.74,389.32,33.65,8.12" target="#fig_2">Figure 14</ref>. In (a), the words are evidently meaningless and are rejected by the user although the boundaries match quite nicely. In (b), a single 'D' appears in the text and hence was rejected by the user as being an out-of-context match. In this respect , an OCR subsystem to suggest words could be useful but may add increased lag due to dictionary search. In terms of 'positive' strategies, once confident of a particular match, this expert usually chose to extend the matches to regions by choosing the shred from the chooser column as the new parent column, rather than finding disconnected groups of matches. Further , while finding the matches, this expert usually chose to view the best suggested matches for a shred by scrolling rapidly via keyboard options. Based on the observations, the expert decided on whether to look into such matches or move to the next shred. Some of the aspects of the Deshredder, such as the highlight feature, helped the expert to keep track of the pieces and avoid spurious matches. Illustrations of these behaviors are shown in <ref type="figure" coords="8,463.07,540.75,33.88,8.12">Figure 15</ref>. The suggested matching regions between two shreds were also in general acceptable and only 17 percent of time dragging a suggested match for a more favorable boundary match was required. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Expert 2 Strategy</head><p>In contrast to the strategy of expert 1, the security expert employed some subtly different ones. This expert really preferred certain features of the Deshredder such as the hiding mode by which he was able to hide blank shreds. Also the linear search seemed more acceptable to this expert than the long mode. This expert felt more comfortable organizing his thoughts and looking into the details via the simple mode. The strategy employed was to match up via lines on pages and then via words. The expert was also seeking distinguishing features of the shreds to make the match. This expert suggested some features such as matching more than two pieces at a time and reorganizing the lines of the strip to retain every piece in the same orientation. The concept of the 'long mode' was appealing in principle , but practically rarely useful, to this expert. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSIONS</head><p>We outline below some of the lessons learned from our overall project leading to possibilities for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Lessons Learned</head><p>The working model of the Deshredder described here was arrived at after a number of missteps-steps which led to the current design considerations. As described in the paper, the Deshredder works by comparing the boundaries of two image fragments and inferring best possible matches based on their similarities. Recall that this matching process was implemented by modeling the boundaries as time series (Section 5) and then finding the best possible matches based on a modified Euclidean distance. <ref type="figure" coords="9,54.00,483.35,33.35,8.12" target="#fig_3">Figure 16</ref>: Matching edges using the time series modeling approach of <ref type="bibr" coords="9,63.71,492.81,13.75,8.12" target="#b14">[16]</ref>. This lack of accuracy in similarity detection could be attributed to the difference in scales of the two images. Scale dissimilarity leads to radial distortions and if such differences in scales are appreciable, the increased dissimilarity between two such time series will lead the nearest-neighbor algorithm to reject the shred in favor of a possible non-matching but similarly scaled shred. A scale invariant approach was outlined in <ref type="bibr" coords="9,112.22,562.40,13.75,8.12" target="#b34">[37]</ref>. However, this method incorporates additional computational complexities. Our proposed method of abstracting out the vertical edges is scale-and rotation-invariant in the sense that the figures in the DARPA Shredder Challenge appeared to be shredded in average in a rectangular manner. Furthermore, comparison of such vertical edges is relatively faster as the possible orientation space has been reduced to 4 combinations for a pair of shreds. </p><p>Fragment matching strategy: Reconstruction of original images from the shred was found to be sub-optimal when the time-series was encoded based on the distance of the boundary from the linear edge alone. For example, image 1 of <ref type="bibr" coords="9,187.30,675.98,10.46,8.12" target="#b0">[1] </ref>appears to be a handwritten note on a legal pad. Hence, two lowest distance matched edges may result in a reconstruction where the edges are non-continuous with respect to the colors. As such, the distance metric was combined with the color information obtained via a variation of neighboring pixel intensity, to produce a color-aware time-series. Visual analytics vs. fully automated strategies: Before adopting a visual analytic approach, we evaluated a fully automated method of stitching. A string-based physics model, where the similarity between the time-series representation of two images shreds were used as the attraction measure, was fitted on the shredded images. The aim was to automatically reduce the distance between neighborshreds and increase the distance between non-neighbor shreds. This induces a multi-level interaction between image shreds and to properly define such interactions, similarities should be recomputed as we combine image shreds into subunits. As the number of initial shreds is quite large, the number of comparisons required at each level quickly grows exponentially. This method further necessitates the similarity measures to have high fidelity. The visual analytic approach of Deshredder however can tolerate a high degree of distortion in matches and the matching process can be done only for the pair of shreds, thus being computationally more tractable. Interface Design: For the interface we started with the 'long mode' as the default view to an user. However, we realized quickly that 'simple mode' helps a user to focus on the current piece more intently than the 'long mode'. But at the same time the 'long mode' offered the possibility of glossing over multiple recommendations very quickly. After debating a number of possibilities for integrating these two modes such as a separate window, a pop out, or a window overlay, we decided on the final design based on expert user suggestions . In the similarity matrix and the color chooser we decided on an integrated approach to have a consistent look and feel of the software. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Future Work</head><p>Deshredder brings together the fields of time series representation, computer vision, and visual analytics to assist users in reconstructing shredded documents. It is our intent to further develop Deshredder into a complete problem solving environment for reconstructing shredded documents. One of the most important aspects of the system is creating good metrics to choose and evaluate matches. We would like to evaluate and develop metrics to further improve the suggestions that the algorithm makes. A second area which we desire to address is the need for a rotationally invariant matching. Our current techniques focus on the vertical edges only, and developing a rotationally invariant matching algorithm would enable us to provide more effective matching for differently shaped shreds as well as avoid the straightening step. Also, akin to Digistrips <ref type="bibr" coords="9,509.51,461.95,9.52,8.12" target="#b1">[2]</ref>, a gesture and touch-screen driven interface for assisting air traffic controllers in schedule management, we aim to evaluate Deshredder on a large high definition touch screen display and evaluate touch-based and gesture-based techniques for user input. Finally, we would like to conduct a complete user study and articulate usability concerns. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>Automatic deshredding of documents makes a powerful argument for a visual analytics strategy, as we have shown here. Deshredder combines the advantages of automated methods (in identifying possible matches) and enables user input (to drive the reconstruction process). Our goals in this paper were to articulate the many design decisions that we have made along the way and catalog them so that others working in this space can build upon these strategies. As we explore larger shredding puzzles using Deshredder, we believe the need for interesting problem solving strategies would become paramount, which will hopefully spur more research into visual analytic methods. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,324.95,302.91,221.67,8.12"><head>Figure 2: </head><figDesc>Figure 2: Extraction of left and right time series from a shred. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,316.96,624.24,237.63,8.12"><head>Figure 3: </head><figDesc>Figure 3: Four possible orientations between two shredded pieces. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="4,313.56,651.75,244.44,8.12;4,313.56,661.21,244.44,8.12;4,313.56,670.68,72.21,8.12"><head>Figure 4: </head><figDesc> Figure 4: An example Deshredder session of interacting with similarity matrices and propagating identified matches to reconstruct a shredded document. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,54.00,537.16,244.43,8.12;5,54.00,546.63,244.44,8.12;5,54.00,556.09,244.44,8.12;5,54.00,565.56,84.05,8.12"><head>Figure 6: </head><figDesc>Figure 6: Two methods for determining the correct shred orientation. (a) Maximizing the number of edge pixels. (b) Minimizing the least squares deviation of the edges of the shred. The arrows represent the deviation from vertical. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="5,313.56,689.62,244.44,8.12;5,313.56,699.09,104.64,8.12"><head>Figure 7: </head><figDesc>Figure 7: Distribution of straightening errors for Puzzle 1 of the DARPA Shredder Challenge. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,54.00,615.41,244.44,8.12;6,54.00,624.88,97.23,8.12"><head>Figure 8: </head><figDesc>Figure 8: A sample Luma time series for the left-and right-hand sides of the example shred. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,54.00,248.74,244.44,8.12;7,54.00,258.21,146.70,8.12;7,203.45,263.43,18.18,2.68;7,224.48,258.21,73.95,8.12;7,54.00,267.67,81.45,8.12;7,137.70,272.88,18.18,2.68"><head>Figure 11: </head><figDesc>Figure 11: Examples of (a) good and (b) bad matches. The good match has a Chamfer similarity value of .880, while the bad match as a similarity value of .743. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,54.00,557.64,244.44,8.12;7,54.00,567.10,244.44,8.12;7,54.00,576.56,244.44,8.12;7,54.00,586.02,244.44,8.12;7,54.00,595.49,244.44,8.12;7,54.00,604.96,141.81,8.12"><head>Figure 12: </head><figDesc>Figure 12: Schematic of a stitching work flow using Deshredder. (a) Basic layout of UI; (b)-(e) One run in long mode and demonstrating reconstruction palette features;(f)-(j) parallel run using more advanced features of the reconstruction palette and use of the parent column; (k) merging of two runs. The similarity matrix is a pop-up feature and not shown in the work flow. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9" coords="8,54.00,502.62,244.44,8.12;8,54.00,512.09,84.17,8.12"><head>Figure 13: </head><figDesc>Figure 13: Performance of matching strategies in DARPA Shredder Challenge Puzzle 1 [1]. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true" coords="2,331.52,393.39,208.51,135.32"><figDesc coords="2,331.52,393.39,208.51,8.12">Table 1: Functional taxonomy of the deshredding process.</figDesc><table coords="2,342.68,404.58,186.19,124.14">Shred representation 
– How are shreds represented? 
image representations 
time series 
Matching algorithm 
– How are shreds matched? 
OCR tags 
pen connectivity 
stroke continuity 
Chamfer distance 
Assembly 
– How are shreds assembled? 
Manual assembly 
Similarity matrix interaction 
Workflow tools 
– Higher-level reconstruction 
Constraint propagation 
State capture and reuse 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true" coords="3,54.00,54.46,244.43,197.04"><figDesc coords="3,54.00,54.46,244.43,8.12;3,54.00,63.92,38.35,8.12">Table 2: Comparative analysis of capabilities of various deshredding algorithms</figDesc><table coords="3,63.79,79.15,220.12,172.34">Criteria 
Deshredder 
Unshredder 

'All Your Shreds Are 
Belong To U.S.' 
'wasabi' 
'mmbd' 
'UCSD' 

Cross-Cut 
Shreds 

Yes 
No 
Yes 
Yes 
Yes 
Yes 

User Collabo-
ration 

Yes 
Yes 
No 
N/A 
N/A 
Yes 

Algorithmic 
Support 
for 
matching 

Yes 
Yes 
Yes 
Yes 
Yes 
No 

Visual Analyt-
ics 

Yes 
No 
No 
No 
No 
No 

Applicable to 
Sensitive Data 

Yes 
Yes 
Yes 
Yes 
Yes 
No 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false" coords="5,54.00,673.14,244.45,50.02"><figDesc coords="5,255.08,673.14,3.36,8.12;5,54.00,684.88,224.73,8.97">. Definition 1. A Vertical Edge Time Series is a time series V (</figDesc><table coords="5,54.00,684.88,244.45,38.28">y) in-
dexed by row and gives the distance from the left-most or right-most 
pixel from a perfectly vertical line. For each shred there exists a ver-
tical edge time series for the left and for the right-hand sides (V l (y) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false" coords="6,54.00,145.52,244.45,48.91"><figDesc coords="6,54.00,145.52,244.44,8.12;6,54.00,155.12,244.45,7.98;6,54.00,164.59,244.43,7.98;6,54.00,173.46,238.15,9.89;6,63.96,186.31,33.87,8.12">Definition 2. A Luma Time Series is a time series L(y) indexed by row and gives the value of the Luma of the left-most or right-most pixel of that row. For each shred there exists a Luma time series for the left and for the right-hand sides (L l (y) and L r (y) respectively). Although</figDesc><table></table></figure>

			<note place="foot">Choice of fragment encoding: There are a number of ways described in the literature [16, 37, 24] for converting a image boundary into a time series. One approach that we initially attempted was to encode the image boundary as a time series based on the radial distance of the boundary from the image center. However, a number of the image shreds were rotated [1] and hence rotation invariant strategies [16] were necessary. To understand the performance of this strategy in comparison to our model of time-series extraction we describe the results of the said strategy on a sample image in Figure 16. The blue image matches the red image exactly along a boundary as shown in the Figure 16 (left). Figure 16 (right) shows the best placement of the time series for the matching part of the two images, computed as described in [16].The dissimilarity of these time-series for perfectly aligned pieces as seen in the figure, indicates these methods as unsuitable for use as a similarity measure.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="10,72.26,70.93,226.11,7.22;10,72.26,80.40,185.70,7.22"  xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName>
				<surname>Darpa Shredder Challenge</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Last Visited</title>
		<imprint>
			<date type="published" when="2012-03-31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,89.86,226.12,7.22;10,72.26,99.48,224.61,6.81;10,72.26,108.79,189.08,7.22"  xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="m">Digistrips : humaniser les interfaces</title>
		<imprint>
			<date type="published" when="2012-06-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,118.25,226.18,7.22;10,72.26,127.72,226.19,7.22;10,72.26,137.19,192.57,7.22"  xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Agarwala</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Dontcheva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Agrawala</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Drucker</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Colburn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Curless</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Salesin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cohen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interactive Digital Photomontage . ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="294" to="302" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,146.64,226.17,7.22;10,72.26,156.11,226.18,7.22;10,72.26,165.58,179.16,7.22"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective Image Mining by Representing Color Histograms as Time Series</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<forename type="middle">A</forename>
				<surname>Aghbari</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advanced Computational Intelligence and Intelligent Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="114" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,175.04,226.17,7.22;10,72.26,184.51,226.18,7.22;10,72.26,193.97,226.18,7.22;10,72.26,203.43,226.19,7.22;10,72.26,212.90,17.94,7.22"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Parametric Correspondence and Chamfer Matching: Two New Techniques for Image Matching</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">G</forename>
				<surname>Barrow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">M</forename>
				<surname>Tenenbaum</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C</forename>
				<surname>Bolles</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<forename type="middle">C</forename>
				<surname>Wolf</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Joint Conference on Artificial intelligence of IJCAI&apos;77</title>
		<meeting>the 5th International Joint Conference on Artificial intelligence of IJCAI&apos;77</meeting>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="659" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,222.36,226.17,7.22;10,72.26,231.83,226.17,7.22;10,72.26,241.29,61.33,7.22"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic Panoramic Image Stitching using Invariant Features</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Brown</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,250.76,226.16,7.22;10,72.26,260.22,226.17,7.22;10,72.26,269.68,46.50,7.22"  xml:id="b6">
	<monogr>
		<title level="m" type="main">UCSD DARPA Shredder Challenge Team</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Cebrin</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012-06-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,279.15,226.17,7.22;10,72.26,288.61,226.18,7.22;10,72.26,298.08,173.58,7.22"  xml:id="b7">
	<analytic>
		<title level="a" type="main">A Real-time Panoramic Vision System for Autonomous Navigation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Dasgupta</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Banerjee</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Winter Simulation, WSC &apos;04</title>
		<meeting>the 36th Conference on Winter Simulation, WSC &apos;04</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1706" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,307.55,226.18,7.22;10,72.26,317.01,226.17,7.22;10,72.26,326.47,226.17,7.22;10,72.26,335.94,160.40,7.22;10,54.00,345.40,58.61,7.22;10,149.52,345.40,148.93,7.22;10,72.26,355.03,176.82,6.81;10,72.26,364.49,188.36,6.81;10,72.26,373.80,134.83,7.22"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Document Analysis applied to Fragments: Feature Set for the Reconstruction of Torn Documents</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Diem</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Kleber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Sablatnig</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th IAPR International Workshop on Document Analysis Systems</title>
		<meeting>the 9th IAPR International Workshop on Document Analysis Systems</meeting>
		<imprint>
			<date type="published" when="2010-08-01" />
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
	<note>Last. visited</note>
</biblStruct>

<biblStruct coords="10,72.26,383.26,226.18,7.22;10,72.26,392.72,226.17,7.22;10,72.26,402.19,226.18,7.22;10,72.26,411.69,226.18,7.09;10,72.26,421.12,188.11,7.22"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Image Classification using Histograms and Time Series Analysis: a Study of Age-related Macular Degeneration Screening in Retinal Image Data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">H A</forename>
				<surname>Hijazi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Coenen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Zheng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Industrial Conference on Advances in Data Mining: Applications and Theoretical Aspects, ICDM &apos;10</title>
		<meeting>the 10th Industrial Conference on Advances in Data Mining: Applications and Theoretical Aspects, ICDM &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,430.59,226.17,7.22;10,72.26,440.08,226.18,7.09;10,72.26,449.51,33.44,7.22"  xml:id="b10">
	<monogr>
		<title level="m" type="main">Studio Encoding Parameters of Digital Television for Standard 4:3 and Wide-Screen 16:9 Aspect Ratios</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,458.97,226.18,7.22;10,72.26,468.44,171.99,7.22"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Data Clustering: A Review</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">K</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">N</forename>
				<surname>Murty</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Flynn</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,477.91,226.18,7.22;10,72.26,487.37,226.11,7.22;10,72.26,496.83,193.86,7.22"  xml:id="b12">
	<analytic>
		<title level="a" type="main">DARPA Shredder Challenge</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Jou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M.-H</forename>
				<surname>Tsai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Pei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Marques</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S.-F</forename>
				<surname>Chang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Last Visited</title>
		<imprint>
			<date type="published" when="2011-06-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,506.30,226.18,7.22;10,72.26,515.76,226.18,7.22;10,72.26,525.23,94.32,7.22"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Reconstructing Shredded Documents through Feature Matching</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Justino</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">S</forename>
				<surname>Oliveira</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Freitas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Science International</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="140" to="147" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,534.69,226.18,7.22;10,72.26,544.16,226.18,7.22;10,72.26,553.62,226.18,7.22;10,72.26,563.08,226.18,7.22;10,72.26,572.55,88.12,7.22"  xml:id="b14">
	<analytic>
		<title level="a" type="main">LB Keogh Supports Exact Indexing of Shapes Under Rotation Invariance with Arbitrary Representations and Distance Measures</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S.-H</forename>
				<surname>Lee</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Vlachos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Very Large Data Bases, VLDB &apos;06</title>
		<meeting>the 32nd International Conference on Very Large Data Bases, VLDB &apos;06</meeting>
		<imprint>
			<date type="published" when="2006-09" />
			<biblScope unit="page" from="882" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,582.01,226.18,7.22;10,72.26,591.52,226.18,7.09;10,72.26,600.95,176.81,7.22"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Object Recognition from Local Scale-Invariant Features</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE International Conference on Computer Vision of ICCV &apos;99</title>
		<meeting>the 7th IEEE International Conference on Computer Vision of ICCV &apos;99</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,610.40,226.17,7.22;10,72.26,619.87,226.19,7.22;10,72.26,629.33,17.94,7.22"  xml:id="b16">
	<monogr>
		<title level="m" type="main">Unshredder: Shredded Document Reconstruction System</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Last. vsited</note>
</biblStruct>

<biblStruct coords="10,72.26,638.80,226.18,7.22;10,72.26,648.27,226.18,7.22;10,72.26,657.73,226.19,7.22;10,72.26,667.20,33.88,7.22"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Reconstructing Strip-shredded Documents using Color as Feature Matching</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A O</forename>
				<surname>Marques</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">O A</forename>
				<surname>Freitas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM Symposium on Applied Computing, SAC &apos;09</title>
		<meeting>the 2009 ACM Symposium on Applied Computing, SAC &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="893" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,72.26,676.66,226.17,7.22;10,72.26,686.12,226.18,7.22;10,72.26,695.59,226.07,7.22;10,72.26,705.21,224.30,6.81;10,72.26,714.52,89.45,7.22;10,313.56,53.95,244.44,7.22;10,331.82,63.42,226.18,7.22;10,331.82,72.88,163.53,7.22"  xml:id="b18">
	<analytic>
		<title level="a" type="main">You should Probably Start Burning your Mail: What I Learned from the DARPA Shredder Challenge. http://www.marcnewlin.com you-should-probably-start-burning-your_02.html, Last Visited Human-centric Panoramic Imaging Stitching</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Newlin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>No</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">No</forename>
				<surname>Snow</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">M</forename>
				<surname>Pants</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kitani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Koike</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Augmented Human International Conference</title>
		<meeting>the 3rd Augmented Human International Conference</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,82.35,226.18,7.22;10,331.82,91.81,226.18,7.22;10,331.82,101.27,218.88,7.22"  xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining Forces to Reconstruct Strip Shredded Text Documents</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Prandtstetter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">R</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Hybrid Metaheuristics, HM &apos;08</title>
		<meeting>the 5th International Workshop on Hybrid Metaheuristics, HM &apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="175" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,110.74,226.18,7.22;10,331.82,120.20,226.18,7.22;10,331.82,129.67,226.18,7.22;10,331.82,139.13,84.57,7.22"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-heuristics for Reconstructing Cross Cut Shredded Text Documents</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Prandtstetter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">R</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation, GECCO &apos;09</title>
		<meeting>the 11th Annual Conference on Genetic and Evolutionary Computation, GECCO &apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,148.60,226.18,7.22;10,331.82,158.07,226.18,7.22;10,331.82,167.52,226.19,7.22;10,331.82,176.99,33.88,7.22"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Mining Historical Documents for Near-Duplicate Figures</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Rakthanmanon</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Zhu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">J</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE 11th International Conference on Data Mining</title>
		<meeting>the 2011 IEEE 11th International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,186.46,226.19,7.22;10,331.82,195.92,226.18,7.22;10,331.82,205.43,226.16,7.09;10,331.82,214.85,127.18,7.22"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Edge Envelope based Reconstruction of Torn Document</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">A</forename>
				<surname>Santoshkumar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">K</forename>
				<surname>Shreyamshakumar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Seventh Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,224.31,226.18,7.22;10,331.82,233.78,226.18,7.22;10,331.82,243.24,226.18,7.22;10,331.82,252.71,69.96,7.22"  xml:id="b23">
	<analytic>
		<title level="a" type="main">A Memetic Algorithm for Reconstructing Cross-cut Shredded Text Documents</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Schauer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Prandtstetter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">R</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on Hybrid metaheuristics, HM&apos;10</title>
		<meeting>the 7th international conference on Hybrid metaheuristics, HM&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,262.17,226.18,7.22;10,331.82,271.68,226.17,7.09;10,331.82,281.10,164.49,7.22"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Path Visualization for Adjacency Matrices</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Shen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K.-L</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroVis07 Joint Eurographics -IEEE VGTC Symposium on Visualization</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,290.56,226.18,7.22;10,331.82,300.03,226.18,7.22;10,331.82,309.49,45.61,7.22"  xml:id="b25">
	<monogr>
		<title level="m" type="main">An Investigation into Automated Shredded Document Reconstruction using Heuristic Search Algorithms</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Skeoch</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,318.96,226.18,7.22;10,331.82,328.43,226.18,7.22;10,331.82,337.88,164.21,7.22"  xml:id="b26">
	<analytic>
		<title level="a" type="main">A Contour Matching Algorithm to Reconstruct Ruptured Documents</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Stieber</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schneider</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Nickolay</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Krüger</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="121" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,347.35,226.18,7.22;10,331.82,356.82,226.18,7.22;10,331.82,366.28,226.18,7.22;10,331.82,375.79,226.18,7.09;10,331.82,385.21,89.00,7.22"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Shredded Document Reconstruction using MPEG-7 Standard Descriptors</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ukovich</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ramponi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Doulaverakis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kompatsiaris</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Strintzis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing and Information Technology Proceedings of the Fourth IEEE International Symposium on</title>
		<meeting><address><addrLine>dec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="334" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,394.67,226.18,7.22;10,331.82,404.14,226.18,7.22;10,331.82,413.60,84.79,7.22"  xml:id="b28">
	<monogr>
		<title level="m" type="main">Classification Visualization with Shaded Similarity Matrix</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Yu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Gasser</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,423.07,226.18,7.22;10,331.82,432.57,226.19,7.09;10,331.82,442.00,153.73,7.22"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Hiding Seams in High Dynamic Range Panoramas</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ward</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Symposium on Applied Perception in Graphics and Visualization, APGV &apos;06</title>
		<meeting>the 3rd Symposium on Applied Perception in Graphics and Visualization, APGV &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="150" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,451.46,226.18,7.22;10,331.82,460.92,41.40,7.22"  xml:id="b30">
	<monogr>
		<title level="m" type="main">Mastering Digital Panoramic Photography. Rocky Nook Series</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Woeste</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,470.39,226.18,7.22;10,331.82,479.85,118.19,7.22"  xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Wei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mafra-Neto</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Finding Motifs in Database of Shapes. SDM &apos;07</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,489.32,226.17,7.22;10,331.82,498.79,226.18,7.22;10,331.82,508.25,109.23,7.22"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast Panorama Stitching for High-Quality Panoramic Images on Mobile Phones</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Xiong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Pulli</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="298" to="306" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,517.71,226.17,7.22;10,331.82,527.18,226.18,7.22;10,331.82,536.64,77.93,7.22"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Manifold Clustering of Shapes</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yankov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Data Mining, ICDM &apos;06</title>
		<meeting>the Sixth International Conference on Data Mining, ICDM &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1167" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,331.82,546.11,226.17,7.22;10,331.82,555.57,226.18,7.22;10,331.82,565.08,226.18,7.09;10,331.82,574.50,130.84,7.22"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting Motifs Under Uniform Scaling</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yankov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Keogh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Medina</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Chiu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Zordan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="7" to="844" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
