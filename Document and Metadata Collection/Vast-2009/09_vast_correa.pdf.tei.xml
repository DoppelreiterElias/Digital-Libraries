<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Framework for Uncertainty-Aware Visual Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Carlos</forename>
								<forename type="middle">D</forename>
								<surname>Correa</surname>
							</persName>
							<affiliation>
								<orgName type="institution">University of California at Davis</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Yu-Hsuan</forename>
								<surname>Chan</surname>
							</persName>
							<affiliation>
								<orgName type="institution">University of California at Davis</orgName>
							</affiliation>
						</author>
						<author>
							<persName>
								<forename type="first">Kwan-Liu</forename>
								<surname>Ma</surname>
							</persName>
							<affiliation>
								<orgName type="institution">University of California at Davis</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Framework for Uncertainty-Aware Visual Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Uncertainty</term>
					<term>Data Transformations</term>
					<term>Principal Compo- nent Analysis</term>
					<term>Model Fitting</term>
				</keywords>
			</textClass>
			<abstract>
				<p>Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and clustering , have been coupled with visualization to help analysts understand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The goal of analytical reasoning is to gain insight from large amounts of disparate and conflicting data with varying levels of structure. Visual analytics seeks to facilitate this process by means of interactive visual metaphors. However, limitations on technology and human power make it difficult to cope with the growing scale and complexity of data. Therefore, it is seldom possible to analyze data in its raw form. It must be transformed to a suitable representation, which facilitates the discovery of interesting patterns . Dolfing <ref type="bibr" coords="1,107.15,456.55,10.46,8.12" target="#b8">[9] </ref>describes the visual analytics process as a series of transformations that facilitate insight from a collection of heterogeneous data sources. Thus, transformations can be categorized as data/visual transformations, which derive representations with increasing structure and meaning, and visual mappings, which convert these structures into visual elements, used by a visualization interface. <ref type="figure" coords="1,92.29,516.33,30.80,8.12">Figure 1</ref>shows an overview of such a visual reasoning process. Data is inherently uncertain and often incomplete and contradictory . For instance, data collected from online news sources and blogs is often populated with misinformation and deception. Measured data contains errors, introduced by the acquisition process or systematically added due to computer imprecision. For the analyst, it is important to be aware of the sources and degree of uncertainty in the data. As data is pre-processed, transformed, and mapped to a visual representation, this uncertainty is compounded and propagated , making it difficult to preserve the quality of data along the reasoning process. In this paper, we present a framework to represent and quantify the uncertainty through a series of data transfor-mations. With an explicit representation of uncertainty at all stages of the process, the analyst can make informed decisions based on the levels of confidence of the data and evaluate the insight gained on previous stages of the reasoning process. This uncertainty is not only propagated from the original data to the visual representations, but also data transformations themselves generate additional uncertainties . For example, complex multi-variate data is often projected to a low dimensional space for easy visualization, such as Principal Component Analysis, which implies a loss of information. For the user, it becomes important to have a visual representation that not only summarizes the uncertainty of the information being presented , but also helps identify the sources of that uncertainty. To illustrate the uses of our framework, we use a case study from the Boston neighborhood housing price data set, consisting of a multi-variate dataset about different factors that affect the mean value of housing in the Boston area, collected in the 1950s <ref type="bibr" coords="1,540.82,303.63,13.75,8.12" target="#b11">[13]</ref>. This dataset is inherently uncertain, due to statistical sampling or errors . It was soon noted that a certain variable contained an incorrect bias. To analyze this data, we follow common data analysis tools, such as model fitting, principal component analysis and clustering, and show how uncertainty is not only propagated, but also aggregated in each of these stages. An uncertainty-aware framework not only reports the sources of uncertainty, but also requires transformations that can deal with uncertain inputs. We enhance traditional visual analytics tools with uncertainty information, which shows an overview of the distribution of error and probabilistic variance of the data. In addition, an explicit visual representation of the sensitivity coefficients reveals correlations between the output uncertainty and specific input variables that may be difficult to discover or easily missed by means of statistic analysis alone. Keim et al. suggested the mantra: " Analyze First -Show the Important -Zoom, Filter and Analyze Further -Details on Demand " to guide the visual analytics process <ref type="bibr" coords="1,454.18,473.04,13.75,8.12" target="#b15">[17]</ref>. In our work, we show that a similar guide applies to uncertainty. We first analyze the data in terms of sensitivity and uncertainty, we show the important, i.e., the most influencing or uncertain variables and then we show details on demand, such as sensitivity coefficients for specific transformations and data points. In this paper, we present a series of visual representations that combines summarized and detailed views of the uncertainty of a multi-dimensional complex data set. Although a proof-of-concept case is depicted, we believe our framework can be extended to incorporate a variety of visual analysis tools. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multivariate analysis is at the core of visual analytics. Methods for this type of analysis include regression <ref type="bibr" coords="1,464.51,604.20,13.75,8.12">[10]</ref>, generalized additive models <ref type="bibr" coords="1,346.83,614.17,14.94,8.12" target="#b12">[14] </ref>and response surface analysis <ref type="bibr" coords="1,475.62,614.17,9.52,8.12" target="#b2">[3]</ref>. These methods in general try to find relationships among variables and fit models to multi-variate data. Other tools are used to reduce the amount of information encoded in the multi-variate data, such as binning and sampling <ref type="bibr" coords="1,353.44,654.02,13.75,8.12" target="#b26">[28]</ref>, projections <ref type="bibr" coords="1,415.75,654.02,13.75,8.12" target="#b23">[25]</ref>, multi-dimensional scaling <ref type="bibr" coords="1,532.00,654.02,10.45,8.12" target="#b3">[4] </ref>and clustering <ref type="bibr" coords="1,355.56,663.98,9.52,8.12" target="#b1">[2]</ref>. Yang et al. integrate analysis tools with visual exploration of multivariate data <ref type="bibr" coords="1,393.59,683.95,14.94,8.12" target="#b31">[33] </ref>using the Nugget Management System, which incorporates user interest to guide the analysis. Barlowe et al. introduce the derivatives of dependent variables to help find correlations between variables <ref type="bibr" coords="1,430.17,713.84,9.52,8.12" target="#b0">[1]</ref>. In our paper, we study another <ref type="figure" coords="2,54.00,232.65,29.09,7.64">Figure 1</ref>: Uncertainty-aware Visual Analytics Process. In general, visual analytics is the process of transforming input data into insight. A similar process occurs for the uncertainty. First, uncertainty modeling generates a model for source uncertainty. As data is transformed, these uncertainties are propagated and aggregated. We obtain such estimates via sensitivity and error modeling. Finally, the uncertainty on the derived data and its sources are mapped to visual representations, which finally populate the view used by the analyst. important aspect when dealing with multivariate data, the issue of data and transformation uncertainty. </p><p>Although there is no consensus on the scope of uncertainty, a number of definitions have been proposed. Hunter and Goodchild <ref type="bibr" coords="2,54.00,330.90,14.94,8.12" target="#b14">[16] </ref> define uncertainty as " the degree to which the lack of knowledge about the amount of error is responsible for hesitancy in accepting results and observations without caution " . This definition has spun a number of interpretations on what can be measured as uncertainty. Taylor and Kuyatt <ref type="bibr" coords="2,166.12,370.75,14.94,8.12" target="#b25">[27] </ref>proposed a series of guidelines for evaluating uncertainty of measurement results, which is classified as either random or systematic error. This led to a classification by Pang et al.<ref type="bibr" coords="2,106.48,400.64,12.95,8.12" target="#b19">[21]</ref>, who suggested three types of uncertainty that are relevant for visualization of complex data: statistical, for measurements with a known distribution, error, a difference between a measure and known ground truth, and range, which represents an interval where data exists. To accommodate the varying data types and transformations on an analytical process, Thomson et al. define an uncertainty typology <ref type="bibr" coords="2,140.89,460.42,13.75,8.12" target="#b27">[29]</ref> , identifying key components of uncertainty such as accuracy/error, precision, completeness, consistency, lineage, credibility, subjectivity, and interrelatedness. These frameworks have stemmed from geospatial information systems. Recently , Zuk and Carpendale extended this typology of uncertainty to reasoning as a way to support visualization of analytic processes <ref type="bibr" coords="2,54.00,520.20,13.75,8.12" target="#b33">[35]</ref>. The study of uncertainty can be further categorized as those concerned with uncertainty modeling and those with uncertainty propagation . To model uncertainty, numerous techniques have been proposed , including probabilistic measures, Bayesian networks <ref type="bibr" coords="2,276.86,564.39,13.75,8.12" target="#b16">[18]</ref>, belief functions <ref type="bibr" coords="2,111.46,574.36,13.75,8.12" target="#b10">[12]</ref>, interval sets <ref type="bibr" coords="2,174.83,574.36,14.94,8.12" target="#b32">[34] </ref>and fuzzy sets <ref type="bibr" coords="2,243.41,574.36,13.75,8.12" target="#b20">[22]</ref> . A different issue is the process of uncertainty propagation, which deals with the fact that uncertainty gets transformed as data moves through the analytics process. As suggested by Taylor and Kuyatt for the analysis of variance of measurements, it is possible to derive the variance propagated by a transformation as a linear combination of the variance of its inputs <ref type="bibr" coords="2,146.04,634.13,13.75,8.12" target="#b25">[27]</ref>. This simplification is also valid for non-linear transformations, as it results from the first order Taylor expansion of the transformation. This model was further simplified by Thomson et al. <ref type="bibr" coords="2,120.75,664.01,13.75,8.12" target="#b27">[29]</ref> , who proposed to model the different uncertainty types of their typology as the output of simple operations on its variances. In their case, the uncertainty of transformations or the analysis process (often a subjective measure) is known. In general, the uncertainty of a transformation must be derived from a sensitivity or perturbation analysis, which involves the differentiation of a transformation with respect to its inputs. Because some transformations are only provided as black boxes, these parameters must be approximated, using methods such as linear least squares and expectation-maximization (EM) algorithms. Frey and Patil review a number of sensitivity analysis methods <ref type="bibr" coords="2,468.15,326.55,13.75,8.12" target="#b9">[11]</ref>. Tanaka surveys the sensitivity analysis in the scope of multivariate data analysis <ref type="bibr" coords="2,540.81,336.51,13.75,8.12" target="#b24">[26]</ref>. Specific analyses of uncertainty for certain common data analysis tools have been proposed. Chan et al. present a sensitivity analysis for variance-based methods in general <ref type="bibr" coords="2,470.93,366.40,9.52,8.12" target="#b4">[5]</ref>. Cormode et al. <ref type="bibr" coords="2,545.30,366.40,9.52,8.12" target="#b6">[7]</ref>, Chau et al. <ref type="bibr" coords="2,358.41,376.36,10.46,8.12" target="#b5">[6] </ref>and Ngai et al. <ref type="bibr" coords="2,424.40,376.36,14.94,8.12" target="#b18">[20] </ref> propose extensions to perform kmeans clustering on uncertain data. Similar studies have been carried out to quantify the sensitivity and uncertainty of the principal components of multi-variate data <ref type="bibr" coords="2,441.00,406.25,14.19,8.12">[30,</ref><ref type="bibr" coords="2,457.99,406.25,10.65,8.12" target="#b30"> 32]</ref>. Kurowicka and Cooke extend the issue of uncertainty analysis with high dimensional dependence modeling, combining both analytical tools with graphic representations <ref type="bibr" coords="2,374.22,436.14,13.75,8.12" target="#b16">[18]</ref> . In most of these cases, sensitivity analysis implies knowing the derivatives of the data transformations. Barlowe et al. incorporate derivatives to help the analyst assess the sensitivity of dependent variables with respect to the source data <ref type="bibr" coords="2,534.61,466.03,9.52,8.12" target="#b0">[1]</ref>. In our paper, we use the derivatives of data transformations in a more general way, as a means to measure and quantify uncertainty propagation and aggregation throughout the visual analytics process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNCERTAINTY FRAMEWORK</head><p>The visual analytics process is often described as a sequence of transformations from raw data to insight, via abstractions and visual representations, as depicted in <ref type="figure" coords="2,446.00,552.76,29.21,8.12">Figure 1</ref> . The process of transforming raw data to abstractions and derived data is in fact a complex network of transformations, which propagates and aggregates uncertainty. To measure this uncertainty, we augment the visual analytics in the following ways: First, the input data uncertainty is modeled. Uncertainty modeling is a rather general approach, and numerous methods have been proposed to achieve it, including parametric and non-parametric models. In the former, a statistical model is applied to the input data. In the case of Gaussian distributions, for example, the standard deviation serves as a measure of the data uncertainty. In the latter, the uncertainty is represented from the data distribution directly , e.g., as a histogram. As the data is transformed, this uncertainty is propagated through the analytic process. The amount of uncertainty propagated by a transformation depends on how sensitive is the output given a set of inputs. To model the uncertainty propagation, we extend data transformations so that we can query their sensitivity parameters. In addition, transformations themselves aggregate uncertainty, typically due to error or loss of information. We obtain the aggregated uncertainty via error modeling of the transformation. Transformations that can be queried for their sensitivity parameters and the aggregated error are known as uncertainty-aware transformatins. Once the uncertainty is modeled and propagated, the results is propagated through the visual mapping stage, via an uncertainty visual mapping and uncertainty views. The following sections describe each of these stages in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Uncertainty Modeling</head><p>Uncertainty modeling consists of deriving a mathematical model to describe the uncertainty of the source data. There are numerous methods for modeling uncertainty, including probability measures, belief functions, interval arithmetic and fuzzy sets <ref type="bibr" coords="3,236.84,209.63,13.75,8.12" target="#b10">[12]</ref> . In this paper , we focus on parametric models and consider the input variables as random variables. In this paper, we consider the input data X to be modeled as a Gaussian Mixture Model (GMM): </p><formula>X ∼ N ∑ i=0 N(µ i , σ i ) (1) </formula><p>where N(µ, σ ) is a Gaussian distribution of mean µ and standard deviation σ . The uncertainty is then represented as a collection of standard deviations. Gaussian mixture models are interesting since they can be adapted to fit a wide range of probability distributions. Although non-parametric models are becoming very popular for representing uncertainty, we believe that the use of parametric models is useful in applications when the analyst aims at deriving quantitative models that explain the distribution of data. It is important to categorize the uncertainty in the visual analytics process as either data or transformation uncertainty. The former deals with the uncertainty inherent in the source and derived data, either due to error, incomplete data or source reliability. The latter represents the uncertainty added by the data transformation. In the visual analytics process, we must be able to represent the interaction between these two types of uncertainty. This is achieved through methods such as uncertainty propagation and aggregation. <ref type="figure" coords="3,54.00,461.14,31.07,8.12" target="#fig_0">Figure 2</ref>shows an example of Gaussian models used to estimate the uncertainty in the distributions of a number of variables from a housing data set <ref type="bibr" coords="3,114.02,481.06,13.75,8.12" target="#b11">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uncertainty Propagation</head><p> Uncertainty propagation occurs as data is transformed along the visual analytics process. The more sensitive a given transformation is to variation, the larger is the uncertainty propagated through it. For the case of Gaussian distributions, it is well known that uncertainty propagates linearly for linear transformations. This generalizes to mixtures of Gaussians as well. Non-linear models, however , do not result in uncertainty propagated using the same transformation , but can be approximated as follows. Let us consider a nonlinear transformation : </p><formula>y = f (x) </formula><formula>(2) </formula><p>of a multi-variate vector x. Let x be modeled as a Gaussian mixture model: </p><formula>p(x) = N ∑ i=1 w i N(x|µ i , Σ i ) (3) </formula><p>where N(x|µ, Σ) is a Gaussian probability distribution with mean µ and covariance Σ. The result of applying f is another Gaussian mixture model with mean µ ′ and covariance Σ ′ , such that: </p><p>(a) DIS (b) RAD </p><p>(c) TAX (d) Estimated CMV </p><formula>µ ′ = f (µ) (4) Σ ′ = J(µ)ΣJ T (µ) </formula><formula>(5) </formula><p>Where J is the Jacobian of the transformation, i.e., </p><formula>J i j = ∂ y i ∂ x i (6) </formula><p>This linearization of the uncertainty propagation (modeled as the covariance) derives from the first order Taylor approximation of f . Alternatives to this method include Montecarlo sampling <ref type="bibr" coords="3,523.28,429.08,13.75,8.12" target="#b13">[15]</ref> , Moment methods <ref type="bibr" coords="3,372.04,439.04,14.94,8.12" target="#b21">[23] </ref>and Polynomial Chaos <ref type="bibr" coords="3,475.23,439.04,13.75,8.12" target="#b29">[31]</ref>, primarily used in risk assessment and engineering uncertainty. In the Taylor series method, used in our paper, the main issue is estimating and representing the sensitivity parameters of the transformation , described next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sensitivity Parameters</head><p>To apply the previous propagation equation, the framework requires to know the Jacobian of the transformations, formed by the partial derivatives of the transformation with respect to its inputs. These are also known as the sensitivity coefficients of the transformation. There are numerous methods for finding sensitivity coefficients <ref type="bibr" coords="3,317.95,558.69,13.75,8.12" target="#b9">[11]</ref> . In this paper, we consider two of them: analytical and linear regression. To compute the sensitivity coefficients analytically, we must know the transformation in its analytic closed form, as a function in terms of the input variables. The derivatives can then be obtained symbolically and applied to the inputs. Another alternative is to approximate the partial derivatives via linear regression. This is obtained by considering the Taylor approximation of an output variable for a number of N samples: </p><formula>y i = y 0 + ∂ y ∂ x (x i − x 0 ) (7) </formula><p>Using linear least squares, we obtain the approximation to the partial derivatives as: </p><formula>∂ y ∂ x ≈ ∑ N i=0 (y i − y 0 )(x i − x 0 ) ∑ N i=0 (x i − x 0 ) 2 (8) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Uncertainty Aggregation</head><p> Data transformations, in general, involve certain error and often result in loss of information. For this reason, transformations themselves aggregate uncertainty to the output variables. Here, we consider the uncertainty of a transformation as Gaussian noise. Therefore , Eq.( 2) can be extended as follows: </p><formula>y = f (x) + e (9) e ∼ N(0, E) (10) </formula><p>where e is an error term, modeled as a Gaussian distribution of zero mean and covariance E. When we account for both propagation and aggregation of uncertainty , the result of applying a transformation results in the un- certainty: </p><formula>Σ ′ = J(µ)ΣJ T (µ) + E (11) </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transformation Uncertainty</head><p> To understand the implications of our framework, we show the analysis of uncertainty for two typical data analysis transformations: projection via Principal Component Analysis, and clustering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Principal Component Analysis (PCA)</head><p> PCA is a projection method that re-expresses a collection of correlated variables into a smaller number of variables called principal components, which maximize the variance of the data. PCA has become an important analysis tool for visual analytics, as Ndimensional data can be projected into a lower dimensional space (typically 2D), which can be represented easily in current display technology. To understand the effects of PCA in input uncertainty, we must perform a sensitivity analysis. Several methods have been proposed before to this purpose <ref type="bibr" coords="4,133.68,399.37,14.19,8.12" target="#b24">[26,</ref><ref type="bibr" coords="4,149.98,399.37,10.65,8.12" target="#b30"> 32]</ref>. Here, we follow a generic approach of multi-dimensional differentiation as described in the previous section. In addition to this propagation, PCA itself adds uncertainty, seen as loss of information as several dimensions (the ones with least variance) are ignored. Let us consider the case of PCA into two dimensions for an m × n matrix X representing m observations of an n-dimensional vector. The PCA projection is a linear transfor- mation: </p><formula>Y = P (k) X (12) </formula><p>where P k is a linear transformation containing the first k principal components of X. A typical projection in 2D uses k = 2. Therefore, the error introduced by this projection can be computed as: </p><formula>E PCA = ||P (2) X − P (n) X|| (13) </formula><p>It can be seen that this is equivalent to </p><formula>E PCA = 1 2 n ∑ i=3 λ i (14) </formula><p>where λ i are the eigenvalues of the covariance matrix resulting of the empirical zero mean data matrix, which summarize the magnitude of the secondary components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Clustering</head><p> Another commonly used transformation is clustering, which arranges data values in a large collection into separate classes that minimize the distance between points in the same class, while maximizing the distance between points in different classes. Methods for clustering include k-means, hierarchical algorithms, locality-and grid-based algorithms <ref type="bibr" coords="4,398.27,271.56,9.52,8.12" target="#b8">[9]</ref> . The k-means algorithm is a greedy algorithm that iteratively assigns data points to the cluster whose centroid is closest <ref type="bibr" coords="4,373.08,291.49,13.75,8.12" target="#b17">[19]</ref> . When the data is uncertain, however, the distance between data points cannot be determined deterministically. Instead, k-means must take into account the variation of the data points. An example is UK-means <ref type="bibr" coords="4,438.81,321.37,9.52,8.12" target="#b5">[6]</ref>, which considers the expected distance to a cluster centroid instead of the actual Euclidean dis- tance. Similar to PCA, clustering introduces error. In general, we can measure the " quality " of the clustering using the total variance that k-means is trying to minimize: </p><formula>E = ∑ k ∑ i∈C k ||x i − µ k || 2 (15) </formula><p>for k clusters with centroid µ k . x i are the data points, classified into the sets C k . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Visual Mapping</head><p>Finally, the uncertainty propagated and aggregated throughout the process is mapped to visual representations. Analogous to the original data, this implies a problem of visualization of multidimensional data, since the uncertainty of the data depends on each of the variables, the intermediate results after data transformations and the data transformations themselves. Following the visual analytics mantra, the visual mapping needs to be multi-functional. On one hand, it should provide an overview of the uncertainty, and on the other hand, it must let analysts gain access to detail information. To achieve the first, we enhance scatter plots of multidimensional data with uncertainty nodes, whose size indicate the magnitude of the uncertainty. Using transparency, we " hide " the effects of uncertainty so that only the most reliable data is highlighted to the user. A different view does the opposite: it enhances the data with higher uncertainty. This is useful for discovering the sources of uncertainty and formulate questions about their distributions. An example is detailed in the following section. For detail information about uncertainty, we explore the use of bar charts (or tornado graphs in Cooke and Noordwijk <ref type="bibr" coords="4,438.53,641.30,13.45,8.12" target="#b22">[24]</ref> ), which depict the contribution of each variable and data transformation in the uncertainty of a given data point. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CASE STUDY</head><p>To test our framework, we used a case study based on the Boston neighborhood housing price data set (BNHP). This data set consists of 14 variables and 506 data records of housing market data in the Boston metropolitan area <ref type="bibr" coords="5,146.25,424.17,13.75,8.12" target="#b11">[13]</ref> . These variables include some structural information such as the number of rooms in a unit (RM) or age of the building (AGE), neighborhood related, such as the proportion of population with lower status (LSTAT), crime rate (CRIM), proportion of nonretail business (INDUS), accessibility variables, such as the distance to five employment centers in Boston (DIS) and accessibility to radial highways (RAD), and an air pollution variable, the concentration of nitrogen oxide (NOX). The data set was collected in an effort to propose a procedural model of the willingness to pay for clean air. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Uncertainty Modeling</head><p> The BNHP dataset consists of 14 variables, which we modeled using mixtures of Gaussians. These variables are depicted in <ref type="figure" coords="5,262.08,571.79,28.60,8.12" target="#fig_1">Figure 3</ref>. We can see that a scatterplot matrix like this makes it difficult to understand the complexity and correlations of multi-variate data, even with interactive capabilities such as zooming and filtering. To better depict this data set, we use principal component analysis to project the 14 dimensions in a 2D plot. To extract the model for each variable, we estimate the probability parameters using Maximum Likelihood criterion using the Expectation-Maximization algorithm <ref type="bibr" coords="5,189.14,654.06,9.52,8.12" target="#b7">[8]</ref>. Examples of the result of GMM modeling for a number of variables in the BNHP dataset is shown in <ref type="figure" coords="5,89.53,673.98,29.46,8.12" target="#fig_0">Figure 2</ref>, including a derived variable. Clearly, a simple Gaussian distribution does not capture the complex shape of their probabilistic distribution. A Gaussian Mixture Model (GMM), consisting of two Gaussian clusters, represents more accurately the uncertainty of these two variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty plot of derived data </head><p>Uncertainty plot showing input sensitivity <ref type="figure" coords="5,317.95,379.06,28.24,7.64">Figure 5</ref>: Uncertainty view of the housing mean value resulting from model fitting. Top: Color indicates the price, lower prices are in blue, while higher prices are in red. Size of a point denotes uncertainty. Note that the results for higher prices are consistently more uncertain than low priced housing. Also note the concentration of these nodes towards one of the ends in the PCA projection. Bottom: To understand the sources of this uncertainty, we map the most important factor that influences it (i.e., the most sensitive parameter). The color coding is defined in <ref type="figure" coords="5,409.41,454.78,28.43,7.64" target="#fig_4">Figure 6</ref>. The largest uncertainty seem to be correlated with the neighborhood variables (LSTAT) while medium and low uncertainties relate to accessibility variables (RAD and DIS). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Fitting Uncertainty</head><p>According to Harrison and Rubinfeld <ref type="bibr" coords="5,457.19,514.16,13.75,8.12" target="#b11">[13]</ref>, it is possible to define the housing value as a nonlinear combination of the 14 input variables , whose parameters can be found using nonlinear regression. Therefore, we can readily find the sensitivity parameters of this derived data with respect to each of the outputs. Examples of these are depicted in <ref type="figure" coords="5,360.41,563.97,29.33,8.12" target="#fig_2">Figure 4</ref>. At the top, we show a traditional scatterplot. This plot does not tell much about the trends in the data. In the bottom , we see a scatterplot augmented with uncertainty information, in the form of line segments. Now the user can clearly see trends in the data. The largest the slope of these segments, the higher the sensitivity . We can clearly see a negative correlation of the estimated mean value MV with respect to the variable DIS (x-axis) and the variable RAD (clustering). Values in the cluster denoted by cyan (high RAD levels) seem to be more uncertain. To visualize the uncertainty, we map the magnitude of the propagated uncertainty to the size of nodes in a 2D scatterplot defined as the PCA projection of the 14 variables. This is shown in <ref type="bibr" coords="6,280.76,244.08,13.29,7.64" target="#b22">[24] </ref>to represent both the magnitude and sign of the sensitivity. Bars to the left indicate a negative sensitivity, while bars to the right indicate a positive sensitivity. more uncertain a data value is, the more transparent is its visual representation. This " hides " the effect of uncertainty and steers the user's attention towards the most reliable data points. With this visualization , we immediately see a correlation of uncertainty with the estimated median value. Highly priced housing seems to carry a lot more uncertainty than low priced housing. We also noted that they appear clustered in a particular region of the projection, suggesting a specific cause for this uncertainty. To understand more the sources of uncertainty, we turn to the sensitivity parameters, discovered in the initial stages of our framework . The most sensitive variable turns out to be NOX (concentration of nitrogen oxides in the air), the only air pollution variable considered in the original study <ref type="bibr" coords="6,173.32,445.52,13.75,8.12" target="#b11">[13]</ref>, which in turn was found to be an important variable and used to measure the willingness to pay for clean air. We then turn to the second most sensitive variable , as depicted in <ref type="figure" coords="6,128.42,475.41,30.36,8.12">Figure 5</ref>-bottom. Here, instead of hiding the effects of uncertainty, we highlight them. Note how the more uncertain nodes are visible and color coded depending on the second most sensitive variable (since the most sensitive is NOX for all of them). We see a correlation of larger uncertainties with the variable LSTAT (the proportion of adults of lower status, in blue), medium uncertainties to RAD (the index of accessibility to radial highways, in cyan), while relatively low uncertainties to DIS (distance to five employment centers in the Boston region, yellow) and RM (number of rooms in owner units, in green). These suggest that more confidence can be attributed to the effect of accessibility variables to the mean housing price, than other neighborhood o structural variables . These results are consistent with the findings in Harrison's original study <ref type="bibr" coords="6,105.80,604.92,13.75,8.12" target="#b11">[13]</ref>. <ref type="figure" coords="6,63.96,624.16,30.17,8.12" target="#fig_4">Figure 6</ref>shows a zoomed-in view of a portion of the scatter plot depicting detail information about the sensitivity parameters. In this case, we use a tornado representation (a bar chart) <ref type="bibr" coords="6,249.28,644.09,14.94,8.12" target="#b22">[24] </ref>to show both the sign and magnitude of the partial derivatives. Again, we see a predominance of the RAD, DIS, LSTAT and NOX variables in the uncertainty of the mean housing value, but we also discover the effects of other variables. We believe that this multi-level approach for exploring uncertainty, from high level overviews (<ref type="figure" coords="6,247.36,693.90,19.83,8.12">Fig. 5</ref> ) to detail information (<ref type="figure" coords="6,117.51,703.87,20.68,8.12" target="#fig_4">Fig 6)</ref>, is essential for making reliable decisions upon the visual analysis of complex data.  of the same data as a scatterplot of the 2 principal components. Color tagging of different clusters helps the analyst understand the dependencies between different variables. Top: Clustering based on NOX variable (air pollution). A simpler visualization that carries more information in a single view can be obtained with uncertainty, as shown in <ref type="figure" coords="6,480.38,291.00,28.13,7.64">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PCA transformation</head><p>PCA is used to reduce the dimensionality of the data set to a 2D plot. An example is shown in <ref type="figure" coords="6,430.95,359.59,29.70,8.12" target="#fig_5">Figure 7</ref> , where we plot the multidimensional data sets along the principal components and use color coding to denote three clusters based on the NOX variable (air pollution ). The visualization also depicts the individual pair-wise scatterplots with each variable in the x and y axes.  , therefore, we obtain a 2D uncertainty for each data point, here represented as ellipses. The x-axis of each ellipse represents uncertainty propagated by the principal component, while the y-axis represents the uncertainty propagated by the secondary component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Clustering</head><p>To understand the effects of clustering in uncertainty, we measure the variance within each cluster for a number of possibilities. Because clustering is an operation that maps from a large set of data points to a small number of classes, uncertainty is only represented as a summary view. <ref type="figure" coords="7,133.54,683.94,26.46,8.12">Fig. 10</ref>shows the stacked histogram of the uncertainty for clustering along different dimensions. The larger the bar, the highest the uncertainty. ters are highly uncertain in relation to others, such as for the CMV and CRIM variables, this suggests that the number of clusters is not necessarily optimal, and the analyst can improve the classification. In other cases, such as for RAD and TAX, the clustering seems to classify the data well. <ref type="figure" coords="7,404.77,416.60,26.47,8.12">Fig. 10</ref>also shows the stacked histogram for 4 clusters. We see that this change improves the uncertainty of clustering for some of the variables, such as CMV, CRIM and PRA- TIO. In general, the optimal number of clusters for each variable can be found using expectation-maximization techniques (EM). In other cases, however, prior information about the data may suggest some expected number of clusters that may not be optimal. Our framework enables the analyst to evaluate the quality of the data transformations. For example, we also show the effect of applying a more appropriate clustering to the data. In <ref type="figure" coords="7,507.48,506.26,33.54,8.12">Figure 10</ref>, we show the uncertainty for variables EMV (result of model fitting) and UEMV, which represents the clustering of EMV using UK-Means <ref type="bibr" coords="7,317.95,536.15,13.75,8.12" target="#b18">[20]</ref> . We can see that, for both 3 and 4 clusters, UK-Means generates a clustering with less variance than the counterpart that does not include uncertainty. With our framework, analysts are able to compare quantitatively the efficacy of their data transformations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LIMITATIONS AND CONCLUSION</head><p>We have presented a general framework for introducing uncertainty in the visual analytics process. We found that mirroring the process of transforming data into insight allows us to define a series of operations on uncertainty, such as modeling, propagation and aggregation , that map input uncertainty to visual representations. We have followed a quantitative approach that models uncertainty as the propagation and aggregation of error in a parametric model of the distribution of data. We believe that this mechanism is useful when the analyst wants to extract a model that explains the behavior of data and helps make projections or extrapolate to different situations. Discrete operations, such as clustering, can also be included in the framework, provided a quantitative measure of uncertainty. More qualitative assessment of the uncertainty is difficult to model in our framework. We believe that our framework can be extended with Bayesian networks to support more general data types. Another aspect of our approach is its scalability. Sensitivity analysis is at the core of the framework, which requires analyzing the effects of every output variable with respect to its inputs. For extreme large data sets, this may be prohibitive. Two solutions to this problem are: (1) either provide a simplification of the data distribution and estimate the sensitivity coefficients with respect to the simplified distribution, or (2) perform uncertainty analysis locally. The former approach is useful for overviews and the latter for detailed views of the uncertainty. The study of uncertainty proves important for understanding the sensitivity of the output with respect to the inputs. On one hand, uncertainty provides a summarized quantity for each data point, which helps the analyst assess the confidence level on the visual representation. For example, overviews of the uncertainty helped us determine a correlation with certain clusters with the confidence level. On the other hand, output uncertainty is a complex multidimensional dataset, which can be further inquired to gain access to detail sensitivity information. We applied common sensitivity visualization tools such as tornado maps and color encodings to show the correlation between uncertainty and specific variables in a multi-dimensional data set. We believe that a similar mapping can be obtained to other common visualizations, such as parallel coordinates and radar views. With the use of general methods such as Gaussian Mixture Models, statistical linearization of sensitivity parameters and uncertainty propagation, we have a framework that can be adapted to a wide variety of probability distributions and data transformations. Although the case study shown in this paper focuses on model fitting and principal component analysis, our approach can be followed to extend other data transformations, such as binning, multidimensional scaling and self-organizing maps, to account for their uncertainty. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="3,317.95,239.12,240.06,7.64;3,317.95,248.58,240.04,7.64;3,317.95,258.05,240.04,7.64;3,317.95,267.51,240.04,7.64;3,317.95,276.97,240.05,7.64;3,317.95,286.44,240.04,7.64;3,317.95,295.90,104.06,7.64"><head>Figure 2: </head><figDesc> Figure 2: Modeling the uncertainty of variables using Gaussian Mixture Models (GMM) for 4 different variables. The first two (a-b) are accessibility variables, (c) is a neighborhood variable and (d) is a derived variable as the model fitting of the 14 variables. A Gaussian model (red) fails to capture the different peaks and misrepresents the uncertainty of the distribution. A GMM models more closely the actual distribution of the data. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="4,317.95,226.47,240.05,7.64;4,317.95,235.94,240.04,7.64;4,317.95,245.41,99.55,7.64"><head>Figure 3: </head><figDesc>Figure 3: BNHP Dataset, represented here as a scatterplot matrix. Clearly, the high dimensionality makes it difficult to understand and find meaningful correlations. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="5,54.00,346.10,240.04,7.64;5,54.00,355.56,240.04,7.64;5,54.00,365.03,240.04,7.64;5,54.00,374.49,240.04,7.64;5,54.00,383.96,240.04,7.64;5,54.00,393.42,73.48,7.64"><head>Figure 4: </head><figDesc>Figure 4: Derivative Visualization. To show the derivatives, we use a linear trend line for each sample value. The overall shape indicates the general direction and expected variation of a data point. Color coding denotes a clustering with respect to the variable RAD. Compare a traditional scatterplot (top) with one augmented with derivative information (bottom). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="5,543.05,673.98,14.95,8.12;5,317.95,683.95,240.05,8.12;5,317.95,693.91,240.03,8.12;5,317.95,703.87,240.05,8.12;5,317.95,713.84,240.05,8.12"><head></head><figDesc>Figure 5. On top, we use color to encode the value of the output variable, resulting from the model fitting of the 14 input variables. Red nodes denote high housing prices while blue nodes denote low prices. Transparency also encodes the degree of uncertainty. The </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4" coords="6,54.00,234.61,240.04,7.64;6,54.00,244.08,240.04,7.64;6,54.00,253.54,240.04,7.64;6,54.00,263.01,240.04,7.64;6,54.00,272.46,71.65,7.64"><head>Figure 6: </head><figDesc> Figure 6: Detail uncertainty information. The different sensitivity parameters are shown for each data point as a bar or tornado chart [24] to represent both the magnitude and sign of the sensitivity. Bars to the left indicate a negative sensitivity, while bars to the right indicate a positive sensitivity. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="6,317.95,243.68,240.05,7.64;6,317.95,253.14,240.06,7.64;6,317.95,262.61,240.04,7.64;6,317.95,272.08,240.04,7.64;6,317.95,281.54,240.05,7.64;6,317.95,291.00,193.88,7.64"><head>Figure 7: </head><figDesc> Figure 7: A summarized representation of the same data as a scatterplot of the 2 principal components. Color tagging of different clusters helps the analyst understand the dependencies between different variables. Top: Clustering based on NOX variable (air pollution). A simpler visualization that carries more information in a single view can be obtained with uncertainty, as shown in Figure 5. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6" coords="7,54.00,339.24,240.04,7.64;7,54.00,348.69,240.04,7.64;7,54.00,358.16,240.05,7.64;7,54.00,367.63,45.19,7.64"><head>Figure 8: </head><figDesc> Figure 8: PCA Sensitivity to a given variable (LSTAT). Color coding denotes a clustering with respect to the variable NOX. Note that the sensitivity analysis shows us a critical region, where derivatives change sign. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,54.00,552.30,240.04,7.64;7,54.00,561.76,240.04,7.64;7,54.00,571.23,240.05,7.64;7,54.00,580.70,240.04,7.64;7,54.00,590.16,240.02,7.64;7,54.00,599.62,104.62,7.64"><head>Figure 9: </head><figDesc>Figure 9: Uncertainty Propagation of PCA. Each component of the PCA projection propagates uncertainty, therefore, we obtain a 2D uncertainty for each data point, here represented as ellipses. The x-axis of each ellipse represents uncertainty propagated by the principal component, while the y-axis represents the uncertainty propagated by the secondary component. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,317.95,275.16,240.04,7.64;7,317.95,284.63,240.04,7.64;7,317.95,294.10,240.04,7.64;7,317.95,303.56,240.04,7.64;7,317.95,313.03,240.05,7.64;7,317.95,322.49,240.04,7.64;7,317.95,331.95,240.05,7.64;7,317.95,341.42,240.04,7.64;7,317.95,350.88,39.25,7.64"><head></head><figDesc>Figure 10: Uncertainty of Clustering. Each bar represents the total variance of the clustering operation for each variable, including the two principal components of the data. Within each bar, the size of each color represents the uncertainty of each cluster. The first and second rows show the uncertainty for 3 and 4 clusters, respectively. The last row shows the difference in uncertainty. A negative difference (green) indicates an improvement of uncertainty. This summarized view helps the analyst evaluate the efficacy of the data trans- formations. </figDesc></figure>

			<note place="foot">To model the uncertainty propagated by the PCA transformation, we estimated the sensitivity parameters via linear regression. Because we want to observe local changes in PCA transformation according to the different input variable, we used moving least squares instead of the total least squares method in Section 3.2.1. In the moving least squares sense, the derivative of PCA with respect to a variable is computed only in a neighborhood around each data point. An example is shown in Figure 8, which depicts the sensitivity of the second principal component with respect to the input variable LSTAT. The color denotes a clustering with respect to another variable (NOX), which suggests a correlation between this sensitivity and the other variable. Notice the presence of critical points, where the sensitivities can become positive or negative. As a data point moves around this critical region, the variable changes from influencing positively the PCA projection to negative influence. Once we have estimated the sensitivity parameters, we can estimate the uncertainty propagated by the PCA transformation. The 2D projection leads to a 2D uncertainty estimate for each data point, which can be represented as ellipses. A ellipse elongated along a given dimension, i.e., horizontal or vertical, shows a larger uncertainty due to the principal or secondary component of the projection , respectively. The uncertainty visualization is depicted in Figure 9. Notice that there is a predominance of uncertainty due to the secondary component, and that there is a spatial consistency in the uncertainty estimates. Unlike Figure 5, there is no apparent correlation between high housing prices and propagated uncertainty. When combined with the uncertainty propagated by the model fitting, this framework helps analysts to quantify and assess their confidence of, not only the input data and models, but also the data transformations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS </head><p>This research was supported in part by the U.S. National Science Foundation through grants CCF-0938114, CCF-0808896 and CCF- 0811422 and by Hewlett-Packard Laboratories. We also thank Nokia Research Center for their support. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="8,72.26,477.90,221.78,7.22;8,72.26,487.36,221.78,7.22;8,72.26,496.83,17.94,7.22"  xml:id="b0">
	<monogr>
		<title level="m" type="main">Multivariate visual explanation for high dimensional datasets</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Barlowe</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Jacobs</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2008-10" />
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,506.30,221.78,7.22;8,72.26,515.76,145.34,7.22"  xml:id="b1">
	<analytic>
		<title level="a" type="main">Survey of clustering data mining techniques</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Berkhin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accrue Software</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,525.23,221.78,7.22;8,72.26,534.69,104.94,7.22"  xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical Model-Building and Response Surfaces</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Box</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Draper</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,544.15,221.79,7.22;8,72.26,553.62,100.53,7.22"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Multidimensional scaling</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Carroll</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Arabie</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Psychology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="607" to="649" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,563.08,221.79,7.22;8,72.26,572.55,221.79,7.22;8,72.26,582.01,221.79,7.22;8,72.26,591.47,33.88,7.22"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Sensitivity analysis of model output: variance-based methods make the difference</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Saltelli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tarantola</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSC &apos;97: Proceedings of the 29th conference on Winter simulation</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,600.94,221.78,7.22;8,72.26,610.40,221.78,7.22;8,72.26,619.91,221.78,7.09;8,72.26,629.33,92.54,7.22"  xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncertain data mining: An example in clustering location data</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Kao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th Pacific- Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>. of the 10th Pacific- Asia Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,638.80,221.78,7.22;8,72.26,648.26,221.79,7.22;8,72.26,657.76,221.79,7.09;8,72.26,667.19,221.60,7.22"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation algorithms for clustering uncertain data</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Cormode</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Mcgregor</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS &apos;08: Proceedings of the twentyseventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,676.66,221.78,7.22;8,72.26,686.12,221.78,7.22;8,72.26,695.59,195.62,7.22"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">M</forename>
				<surname>Laird</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Rubin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,72.26,705.05,221.77,7.22;8,72.26,714.51,189.77,7.22;8,317.95,58.48,240.04,7.22;8,336.22,67.94,221.79,7.22;8,336.22,77.40,17.94,7.22"  xml:id="b8">
	<monogr>
		<title level="m" type="main">A visual analytics framework for feature and classifier engineering Master&apos;s thesis Applied Regression Analysis (Wiley Series in Probability and Statistics)</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Dolfing</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">R</forename>
				<surname>Draper</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Smith</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>John Wiley &amp; Sons Inc</publisher>
		</imprint>
	</monogr>
	<note>sub. edition</note>
</biblStruct>

<biblStruct coords="8,336.22,86.87,221.78,7.22;8,336.22,96.33,147.88,7.22"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Identification and review of sensitivity analysis methods</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Frey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Patil</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Risk Analysis</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="553" to="578" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,105.80,221.78,7.22;8,336.22,115.27,17.94,7.22"  xml:id="b10">
	<monogr>
		<title level="m" type="main">Reasoning about Uncertainty</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">Y</forename>
				<surname>Halpern</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2003-10" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,124.73,221.79,7.22;8,336.22,134.19,221.79,7.22;8,336.22,143.65,114.95,7.22"  xml:id="b11">
	<analytic>
		<title level="a" type="main">Hedonic housing prices and the demand for clean air</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">J</forename>
				<surname>Harrison</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">L</forename>
				<surname>Rubinfeld</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Environmental Economics and Management</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="1978-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,153.12,221.79,7.22;8,336.22,162.59,49.15,7.22"  xml:id="b12">
	<monogr>
		<title level="m" type="main">Generalized Additive Models</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,172.05,221.79,7.22;8,336.22,181.52,221.79,7.22;8,336.22,190.98,221.77,7.22;8,336.22,200.44,69.30,7.22"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Propagation of uncertainty in risk assessments: The need to distinguish between uncertainty due to lack of knowledge and uncertainty due to variability</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<forename type="middle">O</forename>
				<surname>Hoffman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Hammonds</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Risk Analysis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="707" to="712" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,209.91,221.78,7.22;8,336.22,219.37,221.79,7.22;8,336.22,228.84,194.17,7.22"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Managing uncertainty in spatial databases: Putting theory into practice</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Hunter</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Goodchild</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Urban and Regional Information Systems Association</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="62" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,238.31,221.79,7.22;8,336.22,247.76,221.78,7.22;8,336.22,257.23,170.10,7.22"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Challenges in visual data analysis</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schneidewind</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ziegler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV &apos;06: Proceedings of the conference on Information Visualization</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,266.69,221.78,7.22;8,336.22,276.16,138.72,7.22"  xml:id="b16">
	<monogr>
		<title level="m" type="main">Uncertainty Analysis with High Dimensional Dependence Modeling</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Kurowicka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cooke</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,285.63,221.79,7.22;8,336.22,295.09,221.78,7.22;8,336.22,304.59,221.79,7.09;8,336.22,314.01,221.79,7.22"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">B</forename>
				<surname>Macqueen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<editor>L. M. L. Cam and J. Neyman</editor>
		<meeting>. of the fifth Berkeley Symposium on Mathematical Statistics and Probability</meeting>
		<imprint>
			<publisher>University of California Press</publisher>
			<date type="published" when="1967" />
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,323.48,221.78,7.22;8,336.22,332.95,207.87,7.22"  xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient clustering of uncertain data</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">K</forename>
				<surname>Ngai</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Kao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">K</forename>
				<surname>Chui</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Cheng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Yip</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2006-12" />
			<biblScope unit="page" from="436" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,342.41,221.79,7.22;8,336.22,351.88,206.64,7.22"  xml:id="b19">
	<monogr>
		<title level="m" type="main">Approaches to uncertainty visualization. The Visual Computer</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Pang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">M</forename>
				<surname>Wittenbrink</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">K</forename>
				<surname>Lodha</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="370" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,361.34,221.78,7.22;8,336.22,370.80,221.80,7.22;8,336.22,380.27,37.87,7.22"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Visualisation of fuzzy systems: requirements, techniques and framework</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Pham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Brown</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1199" to="1212" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,389.73,221.79,7.22;8,336.22,399.20,221.78,7.22;8,336.22,408.67,221.78,7.22;8,336.22,418.13,77.93,7.22"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Approach for uncertainty propagation and robust design in cfd using sensitivity derivatives</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">M</forename>
				<surname>Putko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">A</forename>
				<surname>Newman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">C T</forename>
				<surname>Iii</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">L</forename>
				<surname>Green</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA 15th Computational Fluid Dynamics Conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="2001" to="2528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,427.59,221.79,7.22;8,336.22,437.05,221.78,7.22;8,336.22,446.52,57.34,7.22"  xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized graphical methods for uncertainty and sensitivity analysis</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">R M</forename>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<forename type="middle">N J M</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bashkir Ecological Journal, (Special Issue)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="54" to="57" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,455.99,221.79,7.22"  xml:id="b23">
	<monogr>
		<title level="m" type="main">A tutorial on principal component analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Shlens</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,465.45,221.78,7.22;8,336.22,474.92,221.80,7.22;8,336.22,484.38,86.13,7.22"  xml:id="b24">
	<analytic>
		<title level="a" type="main">Recent advance in sensitivity analysis in multivariate statistical methods</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Tanaka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Japanese Society of Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,493.84,221.79,7.22;8,336.22,503.31,221.79,7.22;8,336.22,512.77,103.94,7.22"  xml:id="b25">
	<monogr>
		<title level="m" type="main">Guidelines for evaluating and expressing the uncertainty of NIST measurement results</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">N</forename>
				<surname>Taylor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">E</forename>
				<surname>Kuyatt</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,522.24,100.29,7.22"  xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Thompson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<surname>Sampling</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,531.71,221.79,7.22;8,336.22,541.16,221.79,7.22;8,336.22,550.63,221.78,7.22;8,336.22,560.09,221.78,7.22;8,336.22,569.56,17.94,7.22;8,317.96,577.17,31.14,9.08;8,345.55,579.03,212.44,7.22"  xml:id="b27">
	<analytic>
		<title level="a" type="main">A typology for visualizing uncertainty</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thomson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Hetzler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Maceachren</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Gahegan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Pavel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visualization and Data Analysis 2005. Proceedings of the SPIE</title>
		<editor>R. F. Erbacher, J. C. Roberts, M. T. Gröhn, and K. Börner</editor>
		<imprint>
			<date type="published" when="2005-03" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
	<note>[. 30] V. ˇ Smídl and A. Quinn. On bayesian principal component analysis</note>
</biblStruct>

<biblStruct coords="8,336.22,588.49,161.85,7.22"  xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput. Stat. Data Anal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4101" to="4123" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,597.96,221.79,7.22;8,336.22,607.41,82.14,7.22"  xml:id="b29">
	<analytic>
		<title level="a" type="main">The homogeneous chaos</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Wiener</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="897" to="936" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,616.88,221.79,7.22;8,336.22,626.35,221.81,7.22;8,336.22,635.81,17.94,7.22"  xml:id="b30">
	<analytic>
		<title level="a" type="main">Sensitivity analysis in functional principal component analysis</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yamanishi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Tanaka</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="326" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,645.28,221.78,7.22;8,336.22,654.74,221.79,7.22;8,336.22,664.20,216.18,7.22"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis guided visual exploration of multivariate data</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">A</forename>
				<surname>Rundensteiner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">O</forename>
				<surname>Ward</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Analytics Science and Technology</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="83" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,673.67,221.79,7.22;8,336.22,683.17,221.79,7.09;8,336.22,692.60,125.53,7.22"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Interval based uncertain reasoning. Fuzzy Information Processing Society</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Yao</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAFIPS. 19th International Conference of the North American</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="363" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="8,336.22,702.07,221.79,7.22;8,336.22,711.53,169.01,7.22"  xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualization of uncertainty and reasoning</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Zuk</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">S T</forename>
				<surname>Carpendale</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Smart Graphics</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
