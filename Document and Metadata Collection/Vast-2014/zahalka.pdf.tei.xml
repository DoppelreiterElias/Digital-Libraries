<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/pisenberg/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.4.1" ident="GROBID" when="2016-09-09T13:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Interactive, Intelligent, and Integrated Multimedia Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName>
								<forename type="first">Jan</forename>
								<surname>Zahálka</surname>
							</persName>
						</author>
						<author>
							<persName>
								<forename type="first">Marcel</forename>
								<surname>Worring</surname>
								<roleName>Senior Member, Ieee</roleName>
							</persName>
						</author>
						<title level="a" type="main">Towards Interactive, Intelligent, and Integrated Multimedia Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>—The size and importance of visual multimedia collections grew rapidly over the last years, creating a need for sophisticated multimedia analytics systems enabling large-scale, interactive, and insightful analysis. These systems need to integrate the human&apos;s natural expertise in analyzing multimedia with the machine&apos;s ability to process large-scale data. The paper starts off with a comprehensive overview of representation, learning, and interaction techniques from both the human&apos;s and the machine&apos;s point of view. To this end, hundreds of references from the related disciplines (visual analytics, information visualization, computer vision, multimedia information retrieval) have been surveyed. Based on the survey, a novel general multimedia analytics model is synthesized. In the model, the need for semantic navigation of the collection is emphasized and multimedia analytics tasks are placed on the exploration-search axis. The axis is composed of both exploration and search in a certain proportion which changes as the analyst progresses towards insight. Categorization is proposed as a suitable umbrella task realizing the exploration-search axis in the model. Finally, the pragmatic gap, defined as the difference between the tight machine categorization model and the flexible human categorization model is identified as a crucial multimedia analytics topic. Index Terms—Multimedia (image/video/music) visualization, machine learning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years marked a rapid growth of importance and volume of multimedia data. An increasing number of scientific fields, such as physics, biology, and astronomy, utilize scientific imagery to advance the state of the art <ref type="bibr" coords="1,123.30,315.73,13.74,8.02" target="#b40">[41]</ref> . Modern medical science also relies increasingly on multimedia datasets, which greatly assist physicians in diagnosis . In the non-scientific world, the advent of smartphones made devices with good multimedia recording capabilities ubiquitous, resulting in the general public contributing large amounts of social media shared on immensely popular sites like Flickr or Facebook. Such social media bring resources for social sciences and media companies . The abundance of public multimedia data also provides police, intelligence services, and forensics with a major information source for investigation of felonies like child abuse or terrorism. As shown by these examples, multimedia datasets provide a wealth of resources and tremendous potential for knowledge gain in a wide spectrum of application areas. Being able to gain insight into multimedia datasets is thus of paramount importance. However, with current multimedia collections easily comprising millions of images and months of video, it is no longer feasible to have multimedia datasets analyzed by humans only. Sophisticated systems to assist the human analyst in assessing multimedia datasets are needed, and despite the increasing importance of multimedia in our society, few such systems exist. Multimedia analytics, an emerging field combining visual analytics and multimedia analysis, focuses on creating systems for largescale multimedia analysis <ref type="bibr" coords="1,150.41,525.66,13.74,8.02" target="#b12">[13]</ref> . Visual analytics, the science of analytical reasoning facilitated by interactive visual interfaces <ref type="bibr" coords="1,271.84,535.63,13.74,8.02" target="#b76">[78]</ref>, has been successfully applied in diverse fields since its inception in 2005. Multimedia analysis, the other component of multimedia analytics, is an umbrella term for many different automatic multimedia analysis techniques. In this paper, we focus on visual multimedia collections (images/videos) with associated data sources like text annotations and metadata, making the fields of computer vision, image retrieval, and video retrieval our focus prism <ref type="bibr" coords="1,165.61,605.37,14.27,8.02" target="#b71">[73]</ref><ref type="bibr" coords="1,179.88,605.37,14.27,8.02" target="#b73">[75]</ref>. Multimedia analytics aims to guide the analyst to deep insight. They aim to combine the analyst's natural expertise in analyzing multimedia information (humans ter this skill shortly after birth) with the memory and processing power of the machines, which are able to process contemporary large-scale collections. True to the visual analytics spirit embodied by the visual analytics process diagram by Keim et al. <ref type="bibr" coords="1,475.95,314.69,14.27,8.02" target="#b39">[40]</ref><ref type="bibr" coords="1,490.22,314.69,14.27,8.02" target="#b40">[41]</ref>, this integration needs to be interactive. Multimedia analytics also need to be able to utilize the heterogeneous data sources within a collection, combining the visual content with text annotations and string/numeric metadata. The aims of multimedia analytics are ambitious, and the integration of relevant techniques to fulfill these aims is far from trivial. The difficulty of fulfilling the multimedia analystics ambitions stems from multimedia collections being quite a specific data source. While humans perceive multimedia chiefly through the semantic properties of the visual content, machines need a mathematical representation , which does not provide comparable semantic richness and is unintuitive for a human. The heterogeneity of data sources within a multimedia collection also provides a challenge, both for the visualization and the model. Moreover, any one multimedia data instance has a much higher information bandwidth and also size than an instance in a classic dataset. This results in a larger computational load on the machine, and techniques related to multimedia analytics need to be carefully examined before adaptation. In this paper, we provide a comprehensive overview of related stateof-the-art techniques and theory: in Section 2, we focus on the human perception of multimedia; Section 3 reviews machine processing of multimedia data. To this end, we processed the relevant literature in the following four steps: 1. Exhaustive search on articles since 2003 in leading journals and conference proceedings → thousands of references 2. Filtering abstracts and titles based on topical relevance, i.e., papers concerning multimedia analytics, visual analytics theory, multimedia visualization or multimedia analysis → ∼800 ref- erences 3. Topical-relevance filtering based on the key sections of the content → ∼370 references available online <ref type="bibr" coords="1,486.21,645.38,14.94,8.02" target="#b94">[96] </ref> 4. Final filtering based on topical relevance of the complete content and on the impact of the article in the respective community (measured by Google Scholar citations) → ∼100 references in this paper </p><p>In Section 4, we synthesize the relevant techniques into a general multimedia analytics model, which is, to the best of our knowledge, still largely missing in the literature. Section 5 concludes the paper. <ref type="figure" coords="2,45.00,227.02,19.02,7.37">Fig. 1</ref> . Representing multimedia: a multimedia collection comprises individual items, which are composed of content (C), annotations (A), and metadata (M). Thick arrows depict data transformations into derived values (white text, black background): features (F), (dis)similarity (S) and statistics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3"></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HUMAN PERCEPTION OF MULTIMEDIA</head><p> The human's excellent capability to analyze multimedia heavily revolves around being able to see the multimedia items in question. Hence, this section is focused on visualization techniques and analytic interfaces. Issues related to the human processing of multimedia data are outlined in Section 2.1. The text focuses on issues related to data representation and the technical aspects of multimedia visualization . Other important issues, whose thorough description is beyond the scope of this paper, also exist: examples include the cognitive aspects of image understanding <ref type="bibr" coords="2,132.23,387.23,10.45,8.02" target="#b3">[4] </ref>and relevance judgment <ref type="bibr" coords="2,232.81,387.23,14.93,8.02" target="#b29">[30] </ref>by the brain, quality of the data <ref type="bibr" coords="2,112.79,397.19,13.74,8.02" target="#b27">[28]</ref>, or performance issues. Multimedia browsers, the interactive interfaces used to access multimedia collections, are surveyed in Section 2.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Representation and learning</head><p>When working with multimedia data, we are initially provided with a multimedia collection, structurally depicted in <ref type="figure" coords="2,217.71,457.47,29.18,8.02">Figure 1</ref>. Multimedia collections comprise individual raw multimedia items, i.e., single images and/or video clips. Individual items consist of three elements — content, annotations, and metadata. The visual information present in the item and conveyed by it is the item's content. Those individual pieces of visual information carrying an objective semantic meaning, for example " cat " or " dog, " are called semantic concepts. Annotations are text descriptors such as labels, captions, or tags assigned to individual items by a human which relate to the content in an objective or subjective manner. Metadata are string or numeric descriptors related to the technical parameters of the item, such as Exif information or GPS localization. Humans are excellent multimedia analysts by nature, trained to process high-bandwidth visual information through the visual cortex since their early days. Humans can extract semantic information directly from raw multimedia. Then, they synthesize it into concepts and similarity measures with complexity ranging from purely visual-based through visually-semantic (i.e., representing semantics grounded in visual characteristics) to completely abstract <ref type="bibr" coords="2,213.13,636.80,13.74,8.02" target="#b86">[88]</ref> . This entire process is remarkably fast. The reasoning about the data and its structure is heavily context-and intent-based. To illustrate, consider a toy image collection containing a picture of a British phone booth, a picture of Little Red Riding Hood, and a picture of Queen Elizabeth II. The notion of similarity (and thus structure) can be based on the " amount of red colour, " " person, " and " United Kingdom " characteristics, corresponding to a visual-based, visually-semantic, and abstract similarity notion, respectively. Each conveys different structure and none of them is inherently wrong. This learning and reasoning process shows that humans do not perceive multimedia content in a mathematical Visualization Efficiency Navigability Heterogeneous  manner. Mathematical summaries and statistics are thus less prominent than in the case of classic visual analytics approaches. Indeed, Santini and Jain show that modelling multimedia similarities mathematically is rather treacherous <ref type="bibr" coords="2,418.97,231.42,13.74,8.02" target="#b69">[70]</ref>. However, they are still useful for annotations, metadata, and simple visual statistics like the amount of a certain colour. The main limitation of humans is limited cognitive ca- pacity <ref type="bibr" coords="2,331.57,261.31,13.74,8.02" target="#b28">[29]</ref>, barring any attempt at brute-force analysis of a large-scale collection containing million images or more. Analytic interfaces thus need to be semantically navigable to support the human's reasoning process. Since a human needs to see the items in question to derive information from them, the most natural visualization paradigm is directly displaying the items or their respective thumbnails on the screen. Using primitives and conventional charts for visual content is in principle possible, but quite unorthodox: the visual content, a powerful information channel, is lost, diminishing the ability to discriminate between content classes <ref type="bibr" coords="2,362.93,361.65,13.74,8.02" target="#b23">[24]</ref>. Direct visualization has large demands for screen space. The human needs to see the content, so each thumbnail needs to be big enough and occlusion should be minimized. As many items as possible should be displayed on the screen. Analytic interfaces thus need to be screen-space-efficient, minimizing unused screen space. Annotations and metadata might also provide a valuable information gain. They should be visualized in an integrated manner within an analytic interface. </p><formula>Basic grid + − − Similarity space − ++ − Similarity-based + + − Spreadsheet * * ++ Thread − + + </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimedia visualization</head><p>In the current big data era, even the most extravagantly large screens cannot contain entire large-scale collections, and even if they could, the displayed data exceeds the human's cognitive limit. Interactivity is thus crucial for any analytic interface. The classic approach towards visualizing multimedia is the multimedia browser, a prime example of a casual information visualization tool <ref type="bibr" coords="2,459.91,516.53,13.74,8.02" target="#b66">[67]</ref>. Multimedia browsers are used daily by a large number of computer users to examine multimedia collections. Examples include Internet image search interfaces like Google Images or Bing Images and social sites like Flickr or Instagram . Based on the conducted survey, we grouped the state of the art into five groups based on the visual paradigm used. While this grouping is necessarily non-exhaustive, it helps identifying the suitability of state-of-the-art multimedia browsers for different tasks. The main multimedia visualization techniques discussed in this section are conceptually depicted in <ref type="figure" coords="2,385.98,606.19,29.49,8.02" target="#fig_1">Figure 2</ref>, their analytic aspects are summarized in <ref type="figure" coords="2,316.84,616.15,25.30,8.02" target="#tab_1">Table 1</ref>. The most prevailing multimedia collection visualization is a twodimensional grid of thumbnails. The grid is navigated by scrolling, usually one-dimensional and vertical. The user interacts with the thumbnails, usually magnifying the image and revealing annotations and/or metadata in a side panel. A grid is quite efficient in utilizing screen space: except for a side panel (if present), all of it is devoted to displaying individual items with zero overlap. A grid browser may also feature sorting, filtering and/or hierarchical display. In a basic grid, these interactions rely only on non-semantic attributes like file name or time. This makes the semantic exploration capability of a basic grid difficult. This problem is tackled by approaches which order the items in the grid according to a similarity measure. These ap-proaches are discussed below as " similarity-based. " Overall, a grid is certainly a very familiar, screen-space efficient, and usable interface. Its strong suit are tasks where viewing the full content of the item is imperative: high screen-space efficiency ensures high visibility of the thumbnails, and the user can typically magnify the item of interest and inspect it. One of the issues are the lack of integration of annotations and metadata, which are displayed separately in the side panel. Also, in the case of the basic, non-similarity based grid, semantic navigation of large collections is limited. A basic grid is thus unsuitable for tasks where the overall structure of the collection is of importance. Another approach is the similarity space browser. The items' position on the screen is determined by a projection from the highdimensional feature space to the 2-D screen space. The projection should preserve the similarity and the resulting structure present in the feature space as accurately as possible <ref type="bibr" coords="3,199.05,194.72,14.27,8.02" target="#b59">[60]</ref><ref type="bibr" coords="3,213.32,194.72,14.27,8.02" target="#b68">[69]</ref> . Examples of similarity space browsers are numerous, including the browser by Liu et al. <ref type="bibr" coords="3,68.06,214.65,13.74,8.02" target="#b50">[51]</ref>, the semantic image browser by Yang et al. <ref type="bibr" coords="3,251.64,214.65,13.74,8.02" target="#b90">[92]</ref>, the news video databases browser by Luo et al. <ref type="bibr" coords="3,194.68,224.61,13.74,8.02" target="#b53">[54]</ref> , or the Flickr summarization browser by Fan et al. <ref type="bibr" coords="3,157.45,234.57,13.74,8.02" target="#b22">[23]</ref>. Similarity space browsers convey a notion of structure, making them excellently navigable. Scalability and screen space efficiency are an issue, however: some parts of the screen space are empty, and thus wasted; some parts may be cluttered with overlapping thumbnails. Yang et al. tackled this problem using miniature thumbnails <ref type="bibr" coords="3,132.98,284.38,13.74,8.02" target="#b90">[92]</ref>. These display the distribution of colour in the collection quite well, but lack the expressiveness to convey other visual content. Fan et al. use representative images to represent a semantic similarity neighbourhood <ref type="bibr" coords="3,175.06,314.27,13.74,8.02" target="#b22">[23]</ref>. Annotations and metadata are typically displayed in a side panel, making the integration rather poor. However, in some systems, annotations and metadata are used as a basis for the entire similarity structure, for example in Chronosphere by Worring et al. <ref type="bibr" coords="3,123.45,354.12,13.74,8.02" target="#b87">[89]</ref> . Overall, similarity space browsers are useful as a semantics-conveying paradigm due to them directly showing data structure. This makes them an excellent choice for applications focused on uncovering the structure of the collection. However, the screen space and scalability issues make it rather difficult to inspect individual items of interest. Similarity-based approaches utilize a space-filling view (typically a grid or a treemap) where the items are arranged based on a similarity measure. <ref type="bibr" coords="3,88.86,435.68,47.38,8.02">Rodden et al. </ref>showed that arranging similar images together in the grid indeed helps to determine the clusters in the collection, but also somewhat hampers serendipitous discovery, due to interesting images standing out less <ref type="bibr" coords="3,160.22,465.57,13.74,8.02" target="#b68">[69]</ref>. Bederson's PhotoMesa adapts the treemap algorithm to establish similarity <ref type="bibr" coords="3,205.78,475.53,9.52,8.02" target="#b2">[3]</ref>. PhotoMesa's interface is zoomable, which along with the grouping provides overview of the collection and its structure. Zavesky et al. map image features of interest into a 2D abstraction layer based on similarity and then snap the images to a grid, forming visual islands <ref type="bibr" coords="3,204.00,515.38,13.74,8.02" target="#b95">[97]</ref>. Visual islands can be further used for guided navigation: items of interest can be used as probes to rearranging the visual islands. Quadrianto et al. introduce a multiple-tier semantic hierarchy <ref type="bibr" coords="3,171.33,545.27,13.74,8.02" target="#b67">[68]</ref>, with the grid in each successive tier showing representative images for an increasingly detailed neighbourhood of interest. The approach by <ref type="bibr" coords="3,198.88,565.20,73.95,8.02">Brivio et al. utilizes </ref>Voronoi diagrams to fill the display, with the focus item placed in the center and other items being represented on the screen based on their distance to the focus with respect to an ordering of the collection <ref type="bibr" coords="3,257.10,595.09,9.52,8.02" target="#b6">[7]</ref>. Overall, similarity-based approaches combine screen space efficiency with an enhanced notion of structure through visually contiguous regions. This makes them easily navigable and suitable for both structure overview and inspection of individual items. Annotations and metadata are typically still relegated to a side panel, lacking true integration. In the spreadsheet paradigm, rows and columns represent different dimensions of annotations and metadata, and cells display individual items. The first browser based on the spreadsheet paradigm is Photo- Spread by Kandel et al. <ref type="bibr" coords="3,141.57,686.61,14.94,8.02" target="#b38">[39] </ref>MediaTable by de Rooij et al. visualizes each item in one row, with its content, annotations, and metadata forming individual columns <ref type="bibr" coords="3,138.48,706.53,13.74,8.02" target="#b19">[20]</ref>. Multimedia Pivot Tables by Worring and Koelma allow the analyst to define the rows and columns of the table herself <ref type="bibr" coords="3,81.36,726.46,13.74,8.02" target="#b88">[90]</ref>. Navigability and screen space efficiency are dependent on the role of annotations and metadata in the task at hand. If those Basic grid Similarity space Similarity-based Spreadsheet Thread-based are missing or their role is marginal, then a spreadsheet browser is a poor choice, since only few columns will be filled and informative. When annotations and metadata are crucial, however, the spreadsheet provides a very screen-space-efficient way to explore the collection. Other approaches centered on annotations and metadata are driven by specific types of metadata, especially geographic location. Examples include the browser summarizing photographs based on their location by Jaffe et al. <ref type="bibr" coords="3,372.89,291.89,14.94,8.02" target="#b35">[36] </ref>or building a 3D scene of the location from the corresponding photos, as conceived by Szeliski et al. <ref type="bibr" coords="3,513.89,301.85,14.94,8.02" target="#b74">[76] </ref>Out of all the groups mentioned in this paper, spreadsheets emphasize and integrate annotations and metadata the best, making it a great choice for tasks with heavy involvement of annotations and metadata. Moreover, the individual items are easily inspectable. The spreadsheet paradigm, however, falls short when the focus of the task is purely or mostly visual content. Another approach is the thread browser, where the collection can be navigated along threads, i.e., sequences of items based on a certain criterion. Originally, this paradigm was used by de Rooij et al. for navigating large video collections <ref type="bibr" coords="3,440.61,402.02,13.74,8.02" target="#b16">[17]</ref>, where threads were temporal, semantic or based on annotations and metadata ordering. In a thread browser, the interface displays the focus item at the center of the screen and a number of threads relevant to the focus. The user can then navigate along a thread of choice, shifting the focus to the item along the chosen thread. The user is thus able to navigate the collection without requiring her to overview large number of items at once or search manually. Multiple browsers implement this paradigm, the difference lies in the number of threads displayed at once: the CrossBrowser displays two, the ForkBrowser five <ref type="bibr" coords="3,438.29,491.68,13.74,8.02" target="#b17">[18]</ref>, the RotorBrowser up to eight <ref type="bibr" coords="3,316.62,501.65,13.74,8.02" target="#b16">[17]</ref> . The thread browser's design is centered around semantic navigation , which makes it excellent for tasks where inspecting individual items based on semantic dimensions is imperative. Annotation-and metadata-based threads also integrate the heterogeneous data channels seamlessly. Screen-space efficiency is the main issue of the thread browser: only a limited number of threads can be displayed in order not to overwhelm the user <ref type="bibr" coords="3,411.45,561.42,13.74,8.02" target="#b16">[17]</ref>, therefore a part of the screen is empty. This makes the thread browser lacking in cases where an overview of the structure of the collection is needed. There is a great variety of multimedia visualizations, with our text covering the main approaches and being by no means exhaustive. From the analytic perspective, there is no strongly dominant solution, each is strong in different areas and suitable for different tasks. The quest for visualizations and metaphors suitable for multimedia analytics thus remains open. supervised and unsupervised learning <ref type="bibr" coords="4,183.17,53.38,13.74,8.02" target="#b30">[31]</ref>, and ranking. Interactivity in machine learning is surveyed in Section 3.2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation and learning</head><p>The workflow of machine multimedia analysis algorithms is depicted in <ref type="figure" coords="4,55.11,107.64,29.92,8.02" target="#fig_3">Figure 3</ref>. Machines need to extract an explicit and mathematical intermediate representation of the data in order to extract semantics. Building the representation starts with features, i.e., numeric values derived from pixels in a single item mimicking the low-level representation used by the human's perceptual system. There are many kinds of features, each being a different abstraction from pixel data. Typically , multiple features per item are computed, resulting in a feature vector representing that item. Feature vectors are in turn aggregated into a feature representation of the entire collection. Annotations can be treated either as string data and represented directly, or converted to text features or conceptual representations if semantics extraction is needed. Metadata are machine-readable per se and require no conversion. Extracting semantics from feature representations is then performed by machine learning. The semantics extraction process faces numerous challenges. The main one is the semantic gap defined by Smeulders et al., i.e., the disproportion between the information extractable from a multimedia item's content by a human and the information extractable from the feature representation of the same item by a machine <ref type="bibr" coords="4,246.38,287.53,13.74,8.02" target="#b71">[73]</ref>. Simply put, machines are essential to multimedia analytics due to their capability to handle large multimedia collection much better than a human, but their understanding of multimedia is worse than a human's. In addition, not only is the human representation difficult for machines, but also vice versa. The human's semantic, non-numeric perception of multimedia renders the vast majority of features unintuitive. Since most features carry no meaning to a human, the usefulness of modelling the data using basic feature statistics is limited. All in all, the semantic gap remains a major research challenge prohibiting easy highlevel extraction of semantics from multimedia data. In the current state of the art, there are essentially two pipelines for semantic multimedia analysis: explicit feature extraction followed by classification, and deep learning. The former is the more classic one. The most used features are local, each of them corresponding to a certain region within the image. Their advantage is a number of invariances, for example positional invariance, i.e., a concept is detected irregardless of its position in an item. The dominant one is the scale-invariant feature transform (SIFT) by Lowe <ref type="bibr" coords="4,228.18,467.43,13.74,8.02" target="#b52">[53]</ref>. The existing variations of SIFT, each focused on a different visual aspect, are surveyed by Van de Sande et al. <ref type="bibr" coords="4,153.40,487.36,13.74,8.02" target="#b78">[80]</ref>. A faster option, achieving similar properties to SIFT, are the speeded-up robust features (SURF) by Bay et al. <ref type="bibr" coords="4,70.45,507.28,9.52,8.02" target="#b1">[2]</ref>. GIST by Oliva and Torralba is an example of a global feature, which computes scene characteristics directly from the data <ref type="bibr" coords="4,45.00,527.21,13.74,8.02" target="#b62">[63]</ref>. Feature representations involve grouping similar feature vectors together (typically using quantization and/or histograms). The dominant representations in the state of the art are the histogram of oriented gradients (HOG) by Dalal and Triggs <ref type="bibr" coords="4,179.64,557.09,13.74,8.02">[15]</ref>, bag of visual words (BoW) by Sivic and Zisserman <ref type="bibr" coords="4,133.15,567.06,13.74,8.02">[72]</ref>, Fisher vectors by Perronnin et al. <ref type="bibr" coords="4,278.20,567.06,13.74,8.02" target="#b63">[64]</ref>, and the vector of locally aggregated descriptors (VLAD) by Jégou et al. <ref type="bibr" coords="4,57.32,586.98,13.74,8.02" target="#b36">[37]</ref> . Feature representations can be further enhanced by incorporating spatial information, using spatial pyramids by Lazebnik et al. <ref type="bibr" coords="4,45.00,606.91,13.74,8.02" target="#b44">[45]</ref>, part-based models by Felzenszwalb et al. <ref type="bibr" coords="4,219.74,606.91,14.94,8.02" target="#b25">[26] </ref>or codemaps by Li et al. <ref type="bibr" coords="4,75.38,616.87,13.74,8.02" target="#b49">[50]</ref>. Hashing, surveyed by Zhang and Rui <ref type="bibr" coords="4,230.69,616.87,13.74,8.02" target="#b96">[98]</ref>, is often used in order to reduce dimensionality. The second step of the pipeline, classification, is in almost all cases carried out by a support vector machine (SVM) by Cortes and Vapnik <ref type="bibr" coords="4,185.84,646.76,13.74,8.02" target="#b13">[14]</ref>, although other classifiers such as nearest neighbour <ref type="bibr" coords="4,142.50,656.72,10.45,8.02" target="#b5">[6] </ref>or random forests <ref type="bibr" coords="4,223.57,656.72,14.94,8.02" target="#b21">[22] </ref>have also been used. The second pipeline, deep learning, revolves around using deep neural nets, which extract features and semantics using one model. In the visual multimedia domain, convolutional neural networks (CNN) by LeCun and Bengio are used <ref type="bibr" coords="4,158.39,696.57,13.74,8.02" target="#b45">[46]</ref> . While the original idea of a convolutional neural network can be traced back to the 80s, efficient algorithms for training have not existed until fairly recently: the first efficient algorithm for a deep net was conceived in 2006 by Hinton et al. <ref type="bibr" coords="4,58.22,736.42,13.74,8.02" target="#b33">[34]</ref>. Since then, deep nets have successfuly been used both in a <ref type="bibr" coords="4,334.32,336.38,13.74,8.02" target="#b75">[77]</ref>, and a broad domain such as the ImageNet classification performed by Krizhevsky et al. <ref type="bibr" coords="4,426.76,346.34,14.94,8.02" target="#b42">[43] </ref> The ImageNet paper is considered a breakthrough in computer vision, establishing deep learning as the currently dominant approach. Both pipelines remain viable and actively researched though, and overall, the quality of semantics extraction in the state of the art has been steadily rising in recent years <ref type="bibr" coords="4,307.62,396.16,13.74,8.02" target="#b72">[74]</ref>. The typical case of supervised learning in semantics extraction involves classic classification and using only one information channel of the data (typically visual). In multimedia analysis, more and more effort is being made to make the task and the data more flexible. Discovery of new classes and transfer of knowledge between them is the domain of zero-shot learning, which is predominantly realized by attribute learning, i.e., representing each class by a set of attributes <ref type="bibr" coords="4,540.82,466.66,13.74,8.02" target="#b43">[44]</ref>. New classes are characterized by attributes learned from previously available classes. The three heterogeneous data sources (content, annotations , metadata) are widely utilized for learning in the video do- main <ref type="bibr" coords="4,328.26,506.51,13.74,8.02" target="#b73">[75]</ref>. In the image domain, surprisingly enough, comparatively little attention has been given to this phenomenon, even though annotations and metadata have been shown to improve the quality of the machine learning model in several studies <ref type="bibr" coords="4,464.61,536.40,10.87,8.02" target="#b8">[9]</ref>[11]<ref type="bibr" coords="4,489.97,536.40,14.49,8.02" target="#b48">[49]</ref>[87]<ref type="bibr" coords="4,518.94,536.40,14.49,8.02" target="#b89">[91]</ref>. This situation is changing due to the advent of social media, which provide vast annotated collections at the cost of high noise. Learning to annotate, i.e., assign annotations based on the content, has received much attention in the recent years. Annotation as a task is essentially a special case of classification, and the main approaches exploit this <ref type="bibr" coords="4,307.62,596.18,14.59,8.02" target="#b9">[10]</ref>[27]<ref type="bibr" coords="4,336.79,596.18,14.59,8.02" target="#b82">[84]</ref><ref type="bibr" coords="4,351.38,596.18,14.59,8.02" target="#b84">[86]</ref>. The classic annotation approaches rely on training on reliable expert annotations, whose reliability is not a given in the social media era. The emerging field of social image retrieval caters for the low quality and high noise of social tag annotations by incorporating tag relevance learning <ref type="bibr" coords="4,412.14,636.03,13.74,8.02" target="#b47">[48]</ref>. To summarize, supervised learning is very well-studied in the multimedia domain, with an array of wellperforming approaches with increasingly flexible and heterogeneous models. Unsupervised learning in the multimedia domain, useful for structuring the collection, is wholly dependent on the feature representation and similarity measure used. Since neither the representation nor the similarity measure is canonical, clusters of multimedia items convey a structure of the data, rather than the structure of the data, as shown on the " Queen-Little Red Riding Hood-phone booth " example in Section 2.1. Technically, unsupervised learning algorithms do not differ from those used on conventional datasets, since once the collection is converted to features, an unsupervised algorithm does not distinguish whether it is operating on feature representation or on a conventional dataset <ref type="bibr" coords="5,82.03,83.27,13.74,8.02" target="#b15">[16]</ref>. Mainstream algorithms thus include k-means <ref type="bibr" coords="5,272.89,83.27,13.74,8.02" target="#b51">[52]</ref>, the EM algorithm <ref type="bibr" coords="5,106.89,93.23,13.74,8.02" target="#b20">[21]</ref> , and hierarchical clustering, reflecting the application invariance of unsupervised learning. Ranking is crucially important for semantic search, with search results being a reranking of the collection based on relevance to the search query. There are two dominant approaches towards semantic search: text-based search and content-based search. Text-based search relies on performing text search on annotations, while contentbased search performs the search based on the semantics extracted from visual content <ref type="bibr" coords="5,127.12,173.51,13.74,8.02" target="#b96">[98]</ref>. Text-based search is the more mature field, but depends heavily on accuracy and relevance of annotations, a handicap in the current social tagging era. Content-based search, on the other hand, suffers from the semantic gap. Semantic search in multimedia collections does not have a canonical query scheme. Dominant query methods, surveyed by Snoek and Worring <ref type="bibr" coords="5,251.35,223.32,13.74,8.02" target="#b73">[75]</ref>, comprise query by keyword (matching the query against annotations/metadata), query by example (the user provides an example item and wants similar results), and query by concept (results are based on relevance to the specified concept). A comprehensive survey of ranking methods in semantic search has been provided by Mei et al. <ref type="bibr" coords="5,228.64,273.13,13.74,8.02" target="#b56">[57]</ref> . Similarly to supervised learning, ranking has also received much research attention, with the respective state of the art providing respectable performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interaction</head><p>Interactive machine learning techniques operate in a semi-supervised setting where labels exist only for a small fraction of the otherwise unlabeled dataset <ref type="bibr" coords="5,117.84,347.31,13.74,8.02" target="#b11">[12]</ref> . This is precisely the situation we face in multimedia analytics. The flow of interactive machine learning is depicted in <ref type="figure" coords="5,63.24,367.23,29.05,8.02" target="#fig_5">Figure 4</ref> . The quality of the results of an interactive machine learning technique is determined by three factors: relevance, speed of convergence , and speed of response. Relevance of items in each round and speed of convergence are competing principles. The trade-off between them is known in the literature as the precision-recall trade- off <ref type="bibr" coords="5,66.79,417.05,13.74,8.02" target="#b24">[25]</ref>. Increasing precision, i.e., the proportion of relevant items in each round, harms recall, the proportion of the relevant items in the whole dataset returned during all interaction rounds, and vice versa. Each system thus needs to select the position on the precision-recall curve carefully. Search favours precision, since the highest number of relevant items in the early round(s) is imperative; while thorough exploration leans towards recall, since we need to see as many relevant items as possible in a small number of rounds. The speed of response is a hard constraint: interactivity imposes the time between the user input and the system response not exceeding the order of seconds . Relevance of results, speed of convergence, and response are all key to interactive machine learning employed in multimedia analytics. The precision-recall trade-off is most reflected in the query strategy of interactive machine learners, i.e., the selection of items whose relevance is to be judged by the user. The original, and to date still used interactive machine learning paradigm is relevance feedback, surveyed by Zhou and Huang <ref type="bibr" coords="5,127.94,577.02,18.05,8.02" target="#b98">[100]</ref>. Relevance feedback approaches maximize precision, showing the items the learner is most certain about being relevant . Presenting those items means a positive effect on the user, but little gain for the learner, since it is already certain about them. Another approach, currently the dominant one, is active learning, which queries items the learner is least certain about, i.e., those with the biggest information gain. Query strategies in active learning are extensively surveyed by Settles <ref type="bibr" coords="5,162.05,646.76,13.74,8.02" target="#b70">[71]</ref>. Maximizing the information gain for the learner results in better recall and results overall, but requires a patient, cooperative user. The queried items that are difficult for the learner might also be difficult for the user. This phenomenon is called variable labelling cost, with several works devoted to predicting the cost <ref type="bibr" coords="5,71.65,696.57,14.94,8.02" target="#b80">[82] </ref>and conducting active learning on a labelling budget <ref type="bibr" coords="5,287.20,696.57,13.74,8.02" target="#b81">[83]</ref>. For multimedia analytics, both relevance feedback and active learning have their merit: we need to convince the analyst that the machine is learning by showing more relevant items each round, but we also need to maximize the information gain per interaction to retrieve as many relevant items in total as possible. The technical execution of interactive machine learning boils down to two key aspects: the interactions and the algorithms. The most dominant interaction is marking relevant items, possibly with degrees of certainty or explicit marking of irrelevant items <ref type="bibr" coords="5,484.99,343.05,18.05,8.02" target="#b98">[100]</ref>. A newer trend is relative feedback, where the analyst states what attributes the queried item lacks in comparison to the relevant one, enabling pruning items which are more lacking in the particular attribute than the queried item. Examples of approaches successfully using this paradigm include WhittleSearch by Kovashka and Grauman <ref type="bibr" coords="5,489.46,392.87,14.94,8.02" target="#b41">[42] </ref>and the weighted online attribute learner by Biswas and Parikh <ref type="bibr" coords="5,482.38,402.83,9.52,8.02" target="#b4">[5]</ref> . State-of-the-art active learning approaches are thus shifting from simpler relevance indications to more complex, informative ones. Algorithm-wise, the three dominant approaches in active learning, as surveyed and explained in detail by Huang et al. <ref type="bibr" coords="5,397.91,442.68,13.74,8.02" target="#b34">[35]</ref>, are active SVM by Tong and Chang <ref type="bibr" coords="5,549.82,442.68,13.74,8.02" target="#b77">[79]</ref>, biased discriminant analysis (BDA) by Zhou and Huang <ref type="bibr" coords="5,532.72,452.64,13.74,8.02" target="#b97">[99]</ref>, and rank-based approaches surveyed by <ref type="bibr" coords="5,448.77,462.60,35.35,8.02">Mei et al. </ref> in their broader ranking survey <ref type="bibr" coords="5,358.08,472.57,13.74,8.02" target="#b56">[57]</ref>. Approaches with a more flexible model also exist. Weak class labels have been considered by <ref type="bibr" coords="5,474.76,482.53,40.32,8.02">Mitra et al. </ref> in their probabilistic active SVM <ref type="bibr" coords="5,392.38,492.49,13.74,8.02" target="#b58">[59]</ref>, as well as in the conditional active learning setting by Li and Sethi <ref type="bibr" coords="5,402.68,502.46,13.74,8.02" target="#b46">[47]</ref> . Evolving classes and new class discovery are considered for example by the binary feedback framework by Joshi et al. <ref type="bibr" coords="5,357.93,522.38,13.74,8.02" target="#b37">[38]</ref>, and are surveyed by Settles <ref type="bibr" coords="5,477.96,522.38,13.74,8.02" target="#b70">[71]</ref>. Overall, interactive machine learning provides a wide array of techniques with an increasingly elaborate interaction structure, showing promise with respect to incorporation into multimedia analytics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTIMEDIA ANALYTICS</head><p> Now that the individual human-centered and machine-centered components and techniques are reviewed, the multimedia analytics model can be proposed. This is done in Section 4.1. Section 4.2 reviews pioneer multimedia analytics systems in view of the desired capabilities. Section 4.3 presents a research agenda for multimedia analytics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p> The previous sections structurally revised the preliminaries of a multimedia analytics system, with emphasis on interactivity, joint utilization of the heterogeneous data sources in the collection (content, annotations , and metadata) and semantic navigability. The strong need for semantic navigability and the related semantic gap phenomenon are especially important considerations and the main difference between multimedia analytics and the closely-related classic visual analytics. The proposed multimedia analytics model, which we explain in this section, is depicted in <ref type="figure" coords="5,399.54,736.42,30.84,8.02">Figure 5</ref> and divided into four tiers. The first and lowest tier, methods, represents atomic techniques of visualization , interaction and data analysis in a multimedia analytics setting. The second tier, intents, represents combinations of methods which express individual intentions of the analyst. The third tier, procedure, corresponds to a high-level model of activities and intents taking the analyst from the beginning to the completion of her objective. Objectives , the fourth and top tier, are the master goals of the analyst. To illustrate, consider a medical scientist using a multimedia analytics system on a collection of medical scans. Her objective is to find the incidence of cancer in the population. The procedure to take the analyst to her objective involves exploring the scans, searching for symptoms of cancer and determining the distribution of cancer within the population . Each of the steps taken in the procedure exhibits a certain intent, e.g., " sort the patients based on liver abnormality. " This intent is accomplished for example by the following methods: computing the ranking, visualizing thumbnails of the scans in a grid ordered by the ranking, the analyst panning over the grid, and magnifying the thumbnails . The four-tiered model thus provides a structured overview of all cornerstones of multimedia analytics. Let us first examine the objectives. The main, high-level multimedia analytics objective is to guide the analyst through large and complex multimedia collections to knowledge. This knowledge ranges from abstract to particular. Abstract knowledge means that the analyst knows something about the data she did not before, understands the data, or grasps their structure. Abstract knowledge gain models are well known in visual analytics literature. Pirolli and Card coined the term sensemaking <ref type="bibr" coords="6,127.23,315.20,13.74,8.02" target="#b65">[66]</ref>. Insight is maybe the most used term for abstract knowledge gain, appearing both in information visualization <ref type="bibr" coords="6,45.00,335.13,14.94,8.02" target="#b61">[62]</ref><ref type="bibr" coords="6,59.94,335.13,14.94,8.02" target="#b93">[95] </ref>and visual analytics <ref type="bibr" coords="6,155.30,335.13,14.48,8.02" target="#b39">[40]</ref>[41]<ref type="bibr" coords="6,184.25,335.13,14.48,8.02" target="#b76">[78]</ref>. Insight is a notoriously fickle term evading the boundaries of precise definition. Rather, North enumerates the characteristics of insight: insight is complex, deep, qualitative, unexpected, and relevant <ref type="bibr" coords="6,179.77,365.02,13.74,8.02" target="#b61">[62]</ref>. Particular knowledge gain means completing a complex, high-level task, like the medical scientist determining the incidence of cancer. Gaining particular knowledge involves gaining abstract knowledge: in any domain, the analyst still needs to explore and understand the data. Hence, from the model point of view, we consider the different terms for knowledge gain largely equivalent. In the rest of the paper, we will use the term insight to indicate the main objective. Several works develop the methods and intents as incorporated in the model. Their grouping in the model depicted in <ref type="figure" coords="6,227.20,457.47,29.71,8.02">Figure 5</ref>is inspired by the work of Pike et al. <ref type="bibr" coords="6,139.88,467.43,13.74,8.02" target="#b64">[65]</ref> . For analytic purposes, it is highly desirable that they operate on the semantics of the content. Thus, they need to be realized by employing machine learning techniques, and as such are affected by the semantic gap. Structural methods, focusing on uncovering the structure of the data, correspond to the interactions by Amar et al. <ref type="bibr" coords="6,106.65,517.24,9.52,8.02" target="#b0">[1]</ref>. Their usefulness in the multimedia domain is limited: since the human perception of multimedia is non-numeric, retrieve value, compute derived value, find extremum, determine range, and characterize distribution are all meaningless in their purely numeric sense. Find anomalies, cluster, and correlate are viable, but the analyst needs to see the items and perform them in a semantic manner. Visualization methods (particular visualization techniques like charts or grids) and interaction methods (atomic interactions like panning or zooming), both adapted from the work of Pike et al. <ref type="bibr" coords="6,256.11,596.94,14.94,8.02" target="#b64">[65] </ref>are in their technical sense unaffected by the semantic gap. Interaction intents , corresponding to the 7 interactions by Yi et al. <ref type="bibr" coords="6,242.51,616.87,13.74,8.02" target="#b92">[94]</ref>, represent individual steps towards insight. Four are impacted by the semantic gap: explore, reconfigure, filter, and abstract/elaborate. Indeed, the analyst might want to navigate further based on a semantic dimension (exploration, " show me other dogs of this colour " ); sort based on a semantic concept (reconfigure, " sort dresses from least formal to most formal " ); filter semantically ( " show me only pictures of people hiking " ); and visualize item subsets based on concept hierarchies, if present (abstract/elaborate, " show me all animals → all mammals → all foxes " ). The usefulness of " encode " in the visual domain is limited due to the items being displayed directly. Finally, the visualization intents, conceived by Pike et al. <ref type="bibr" coords="6,170.48,726.46,13.74,8.02" target="#b64">[65]</ref> , represent visualization capabilities leading to insight. In the multimedia domain, " differentiate, " <ref type="figure" coords="6,307.62,431.77,19.33,7.37">Fig. 5</ref>. The four-tiered multimedia analytics model. The tiers progress from low-level methods (bottom) to high-level objectives (top). Bold text indicates components critical for multimedia analytics tasks, orange text denotes components desired to operate on content semantics. " show outliers, " and " compare " are expected to be semantic. All intents and methods affected by the semantic gap are precisely those that are actually related to the analysis of the collection as a data resource. Indeed, if we select only those methods and intents unaffected by the semantic gap, we end up with the capabilities of a generic multimedia browser as discussed in Section 2.2. These are certainly useful for browsing and visualizing data, but might not be very suitable for the analysis of large-scale multimedia resources. This emphasizes the need to maintain a semantic model of the data to facilitate semantic methods and intents. Now that the high-level objectives and the lower-level methods and intents are in place, the model of the procedure actually bringing the analyst to her objectives using the methods and intents needs to be discussed. In visual analytics, the process of attaining insight in visual analytics has been modelled by Keim et al. <ref type="bibr" coords="6,472.44,636.80,14.27,8.02" target="#b39">[40]</ref><ref type="bibr" coords="6,486.71,636.80,14.27,8.02" target="#b40">[41]</ref>. This process, which takes the analyst from data processing through iteratively updated visualization and model to knowledge, is also highly relevant in the multimedia analytics domain and it is highly desirable its flow gets fully adapted. Its iterative flow captures the nature of building insight, which takes time, non-trivial interactions, and reasoning. The visual analytics process also advocates tight integration of the visualization with the model, a notion equally important for multimedia analytics due to the emphasis on semantics. Bearing in mind the nature of multimedia analytics objectives, there are two basic approaches towards attaining them <ref type="bibr" coords="6,361.42,736.42,13.94,8.02" target="#b54">[55]</ref>: Finding " needles in the haystack " Ranking  @BULLET Exploration, applicable when the analyst is faced with a collection she does not know much about beforehand, and wants to discover what is inside and/or how the data are structured. An exploratory session typically takes time and involves a very dynamic model of the data, continuously refined as the analyst iteratively gains knowledge. @BULLET Search, applicable when the analyst has a clear idea what she is looking for and queries the system for items relevant to certain attributes. A search session is then a sequence of query-response pairs, and the analyst expects fast response. The data model is static, since the analyst knows exactly what she is looking for, and communicated to the system through a query. </p><p> A typical multimedia analytics task has elements of both. For example , a forensics expert might not only want to judge if the multimedia content of a suspect's seized computer contains incriminating content (search), but also determine the full extent of the suspect's illegal activities (exploration). Based on this observation, we model multimedia analytics tasks as lying on an exploration-search axis as depicted in <ref type="figure" coords="7,54.00,337.25,29.88,8.02" target="#fig_7">Figure 6</ref>. During the analytic session, the proportion of exploration and search in a task at hand changes over time. Typically, the task is more exploration-oriented in the beginning, as the analyst does not know much about the collection yet, and progress more towards search when the analyst already has a good grasp of the collection's content and knows what to look for. The exploration-search axis is an umbrella model for most multimedia analytics tasks. The depiction of some archetypal tasks in the multimedia domain is included in <ref type="figure" coords="7,280.97,406.99,23.41,8.02;7,54.00,416.95,3.36,8.02" target="#fig_7">Figure  6</ref>. In order to help the analyst achieve insight, multimedia analytics systems need to enable the user to alternate between exploration and search. Even though exploration and search have different, sometimes antagonistic properties, a multimedia analytics model needs to incorporate exploration and search integrally. Otherwise, the analyst cannot alternate between the two and the system breaks down into disjoint exploratory and search components. The analyst perceives multimedia content through attributes which are chiefly semantic, optionally also involving annotation and metadata and the related statistics. Pure search is then simply filtering or reranking based on those attributes. When building a mental model of the data, the analyst structures her interest into attribute aggregates, e.g., " hiking in the Alps " (combining the " person " and " outdoors " semantic concepts with geo coordinates matching the Alps region). The attributes are categorical; the aggregations of the attributes into meaningful concepts, if we disregard fringe cases where the semantics of the content play no role, are thus categories . The analyst then assigns, often implicitly, its labels to individual items. This applies, for example, to the nowadays typical tasks where we select images based on some notion of relevance or interest, like picking out nice photos from the collection shot on vacation or searching for pictures of a favourite celebrity. Even though we might not explicitly assign labels, by picking relevant images we actually categorize them as relevant, while the non-picked ones are effectively categorized as irrelevant. Exploration is then an instance of categorization with few dynamically evolving categories, while search could be instantiated as categorization with fixed categories corresponding to the degree of relevance of items to the search query. Indeed, the vast majority of multimedia analysis techniques concerning semantic tasks is a variation on supervised learning, and classification (i.e., categorization ) in particular. Categorization is thus a useful umbrella task for the exploration-search axis, and thus, by proxy, able to accomodate most multimedia analytics tasks: As discussed extensively in the previous text and shown by the majority of the model components involving semantics in <ref type="figure" coords="7,533.59,252.50,30.05,8.02">Figure 5</ref>, categorization in multimedia analytics is heavily revolving around semantics . In addition, visual multimedia categorization is much less dependent on statistics (semantic statistics are unintuitive and hard to obtain) and has much lower tolerance for error from the users (since the individual errors are spotted instantly) than categorization on other types of multimedia collections. Hence, it is doubly imperative that the learned categorical model very closely follows the mental model of the analyst. There is, however, a difference between analytic categorization as a model of human reasoning and categorization or classification in the classic statistics and machine learning sense. The human's notion of categories is quite flexible: she can adapt new information into her knowledge schema and conversely, adapt the knowledge schema to fit the newly acquired information <ref type="bibr" coords="7,453.08,382.01,13.74,8.02" target="#b28">[29]</ref> . Categorization in the machine world, i.e., classification, is in its classic form much more rigid: the class schema is defined beforehand and it cannot be changed without retraining the model from scratch. The meaning and parameters of categorization thus depend on whether it is performed by a human or a machine. In linguistics, the phenomenon of the meaning of words being dependent on context is studied in pragmatics <ref type="bibr" coords="7,507.66,441.79,13.74,8.02" target="#b57">[58]</ref>. Hence, we call this phenomenon the pragmatic gap: @BULLET Pragmatic gap — the gap between the parameters of a categorization task as performed by the human and the parameters of a categorization task as performed by the machine. </p><p>In other words, the pragmatic gap is related to the adaptability of the model as the analyst progresses towards insight. To support the multimedia analyst, the machine models need to be able to mimic her flexible mental model as closely as possible. That involves at least three aspects: @BULLET New categories on the fly — For example, when a forensics expert who has initially worked with " child abuse " and " harmless " categories encounters evidence of terrorism, she adapts her cognitive model to include a new " terrorism " category. The machine should be able to do the same. @BULLET Non-exclusive categories — If one category is " people, " and another is " Rome, " then the analyst should not be forced to choose where to put a photo of a couple in front of Fontana di Trevi. @BULLET Dynamic category semantics — Suppose an analyst is looking for evidence of arms trafficking and his " suspicious " category contains firearms. Further exploration reveals photos involving explosives, so they are added into the model of the " suspicious " category. Later, the firearms photos are deemed no longer suspicious and removed from the model of the category (e.g., due to the suspect's firearm being properly licensed), which now concerns explosives only. The machine's category model should follow all these steps. <ref type="bibr" coords="8,204.72,190.75,11.09,5.95" target="#b91">[93] </ref>sVisit <ref type="bibr" coords="8,206.75,178.91,11.09,5.95" target="#b55">[56] </ref>Informedia <ref type="bibr" coords="8,157.16,178.91,11.09,5.95" target="#b32">[33] </ref>Canopy <ref type="bibr" coords="8,185.92,168.47,7.76,5.95" target="#b7">[8] </ref>Similiarity browser <ref type="bibr" coords="8,125.77,154.17,11.09,5.95" target="#b60">[61] </ref>INA browser <ref type="bibr" coords="8,219.61,133.94,11.09,5.95" target="#b79">[81] </ref>MediaTable <ref type="bibr" coords="8,215.77,110.27,11.09,5.95" target="#b18">[19]</ref><ref type="bibr" coords="8,226.85,110.27,11.09,5.95" target="#b19">[20] </ref>semantic gap pragmatic gap <ref type="figure" coords="8,45.00,233.34,19.22,7.37">Fig. 8</ref>. Comparison of pioneer multimedia analytics systems based on their position with respect to the semantic and pragmatic gaps (axis ticks correspond to system capabilities described in <ref type="figure" coords="8,207.99,252.26,27.51,7.37" target="#fig_8">Figure 7</ref>). Orange points represent systems whose model involves heterogeneous data (visual content, annotations, metadata), black points correspond to systems without a heterogeneous model. </p><p>The pragmatic gap is orthogonal to the semantic gap: the former's concern is the adaptability of the model, the latter is related to the semantic expressiveness of the model. <ref type="figure" coords="8,183.56,332.86,31.07,8.02" target="#fig_8">Figure 7</ref>highlights the impact of the gaps on multimedia analytics models. Bridging the gaps will foster better overall " understanding " between the machine and the human , allowing the machine to create models more closely resembling the analyst's actual model. Bridging the gaps in the context of exploration and search is thus one of the core challenges in multimedia analytics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pioneer systems</head><p> To the best of our knowledge, no multimedia analytics system in existence can handle both the semantic and the pragmatic gaps fully. In this section, we review pioneer multimedia analytics systems that have so far paved the way towards narrowing the gaps. <ref type="figure" coords="8,224.49,456.45,30.11,8.02">Figure 8</ref>depicts the discussed systems in the semantic gap-pragmatic gap space, mapping the current state of the art in multimedia analytics. One of the first pioneers is the Informedia system used for semantic navigation of the eponymous online digital library, conceived by Hauptmann and Smith in 1995 <ref type="bibr" coords="8,166.06,506.77,13.74,8.02" target="#b31">[32]</ref>. Being continuously updated since then, Informedia has received a relevance feedback component in 2008 <ref type="bibr" coords="8,75.39,526.70,14.94,8.02" target="#b32">[33] </ref> (albeit one refining search results, rather than maintaining an adaptive model). Another early pioneer system is the similarity manipulation browser by Nguyen et al. <ref type="bibr" coords="8,186.35,546.62,13.74,8.02" target="#b60">[61]</ref>. This approach employs a similarity space browser through which the user directly manipulates the similarity space, with the machine recomputing the used similarity and rearranging the items based on the interactions. The last example of an early adopter is the multimedia browser developed for the French Audiovisual Institute (INA) by Viaud et al. <ref type="bibr" coords="8,227.47,596.43,14.94,8.02" target="#b79">[81] </ref>This approach combines a similarity space browser, visual summary techniques, and active learning for interactive exploration of the French TV archives. The field of multimedia analytics has been defined in 2010 by Chinchor et al. <ref type="bibr" coords="8,84.72,636.80,14.94,8.02" target="#b12">[13] </ref> and since then, the first systems bearing the multimedia analytics label have started appearing. One group of approaches, including Newdle by Yang et al. <ref type="bibr" coords="8,162.17,656.72,14.94,8.02" target="#b91">[93] </ref>and I-SI by Wang et al. <ref type="bibr" coords="8,263.85,656.72,13.74,8.02" target="#b83">[85]</ref> , targets news and social media, bringing interactive exploration of topic trends in news archives and on social networks.  on the extensive survey, therefore grounded in the established scientific theory and bearing in mind the possibilities and limitations of the state-of-the-art techniques in the related fields. This paper thus paves the way towards interactive, intelligent, and integrated multimedia analytics systems of the future, schematically depicted in <ref type="figure" coords="9,255.86,341.17,29.42,8.02" target="#fig_11">Figure 9</ref>. We believe that those systems will play an increasingly important role in our increasingly digital and multimedia society. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="1,56.99,656.10,193.45,7.52;1,65.96,666.22,58.03,6.86;1,56.99,675.75,205.09,7.52;1,65.96,685.87,62.46,6.86"><head>@BULLET </head><figDesc>Jan Zahálka is with the University of Amsterdam. E-mail: j.zahalka@uva.nl. @BULLET Marcel Worring is with the University of Amsterdam. E-mail: m.worring@uva.nl. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="3,316.62,189.80,195.42,7.37"><head>Fig. 2. </head><figDesc>Fig. 2. Multimedia visualizations (conceptual depiction). </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="4,307.62,274.69,250.38,7.37;4,307.62,284.16,250.38,7.37;4,307.62,293.62,238.24,7.37"><head>Fig. 3. </head><figDesc>Fig. 3. Workflow of machine-centered approaches related to the visual analytics process diagram by Keim et al. [40][41]. Elements of the visual analytics process not utilized in machine processing are greyed out. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5" coords="5,316.62,242.95,250.38,7.37;5,316.62,252.41,250.38,7.37;5,316.62,261.88,250.38,7.37;5,316.62,271.34,39.48,7.37"><head>Fig. 4. </head><figDesc> Fig. 4. Interactive machine learning workflow, incorporated into the visual analytics process diagram by Keim et al. [40][41]. Elements of the visual analytics process not utilized in interactive machine learning are greyed out. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7" coords="7,54.00,102.11,179.03,7.37"><head>Fig. 6. </head><figDesc>Fig. 6. Exploration-search axis with example tasks. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8" coords="7,316.62,166.17,250.38,7.37;7,316.62,175.64,61.00,7.37"><head>Fig. 7. </head><figDesc>Fig. 7. Semantic and pragmatic gaps and their effect on multimedia analytics models. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11" coords="9,54.00,261.68,250.38,7.37;9,54.00,271.15,110.19,7.37"><head>Fig. 9. </head><figDesc>Fig. 9. The proposed multimedia analytics process, expanding upon the diagram by Keim et al. [40][41]. </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false" coords="2,307.62,121.62,250.38,54.70"><figDesc coords="2,307.62,121.62,28.13,7.37">Table 1.</figDesc><table coords="2,307.62,121.62,250.38,54.70">Summary of analytic capabilities of multimedia visualizations: 
screen space efficiency, semantic navigability, and integration of het-
erogeneous information channels, i.e., content, annotations and meta-
data. The values range from − (poor) through + (good) to ++ (excel-
lent),  *  denotes a value dependent on the task at hand (explained in the 
text). 

</table></figure>

			<note place="foot" n="3"> MACHINE PERCEPTION OF MULTIMEDIA Unlike humans, machines have an excellent capability to process large-scale data, thanks to their large memory and processing power. This makes them excellent assistants to the human analyst, who struggles when faced with large collections. In this section, techniques enabling the machine to provide such assistance are reviewed: Section 3.1 focuses on machine representations of multimedia and machine learning in multimedia analysis, chiefly focusing on feature extraction,</note>

			<note place="foot">Rooij et al. [20] facilitates categorization of large image collections using the spreadsheet visual metaphor, making it the first approach in multimedia analytics devoted to categorization. MediaTable has been extended with the active buckets framework maintaining an adaptive model of the data [19]. Meghdadi and Irani adapted the multimedia analytics spirit to the surveillance domain: their sViSIT system allows security experts to search for objects of interest within long video segments and track their trajectory [56]. Canopy by Burtner et al. [8] has a strong focus on integrating content with annotations and metadata , involving the three data sources both in learning and visualization . However, the machine model is not adaptive. These examples show that the field is steadily growing and that multimedia analytics systems are increasingly capable to fill their respective niches. There is no perfect solution which covers all the aspects yet, opening exciting opportunities for multimedia analytics research. 4.3 Research agenda The previous section concluded that so far, no perfect solution covering both gaps exists. In order to advance multimedia analytics, numerous research questions need to be answered. In this section, we propose several key research questions based on the insight gained from the survey process, establishing a multimedia analytics research agenda: 1. Multimedia visualizations and interfaces (a) Are the existing multimedia visualizations and interfaces described in Section 2.2 suitable for categorization as defined in this paper? If not, how can such an interface be created? (b) How can the heterogeneous data in multimedia collections be presented in a truly integrated manner? (c) What is the best way to present large-scale (&gt;1M items) collections to the user? (d) How can multimedia analytics systems be evaluated? 2. Semantic gap (a) How can increasingly higher-level semantics be extracted from raw multimedia? (b) Can high-level semantics be extracted in a manner not prohibiting an interactive multimedia analytics experience? (c) How can the heterogeneous data in multimedia collections be leveraged to improve the semantic quality of the model? 3. Pragmatic gap (a) How can the performance and learning speed of active learning be improved, especially in view of the large scale of the data? (b) Do the currently used interactions with the model described in Section 3.2 have sufficient information bandwidth for the model to improve? How can they be im- proved? (c) What is the most efficient way to introduce new categories on the fly? Is it attribute-based zero-shot learning [44], or is there a better way? (d) What is the best way to introduce non-exclusive categories? Is there a better way than training n 1-vs-all classifiers as done for example in MediaTable [19]? (e) What is the best way to model and introduce fully dynamic , human-like categories whose semantics and boundaries evolve over time? 5 CONCLUSION The mission of multimedia analytics, facilitating understanding and insight into large-scale multimedia, is very important and ambitious. In this paper, we have surveyed a large body of work related to multimedia analytics and proposed a novel general multimedia analytics model. This model brings two major benefits: it provides a structured overview of all levels of multimedia analytics and establishes a clear agenda for the future of multimedia analytics. Both benefits are based</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS </head><p> The authors thank Jack van Wijk for his insightful comments. This research is supported by the Dutch Technology Foundation STW, which is part of the Netherlands Organisation for Scientific Research (NWO), and which is partly funded by the Ministry of Economic Affairs. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct coords="9,76.24,453.16,228.14,7.13;9,76.24,462.63,228.13,7.13"  xml:id="b0">
	<analytic>
		<title level="a" type="main">Low-level components of analytic activity in information visualization</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Amar</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Eagan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOVIS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="111" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,472.09,228.14,7.13;9,76.24,481.55,228.13,7.13;9,76.24,491.02,17.93,7.13"  xml:id="b1">
	<analytic>
		<title level="a" type="main">SURF: Speeded-up robust features</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Bay</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Ess</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tuytelaars</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<forename type="middle">V</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,500.48,228.14,7.13;9,76.24,509.95,178.91,7.13"  xml:id="b2">
	<analytic>
		<title level="a" type="main">PhotoMesa: A zoomable image browser using quantum treemaps and bubblemaps</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">B</forename>
				<surname>Bederson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,519.41,228.14,7.13;9,76.24,528.88,193.03,7.13"  xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Biederman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,538.34,228.14,7.13;9,76.24,547.81,204.93,7.13"  xml:id="b4">
	<analytic>
		<title level="a" type="main">Simultaneous active learning of classifiers &amp; attributes via relative feedback</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Biswas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Parikh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="644" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,557.27,228.14,7.13;9,76.24,566.74,138.57,7.13"  xml:id="b5">
	<analytic>
		<title level="a" type="main">In defense of nearest-neighbor based image classification</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Boiman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Shechtman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Irani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,576.20,228.14,7.13;9,76.24,585.66,191.04,7.13"  xml:id="b6">
	<analytic>
		<title level="a" type="main">Browsing large image datasets through Voronoi diagrams</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Brivio</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Tarini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Cignoni</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1261" to="1270" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,595.13,228.14,7.13;9,76.24,604.59,210.47,7.13"  xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive visual comparison of multimedia data through type-specific views</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Burtner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Bohn</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Payne</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE VDA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,614.06,228.14,7.13;9,76.24,623.52,212.62,7.13"  xml:id="b8">
	<analytic>
		<title level="a" type="main">Annotating collections of photos using hierarchical event and scene models</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Cao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Kautz</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,632.99,228.14,7.13;9,76.24,642.45,228.14,7.13;9,76.24,651.92,69.29,7.13"  xml:id="b9">
	<analytic>
		<title level="a" type="main">Supervised learning of semantic classes for image annotation and retrieval</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Carneiro</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">B</forename>
				<surname>Chan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">J</forename>
				<surname>Moreno</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Vasconcelos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="394" to="410" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,661.38,228.14,7.13;9,76.24,670.84,152.77,7.13"  xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi modal semantic indexing for image retrieval</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Chandrika</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">V</forename>
				<surname>Jawahar</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="342" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,680.31,228.14,7.13;9,76.24,689.77,89.43,7.13"  xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Chapelle</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Schölkopf</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Zien</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semi-Supervised Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,699.24,228.14,7.13;9,76.24,708.70,228.14,7.13;9,76.24,718.17,85.68,7.13"  xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimedia analysis + visual analytics = multimedia analytics</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">A</forename>
				<surname>Chinchor</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">C</forename>
				<surname>Wong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">G</forename>
				<surname>Christel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCGA</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,76.24,727.63,228.14,7.13;9,76.24,737.10,69.29,7.13"  xml:id="b13">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Cortes</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Vapnik</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,320.61,54.06,246.40,7.13;9,338.86,63.52,136.58,7.13"  xml:id="b14">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Dalal</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Triggs</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,72.99,228.14,7.13;9,338.86,82.45,228.14,7.13;9,338.86,91.92,17.93,7.13"  xml:id="b15">
	<analytic>
		<title level="a" type="main">Image retrieval: Ideas, influences , and trends of the new age</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Datta</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Joshi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">Z</forename>
				<surname>Wang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="60" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,101.38,228.14,7.13;9,338.86,110.85,90.99,7.13"  xml:id="b16">
	<analytic>
		<title level="a" type="main">Browsing video along multiple threads</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>De Rooij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,120.31,228.14,7.13;9,338.86,129.78,179.93,7.13"  xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient targeted search using a focus and context video browser</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>De Rooij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,139.24,228.14,7.13;9,338.86,148.70,141.71,7.13"  xml:id="b18">
	<analytic>
		<title level="a" type="main">Active bucket categorization for high recall video retrieval</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>De Rooij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="898" to="907" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,158.17,228.14,7.13;9,338.86,167.63,219.03,7.13"  xml:id="b19">
	<analytic>
		<title level="a" type="main">MediaTable: Interactive categorization of multimedia collections</title>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>De Rooij</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Van Wijk</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCGA</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,177.10,228.14,7.13;9,338.86,186.56,228.14,7.13;9,338.86,196.03,79.92,7.13"  xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">P</forename>
				<surname>Dempster</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">M</forename>
				<surname>Laird</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">B</forename>
				<surname>Rubin</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Royal Statistical Society, Ser. B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,205.49,228.14,7.13;9,338.86,214.96,228.14,7.13;9,338.86,224.42,148.39,7.13"  xml:id="b21">
	<analytic>
		<title level="a" type="main">Longterm incremental web-supervised learning of visual concepts via random savannas</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Ewerth</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Ballafkir</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mühling</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Seiler</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Freisleben</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1008" to="1020" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,233.88,228.14,7.13;9,338.86,243.35,228.14,7.13;9,338.86,252.81,140.53,7.13"  xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel approach to enable semantic and visual image summarization for exploratory image search</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Gao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Li</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,262.28,228.14,7.13;9,338.86,271.74,228.14,7.13;9,338.86,281.21,179.53,7.13"  xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Fang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">K</forename>
				<surname>Tam</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Borgo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Aubrey</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">W</forename>
				<surname>Grant</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">L</forename>
				<surname>Rosin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wallraven</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Cunningham</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Marshall</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Chen</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visualizing natural image statistics. TVCG</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="191228" to="1241" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,290.67,228.13,7.13;9,338.86,300.14,85.15,7.13"  xml:id="b24">
	<analytic>
		<title level="a" type="main">An introduction to ROC analysis</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Fawcett</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,309.60,228.14,7.13;9,338.86,319.07,228.14,7.13;9,338.86,328.53,77.26,7.13"  xml:id="b25">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<forename type="middle">F</forename>
				<surname>Felzenszwalb</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">B</forename>
				<surname>Girshick</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Mcallester</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Ramanan</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,337.99,228.14,7.13;9,338.86,347.46,228.13,7.13;9,338.86,356.92,67.08,7.13"  xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple Bernoulli relevance models for image and video annotation</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">L</forename>
				<surname>Feng</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Manmatha</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">V</forename>
				<surname>Lavrenko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1002" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,366.39,228.14,7.13;9,338.86,375.85,204.98,7.13"  xml:id="b27">
	<analytic>
		<title level="a" type="main">Quality of perception: User quality of service in multimedia presentations</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ghinea</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">P</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="786" to="789" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,385.32,228.14,7.13;9,338.86,394.78,220.50,7.13"  xml:id="b28">
	<analytic>
		<title level="a" type="main">Building and applying a human cognition model for visual analytics</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">M</forename>
				<surname>Green</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Fisher</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAGE InfoVis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,404.25,228.14,7.13;9,338.86,413.71,228.14,7.13;9,338.86,423.17,17.93,7.13"  xml:id="b29">
	<analytic>
		<title level="a" type="main">Google and the mind: Predicting fluency with PageRank</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">L</forename>
				<surname>Griffiths</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Steyvers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Firl</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1069" to="1076" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,432.64,228.13,7.13;9,338.86,442.10,228.13,7.13;9,338.86,451.57,34.32,7.13"  xml:id="b30">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Hastie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Tibshirani</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Friedman</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="9,338.86,461.03,228.14,7.13;9,338.86,470.50,228.13,7.13;9,338.86,479.96,74.69,7.13"  xml:id="b31">
	<analytic>
		<title level="a" type="main">Text, speech and vision for video segmentation: The Informedia project</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">G</forename>
				<surname>Hauptmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">A</forename>
				<surname>Smith</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Symp Compu Mod for Int Lang and Vis</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,489.43,228.14,7.13;9,338.86,498.89,228.14,7.13;9,338.86,508.36,69.95,7.13"  xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient search: The Informedia video retrieval system</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">G</forename>
				<surname>Hauptmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">H</forename>
				<surname>Lin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Christel</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIVR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="543" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,517.82,228.14,7.13;9,338.86,527.28,201.77,7.13"  xml:id="b33">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Osindero</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<forename type="middle">W</forename>
				<surname>Teh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,536.75,228.14,7.13;9,338.86,546.21,228.14,7.13;9,338.86,555.68,138.69,7.13"  xml:id="b34">
	<analytic>
		<title level="a" type="main">Active learning for interactive multimedia retrieval</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Huang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">K</forename>
				<surname>Dagli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Rajaram</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">Y</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">I</forename>
				<surname>Mandel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Poliner</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">P W</forename>
				<surname>Ellis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="648" to="667" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,565.14,228.14,7.13;9,338.86,574.61,228.14,7.13;9,338.86,584.07,107.34,7.13"  xml:id="b35">
	<analytic>
		<title level="a" type="main">Generating summaries and visualization for large collections of geo-referenced photographs</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Jaffe</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Naaman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tassa</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Davis</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="89" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,593.54,228.14,7.13;9,338.86,603.00,228.13,7.13;9,338.86,612.46,37.86,7.13"  xml:id="b36">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Jégou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Douze</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Schmid</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pérez</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,621.93,228.14,7.13;9,338.86,631.39,228.13,7.13;9,338.86,640.86,17.93,7.13"  xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable active learning for multiclass image classification</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">J</forename>
				<surname>Joshi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Porikli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<forename type="middle">P</forename>
				<surname>Papanikolopoulos</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2259" to="2273" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,650.32,228.14,7.13;9,338.86,659.79,226.02,7.13"  xml:id="b38">
	<analytic>
		<title level="a" type="main">PhotoSpread: A spreadsheet for managing photos</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Kandel</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Abelson</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Garcia-Molina</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Paepcke</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Theobald</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,669.25,228.14,7.13;9,338.86,678.80,228.14,6.86;9,338.86,688.18,64.41,7.13"  xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Kohlhammer</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Ellis</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mastering The Information Age -Solving Problems with Visual Analytics. Eurographics</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,697.65,228.14,7.13;9,338.86,707.11,228.13,7.13;9,338.86,716.57,228.13,7.13"  xml:id="b40">
	<analytic>
		<title level="a" type="main">Visual analytics: Scope and challenges</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">A</forename>
				<surname>Keim</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Mansmann</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Schneidewind</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Ziegler</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Data Mining: Theory, Techniques and Tools for Visual Analytics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="76" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="9,338.86,726.04,228.14,7.13;9,338.86,735.50,214.23,7.13"  xml:id="b41">
	<analytic>
		<title level="a" type="main">WhittleSearch: Image search with relative attribute feedback</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Kovashka</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Parikh</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Grauman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2973" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,54.06,228.14,7.13;10,67.25,63.52,181.84,7.13"  xml:id="b42">
	<monogr>
		<title level="m" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Krizhevsky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<surname>Sutskever</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,72.99,228.14,7.13;10,67.25,82.45,228.13,7.13;10,67.25,91.92,33.87,7.13"  xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">H</forename>
				<surname>Lampert</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Nickish</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Harmeling</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,101.38,228.14,7.13;10,67.25,110.85,228.14,7.13;10,67.25,120.31,77.92,7.13"  xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Lazebnik</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Schmid</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Ponce</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2169" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,129.78,228.14,7.13;10,67.25,139.24,228.13,7.13;10,67.25,148.70,193.81,7.13"  xml:id="b45">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech and time series The Handbook of Brain Theory and Neural Networks</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Lecun</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Bengio</surname>
			</persName>
		</author>
		<editor>M. A. Arbib</editor>
		<imprint>
			<date type="published" when="1995" />
			<publisher>The MIT Press</publisher>
			<biblScope unit="page" from="255" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,158.17,228.14,7.13;10,67.25,167.63,77.26,7.13"  xml:id="b46">
	<analytic>
		<title level="a" type="main">Confidence-based active learning</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">I</forename>
				<forename type="middle">K</forename>
				<surname>Sethi</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1251" to="1261" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,177.10,228.14,7.13;10,67.25,186.56,144.39,7.13"  xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning social tag relevance by neighbor voting</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,196.03,228.14,7.13;10,67.25,205.49,228.14,7.13;10,67.25,214.96,17.93,7.13"  xml:id="b48">
	<analytic>
		<title level="a" type="main">Fusing concept detection and geo context for visual search</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">W M</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ICMR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,224.42,228.14,7.13;10,67.25,233.88,228.14,7.13;10,67.25,243.35,228.13,7.13;10,67.25,252.81,17.93,7.13"  xml:id="b49">
	<analytic>
		<title level="a" type="main">Codemaps: Segment, classify and search objects locally</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Gavves</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">E A</forename>
				<surname>Van De Sande</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">W M</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,262.28,228.14,7.13;10,67.25,271.74,192.10,7.13"  xml:id="b50">
	<analytic>
		<title level="a" type="main">Effective browsing of web image search results</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Xie</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Tang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<forename type="middle">W</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">Y</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="84" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,281.21,228.13,7.13;10,67.25,290.67,17.93,7.13"  xml:id="b51">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">P</forename>
				<surname>Lloyd</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIT</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,300.14,228.14,7.13;10,67.25,309.60,100.06,7.13"  xml:id="b52">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">G</forename>
				<surname>Lowe</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,319.07,228.14,7.13;10,67.25,328.53,228.14,7.13;10,67.25,337.99,153.31,7.13"  xml:id="b53">
	<analytic>
		<title level="a" type="main">Analyzing largescale news video databases to support knowledge visualization and intuitive retrieval</title>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Satoh</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VAST</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,347.46,228.14,7.13;10,67.25,356.92,104.69,7.13"  xml:id="b54">
	<analytic>
		<title level="a" type="main">Exploratory search: From finding to understanding</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<surname>Marchionini</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="41" to="46" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,366.39,228.14,7.13;10,67.25,375.85,228.14,7.13;10,67.25,385.32,105.60,7.13"  xml:id="b55">
	<analytic>
		<title level="a" type="main">Interactive exploration of surveillance video through action shot summarization and trajectory visualization</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">H</forename>
				<surname>Meghdadi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Irani</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2119" to="2128" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,394.78,228.14,7.13;10,67.25,404.25,199.54,7.13"  xml:id="b56">
	<monogr>
		<title level="m" type="main">Multimedia search reranking: A literature survey</title>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Mei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Rui</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Li</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Q</forename>
				<surname>Tian</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>to. appear in ACM Computing Surveys</note>
</biblStruct>

<biblStruct coords="10,67.24,413.71,228.14,7.13;10,67.25,423.17,34.32,7.13"  xml:id="b57">
	<monogr>
		<title level="m" type="main">Pragmatics: An Introduction</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">L</forename>
				<surname>Mey</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Blackwell</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
	<note>2nd. edition</note>
</biblStruct>

<biblStruct coords="10,67.24,432.64,228.14,7.13;10,67.25,442.10,181.78,7.13"  xml:id="b58">
	<analytic>
		<title level="a" type="main">A probabilistic active support vector learning algorithm</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Mitra</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">A</forename>
				<surname>Murthy</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">K</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="418" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,451.57,228.14,7.13;10,67.25,461.03,228.14,7.13;10,67.25,470.50,69.29,7.13"  xml:id="b59">
	<analytic>
		<title level="a" type="main">Interactive access to large image collections using similarity-based visualization</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">P</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis Lang and Comp</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,479.96,228.14,7.13;10,67.25,489.43,228.13,7.13;10,67.25,498.89,17.93,7.13"  xml:id="b60">
	<analytic>
		<title level="a" type="main">Interactive search by direct manipulation of dissimilarity space</title>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">P</forename>
				<surname>Nguyen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">W M</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1404" to="1415" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,508.36,228.14,7.13;10,67.25,517.82,17.93,7.13"  xml:id="b61">
	<analytic>
		<title level="a" type="main">Towards measuring visualization insight</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>North</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCGA</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="6" to="9" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,527.28,228.14,7.13;10,67.25,536.75,228.14,7.13;10,67.25,546.21,17.93,7.13"  xml:id="b62">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Oliva</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Torralba</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comp Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,555.68,228.14,7.13;10,67.25,565.14,228.13,7.13;10,67.25,574.61,17.93,7.13"  xml:id="b63">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed Fisher vectors</title>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Perronnin</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Sánchez</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Poirier</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,584.07,228.14,7.13;10,67.25,593.54,151.87,7.13"  xml:id="b64">
	<analytic>
		<title level="a" type="main">The science of interaction</title>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">A</forename>
				<surname>Pike</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">A</forename>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SAGE InfoVis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,603.00,228.14,7.13;10,67.25,612.47,228.13,7.13;10,67.25,621.93,83.02,7.13"  xml:id="b65">
	<analytic>
		<title level="a" type="main">The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis</title>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Pirolli</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Card</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int Conf Intel Analysis</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,631.39,228.14,7.13;10,67.25,640.86,228.14,7.13;10,67.25,650.32,17.93,7.13"  xml:id="b66">
	<analytic>
		<title level="a" type="main">Casual information visualization: Depictions of data in everyday life</title>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Pousman</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Mateas</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1145" to="1152" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,659.79,228.14,7.13;10,67.25,669.25,228.14,7.13;10,67.25,678.72,106.68,7.13"  xml:id="b67">
	<analytic>
		<title level="a" type="main">Beyond 2D-grids: A dependence maximization view on image browsing</title>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Quadrianto</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Kersting</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Tuytelaars</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">L</forename>
				<surname>Buntine</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MIR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="339" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,688.18,228.14,7.13;10,67.25,697.65,224.51,7.13"  xml:id="b68">
	<analytic>
		<title level="a" type="main">Does organisation by similarity assist image browsing?</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Rodden</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Basalaj</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Sinclair</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Wood</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="190" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,707.11,228.14,7.13;10,67.25,716.57,17.93,7.13"  xml:id="b69">
	<monogr>
		<title level="m" type="main">Similarity measures. TPAMI</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Santini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="871" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,67.24,726.04,228.14,7.13;10,67.25,735.50,192.18,7.13;10,311.61,54.06,246.40,7.13;10,329.87,63.52,228.13,7.13"  xml:id="b70">
	<analytic>
		<title level="a" type="main">Active learning literature survey Sivic and A. Zisserman. Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName>
				<forename type="first">B</forename>
				<surname>Settles</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1648" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,72.99,228.14,7.13;10,329.87,82.45,228.14,7.13;10,329.87,91.92,81.25,7.13"  xml:id="b71">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">W M</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Santini</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Gupta</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Jain</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="221349" to="1380" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,101.38,228.14,7.13;10,329.87,110.85,97.19,7.13"  xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual-concept search solved?</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<forename type="middle">W M</forename>
				<surname>Smeulders</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="76" to="78" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,120.31,228.14,7.13;10,329.87,129.78,178.08,7.13"  xml:id="b73">
	<analytic>
		<title level="a" type="main">Concept-based video retrieval</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,139.24,228.14,7.13;10,329.87,148.70,205.51,7.13"  xml:id="b74">
	<analytic>
		<title level="a" type="main">Navigating the worldwide community of photos</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Szeliski</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">N</forename>
				<surname>Snavely</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">M</forename>
				<surname>Seitz</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page" from="47" to="48" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,158.17,228.14,7.13;10,329.87,167.63,228.13,7.13"  xml:id="b75">
	<analytic>
		<title level="a" type="main">Robust Boltzmann machines for recognition and denoising</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Tang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">R</forename>
				<surname>Salakhutdinov</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">G</forename>
				<forename type="middle">E</forename>
				<surname>Hinton</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2264" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,177.10,228.13,7.13;10,329.87,186.56,216.96,7.13"  xml:id="b76">
	<analytic>
		<title level="a" type="main">Illuminating the Path: The Research and Development Agenda for Visual Analytics</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">J</forename>
				<surname>Thomas</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">A</forename>
				<surname>Cook</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Society</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,196.03,228.14,7.13;10,329.87,205.49,145.45,7.13"  xml:id="b77">
	<analytic>
		<title level="a" type="main">Support vector machine active learning for image retrieval</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Tong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Chang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,214.96,228.14,7.13;10,329.87,224.42,228.14,7.13;10,329.87,233.88,17.93,7.13"  xml:id="b78">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName>
				<forename type="first">K</forename>
				<forename type="middle">E A</forename>
				<surname>Van De Sande</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Gevers</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">G M</forename>
				<surname>Snoek</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1582" to="1596" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,243.35,228.14,7.13;10,329.87,252.81,228.13,7.13;10,329.87,262.28,91.65,7.13"  xml:id="b79">
	<analytic>
		<title level="a" type="main">Interactive components for visual exploration of multimedia archives</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<forename type="middle">L</forename>
				<surname>Viaud</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Thièvre</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Goëau</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Saulnier</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">O</forename>
				<surname>Buisson</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIVR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,271.74,228.14,7.13;10,329.87,281.21,228.14,7.13;10,329.87,290.67,110.46,7.13"  xml:id="b80">
	<analytic>
		<title level="a" type="main">What&apos;s it going to cost you?: Predicting effort vs. informativeness for multi-label image annotations</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Vijayanarasimhan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Grauman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2262" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,300.14,228.14,7.13;10,329.87,309.60,228.14,7.13;10,329.87,319.07,37.86,7.13"  xml:id="b81">
	<analytic>
		<title level="a" type="main">Far-sighted active learning on a budget for image and video recognition</title>
		<author>
			<persName>
				<forename type="first">S</forename>
				<surname>Vijayanarasimhan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">P</forename>
				<surname>Jain</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">K</forename>
				<surname>Grauman</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3035" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,328.53,228.14,7.13;10,329.87,337.99,148.98,7.13"  xml:id="b82">
	<analytic>
		<title level="a" type="main">Simultaneous image classification and annotation</title>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Blei</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Fei-Fei</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1903" to="1910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,347.46,228.14,7.13;10,329.87,356.92,228.14,7.13;10,329.87,366.39,228.13,7.13;10,329.87,375.85,17.93,7.13"  xml:id="b83">
	<analytic>
		<title level="a" type="main">I-SI: Scalable architecture for analyzing latent topical-level information from social media data</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Dou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Z</forename>
				<surname>Ma</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Villalobos</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Chen</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<surname>Kraft</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp Graph For</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311275" to="1284" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,385.32,228.14,7.13;10,329.87,394.78,181.29,7.13"  xml:id="b84">
	<analytic>
		<title level="a" type="main">AnnoSearch: Image autoannotation by search</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<forename type="middle">J</forename>
				<surname>Wang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">F</forename>
				<surname>Jing</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<forename type="middle">Y</forename>
				<surname>Ma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1483" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,404.25,228.14,7.13;10,329.87,413.71,185.80,7.13"  xml:id="b85">
	<analytic>
		<title level="a" type="main">Automatic semantic annotation of real-world web images</title>
		<author>
			<persName>
				<forename type="first">R</forename>
				<forename type="middle">C F</forename>
				<surname>Wong</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">H C</forename>
				<surname>Leung</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1933" to="1944" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,423.17,228.14,7.13;10,329.87,432.64,228.14,7.13;10,329.87,442.10,69.95,7.13"  xml:id="b86">
	<analytic>
		<title level="a" type="main">Easy categorization of large image collections by automatic analysis and information visualization</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class &amp; Vis Int UDC Sem</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,451.57,228.14,7.13;10,329.87,461.03,228.14,7.13;10,329.87,470.50,49.81,7.13"  xml:id="b87">
	<analytic>
		<title level="a" type="main">A multimedia analytics framework for browsing image collections in digital forensics</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">A</forename>
				<surname>Engl</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<surname>Smeria</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM MM</title>
		<imprint>
			<biblScope unit="page" from="289" to="298" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,479.96,228.14,7.13"  xml:id="b88">
	<analytic>
		<title level="a" type="main">Multimedia pivot tables</title>
		<author>
			<persName>
				<forename type="first">M</forename>
				<surname>Worring</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<forename type="middle">C</forename>
				<surname>Koelma</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VAST</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,489.43,228.14,7.13;10,329.87,498.89,166.88,7.13"  xml:id="b89">
	<analytic>
		<title level="a" type="main">Multimodal metadata fusion using causal strength</title>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Wu</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">E</forename>
				<forename type="middle">Y</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">B</forename>
				<forename type="middle">L</forename>
				<surname>Tseng</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM MM</title>
		<imprint>
			<biblScope unit="page" from="872" to="881" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,508.36,228.14,7.13;10,329.87,517.82,228.14,7.13;10,329.87,527.28,186.58,7.13"  xml:id="b90">
	<analytic>
		<title level="a" type="main">Semantic image browser: Bridging information visualization with automated intelligent image analysis</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Fan</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Hubball</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Gao</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">H</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">W</forename>
				<surname>Ribarsky</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VAST</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,536.75,228.14,7.13;10,329.87,546.21,182.94,7.13"  xml:id="b91">
	<analytic>
		<title level="a" type="main">Newdle: Interactive visual exploration of large online news collections</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Yang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">D</forename>
				<surname>Luo</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Liu</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCGA</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,555.68,228.14,7.13;10,329.87,565.14,228.14,7.13;10,329.87,574.61,77.26,7.13"  xml:id="b92">
	<analytic>
		<title level="a" type="main">Toward a deeper understanding of the role of interaction in information visualization</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Yi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Jacko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TVCG</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1224" to="1231" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,584.07,228.14,7.13;10,329.87,593.54,228.14,7.13;10,329.87,603.00,136.38,7.13"  xml:id="b93">
	<analytic>
		<title level="a" type="main">Understanding and characterizing insights: How do people gain insights using information visualization?</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">S</forename>
				<surname>Yi</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Kang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">T</forename>
				<surname>Stasko</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">J</forename>
				<forename type="middle">A</forename>
				<surname>Jacko</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM BELIV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,612.47,228.14,7.13;10,329.87,622.35,150.24,6.15"  xml:id="b94">
	<monogr>
		<title level="m" type="main">Multimedia analytics article library</title>
		<author>
			<persName>
				<forename type="first">J</forename>
				<surname>Zahálka</surname>
			</persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,631.39,228.14,7.13;10,329.87,640.86,208.72,7.13"  xml:id="b95">
	<analytic>
		<title level="a" type="main">Visual islands: Intuitive browsing of visual search results</title>
		<author>
			<persName>
				<forename type="first">E</forename>
				<surname>Zavesky</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">S</forename>
				<forename type="middle">F</forename>
				<surname>Chang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">C</forename>
				<forename type="middle">C</forename>
				<surname>Yang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CIVR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="617" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,650.32,228.14,7.13;10,329.87,659.79,119.59,7.13"  xml:id="b96">
	<analytic>
		<title level="a" type="main">Image search — from thousands to billions in 20 years</title>
		<author>
			<persName>
				<forename type="first">L</forename>
				<surname>Zhang</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">Y</forename>
				<surname>Rui</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMCCAP</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1s</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.86,669.25,228.14,7.13;10,329.87,678.72,188.86,7.13"  xml:id="b97">
	<analytic>
		<title level="a" type="main">Small sample learning during multimedia retrieval using BiasMap</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<forename type="middle">S</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct coords="10,329.87,688.18,228.14,7.13;10,329.87,697.65,208.82,7.13"  xml:id="b98">
	<analytic>
		<title level="a" type="main">Relevance feedback in image retrieval: A comprehensive review</title>
		<author>
			<persName>
				<forename type="first">X</forename>
				<forename type="middle">S</forename>
				<surname>Zhou</surname>
			</persName>
		</author>
		<author>
			<persName>
				<forename type="first">T</forename>
				<forename type="middle">S</forename>
				<surname>Huang</surname>
			</persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="536" to="544" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
